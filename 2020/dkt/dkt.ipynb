{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置config\n",
    "class TrainConfig(object):\n",
    "    epochs = 10\n",
    "    decay_rate = 0.92\n",
    "    learning_rate = 0.01\n",
    "    evaluate_every = 100\n",
    "    checkpoint_every = 100\n",
    "    max_grad_norm = 3.0\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    hidden_layers = [200]\n",
    "    dropout_keep_prob = 0.6\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    batch_size = 32\n",
    "    num_skills = 124\n",
    "    input_size = num_skills * 2\n",
    "\n",
    "    trainConfig = TrainConfig()\n",
    "    modelConfig = ModelConfig()\n",
    "    \n",
    "\n",
    "# 实例化config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "class DataGenerator(object):\n",
    "    # 导入的seqs是train_seqs，或者是test_seqs\n",
    "    def __init__(self, fileName, config):\n",
    "        self.fileName = fileName\n",
    "        self.train_seqs = []\n",
    "        self.test_seqs = []\n",
    "        self.infer_seqs = []\n",
    "        self.batch_size = config.batch_size\n",
    "        self.pos = 0\n",
    "        self.end = False\n",
    "        self.num_skills = config.num_skills\n",
    "        self.skills_to_int = {}  # 知识点到索引的映射\n",
    "        self.int_to_skills = {}  # 索引到知识点的映射\n",
    "\n",
    "    def read_file(self):\n",
    "        # 从文件中读取数据，返回读取出来的数据和知识点个数\n",
    "        # 保存每个学生的做题信息 {学生id: [[知识点id，答题结果], [知识点id，答题结果], ...]}，用一个二元列表来表示一个学生的答题信息\n",
    "        seqs_by_student = {}\n",
    "        skills = []  # 统计知识点的数量，之后输入的向量长度就是两倍的知识点数量\n",
    "        count = 0\n",
    "        with open(self.fileName, 'r') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split(\" \")  # 一个列表，[学生id，知识点id，答题结果]\n",
    "                student, skill, is_correct = int(fields[0]), int(fields[1]), int(fields[2])\n",
    "                skills.append(skill)  # skill实际上是用该题所属知识点来表示的\n",
    "                seqs_by_student[student] = seqs_by_student.get(student, []) + [[skill, is_correct]]  # 保存每个学生的做题信息\n",
    "        return seqs_by_student, list(set(skills))\n",
    "\n",
    "    def gen_dict(self, unique_skills):\n",
    "        \"\"\"\n",
    "        构建知识点映射表，将知识点id映射到[0, 1, 2...]表示\n",
    "        :param unique_skills: 无重复的知识点列表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sorted_skills = sorted(unique_skills)\n",
    "        skills_to_int = {}\n",
    "        int_to_skills = {}\n",
    "        for i in range(len(sorted_skills)):\n",
    "            skills_to_int[sorted_skills[i]] = i\n",
    "            int_to_skills[i] = sorted_skills[i]\n",
    "\n",
    "        self.skills_to_int = skills_to_int\n",
    "        self.int_to_skills = int_to_skills\n",
    "\n",
    "    def split_dataset(self, seqs_by_student, sample_rate=0.2, random_seed=1):\n",
    "        # 将数据分割成测试集和训练集\n",
    "        sorted_keys = sorted(seqs_by_student.keys())  # 得到排好序的学生id的列表\n",
    "\n",
    "        random.seed(random_seed)\n",
    "        # 随机抽取学生id，将这部分学生作为测试集\n",
    "        test_keys = set(random.sample(sorted_keys, int(len(sorted_keys) * sample_rate)))\n",
    "\n",
    "        # 此时是一个三层的列表来表示的，最外层的列表中的每一个列表表示一个学生的做题信息\n",
    "        test_seqs = [seqs_by_student[k] for k in seqs_by_student if k in test_keys]\n",
    "        train_seqs = [seqs_by_student[k] for k in seqs_by_student if k not in test_keys]\n",
    "        return train_seqs, test_seqs\n",
    "\n",
    "    def gen_attr(self, is_infer=False):\n",
    "        \"\"\"\n",
    "        生成待处理的数据集\n",
    "        :param is_infer: 判断当前是训练模型还是利用模型进行预测\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if is_infer:\n",
    "            seqs_by_students, skills = self.read_file()\n",
    "            self.infer_seqs = seqs_by_students\n",
    "        else:\n",
    "            seqs_by_students, skills = self.read_file()\n",
    "            train_seqs, test_seqs = self.split_dataset(seqs_by_students)\n",
    "            self.train_seqs = train_seqs\n",
    "            self.test_seqs = test_seqs\n",
    "\n",
    "        self.gen_dict(skills)  # 生成知识点到索引的映射字典\n",
    "\n",
    "    def pad_sequences(self, sequences, maxlen=None, value=0.):\n",
    "        # 按每个batch中最长的序列进行补全, 传入的sequences是二层列表\n",
    "        # 统计一个batch中每个序列的长度，其实等于seqs_len\n",
    "        lengths = [len(s) for s in sequences]\n",
    "        # 统计下该batch中序列的数量\n",
    "        nb_samples = len(sequences)\n",
    "        # 如果没有传入maxlen参数就自动获取最大的序列长度\n",
    "        if maxlen is None:\n",
    "            maxlen = np.max(lengths)\n",
    "        # 构建x矩阵\n",
    "        x = (np.ones((nb_samples, maxlen)) * value).astype(np.int32)\n",
    "\n",
    "        # 遍历batch，去除每一个序列\n",
    "        for idx, s in enumerate(sequences):\n",
    "            trunc = np.asarray(s, dtype=np.int32)\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_to_one_hot(self, num, dim):\n",
    "        # 将题目转换成one-hot的形式， 其中dim=num_skills * 2，前半段表示错误，后半段表示正确\n",
    "        base = np.zeros(dim)\n",
    "        if num >= 0:\n",
    "            base[num] += 1\n",
    "        return base\n",
    "\n",
    "    def format_data(self, seqs):\n",
    "        # 生成输入数据和输出数据，输入数据是每条序列的前n-1个元素，输出数据是每条序列的后n-1个元素\n",
    "\n",
    "        # 统计一个batch_size中每条序列的长度，在这里不对序列固定长度，通过条用tf.nn.dynamic_rnn让序列长度可以不固定\n",
    "        seq_len = np.array(list(map(lambda seq: len(seq) - 1, seqs)))\n",
    "        max_len = max(seq_len)  # 获得一个batch_size中最大的长度\n",
    "        # i表示第i条数据，j只从0到len(i)-1，x作为输入只取前len(i)-1个，sequences=[j[0] + num_skills * j[1], ....]\n",
    "        # 此时要将知识点id j[0] 转换成index表示\n",
    "        x_sequences = np.array([[(self.skills_to_int[j[0]] + self.num_skills * j[1]) for j in i[:-1]] for i in seqs])\n",
    "        # 将输入的序列用-1进行补全，补全后的长度为当前batch的最大序列长度\n",
    "        x = self.pad_sequences(x_sequences, maxlen=max_len, value=-1)\n",
    "\n",
    "        # 构建输入值input_x，x为一个二层列表，i表示一个学生的做题信息，也就是一个序列，j就是一道题的信息\n",
    "        input_x = np.array([[self.num_to_one_hot(j, self.num_skills * 2) for j in i] for i in x])\n",
    "\n",
    "        # 遍历batch_size，然后取每条序列的后len(i)-1 个元素中的知识点id为target_id\n",
    "        target_id_seqs = np.array([[self.skills_to_int[j[0]] for j in i[1:]] for i in seqs])\n",
    "        target_id = self.pad_sequences(target_id_seqs, maxlen=max_len, value=0)\n",
    "\n",
    "        # 同target_id\n",
    "        target_correctness_seqs = np.array([[j[1] for j in i[1:]] for i in seqs])\n",
    "        target_correctness = self.pad_sequences(target_correctness_seqs, maxlen=max_len, value=0)\n",
    "\n",
    "        return dict(input_x=input_x, target_id=target_id, target_correctness=target_correctness,\n",
    "                    seq_len=seq_len, max_len=max_len)\n",
    "\n",
    "    def next_batch(self, seqs):\n",
    "        # 接收一个序列，生成batch\n",
    "\n",
    "        length = len(seqs)\n",
    "        num_batchs = length // self.batch_size\n",
    "        start = 0\n",
    "        for i in range(num_batchs):\n",
    "            batch_seqs = seqs[start: start + self.batch_size]\n",
    "            start += self.batch_size\n",
    "            params = self.format_data(batch_seqs)\n",
    "\n",
    "            yield params\n",
    "\n",
    "            \n",
    "fileName = \"./data/assistments.txt\"\n",
    "dataGen = DataGenerator(fileName, config)\n",
    "dataGen.gen_attr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skill num: 124\n",
      "train_seqs length: 3374\n",
      "test_seqs length: 843\n",
      "input_x shape: (32, 153, 248)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_seqs = dataGen.train_seqs\n",
    "params = next(dataGen.next_batch(train_seqs))\n",
    "print(\"skill num: {}\".format(len(dataGen.skills_to_int)))\n",
    "print(\"train_seqs length: {}\".format(len(dataGen.train_seqs)))\n",
    "print(\"test_seqs length: {}\".format(len(dataGen.test_seqs)))\n",
    "print(\"input_x shape: {}\".format(params['input_x'].shape))\n",
    "print(params[\"input_x\"][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class TensorFlowDKT(object):\n",
    "    def __init__(self, config):\n",
    "        # 导入配置好的参数\n",
    "        self.hiddens = hiddens = config.modelConfig.hidden_layers\n",
    "        self.num_skills = num_skills = config.num_skills\n",
    "        self.input_size = input_size = config.input_size\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.keep_prob_value = config.modelConfig.dropout_keep_prob\n",
    "\n",
    "        # 定义需要喂给模型的参数\n",
    "        self.max_steps = tf.placeholder(tf.int32, name=\"max_steps\")  # 当前batch中最大序列长度\n",
    "        self.input_data = tf.placeholder(tf.float32, [batch_size, None, input_size], name=\"input_x\")\n",
    "\n",
    "        self.sequence_len = tf.placeholder(tf.int32, [batch_size], name=\"sequence_len\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")  # dropout keep prob\n",
    "\n",
    "        self.target_id = tf.placeholder(tf.int32, [batch_size, None], name=\"target_id\")\n",
    "        self.target_correctness = tf.placeholder(tf.float32, [batch_size, None], name=\"target_correctness\")\n",
    "        self.flat_target_correctness = None\n",
    "\n",
    "        # 构建lstm模型结构\n",
    "        hidden_layers = []\n",
    "        for idx, hidden_size in enumerate(hiddens):\n",
    "            lstm_layer = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            hidden_layer = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_layer,\n",
    "                                                         output_keep_prob=self.keep_prob)\n",
    "            hidden_layers.append(hidden_layer)\n",
    "        self.hidden_cell = tf.nn.rnn_cell.MultiRNNCell(cells=hidden_layers, state_is_tuple=True)\n",
    "\n",
    "        # 采用动态rnn，动态输入序列的长度\n",
    "        outputs, self.current_state = tf.nn.dynamic_rnn(cell=self.hidden_cell,\n",
    "                                                        inputs=self.input_data,\n",
    "                                                        sequence_length=self.sequence_len,\n",
    "                                                        dtype=tf.float32)\n",
    "\n",
    "        # 隐层到输出层的权重系数[最后隐层的神经元数量，知识点数]\n",
    "        output_w = tf.get_variable(\"W\", [hiddens[-1], num_skills])\n",
    "        output_b = tf.get_variable(\"b\", [num_skills])\n",
    "\n",
    "        self.output = tf.reshape(outputs, [batch_size * self.max_steps, hiddens[-1]])\n",
    "        # 因为权值共享的原因，对生成的矩阵[batch_size * self.max_steps, num_skills]中的每一行都加上b\n",
    "        self.logits = tf.matmul(self.output, output_w) + output_b\n",
    "\n",
    "        self.mat_logits = tf.reshape(self.logits, [batch_size, self.max_steps, num_skills])\n",
    "\n",
    "        # 对每个batch中每个序列中的每个时间点的输出中的每个值进行sigmoid计算，这里的值表示对某个知识点的掌握情况，\n",
    "        # 每个时间点都会输出对所有知识点的掌握情况\n",
    "        self.pred_all = tf.sigmoid(self.mat_logits, name=\"pred_all\")\n",
    "\n",
    "        # 计算损失loss\n",
    "        flat_logits = tf.reshape(self.logits, [-1])\n",
    "\n",
    "        flat_target_correctness = tf.reshape(self.target_correctness, [-1])\n",
    "        self.flat_target_correctness = flat_target_correctness\n",
    "\n",
    "        flat_base_target_index = tf.range(batch_size * self.max_steps) * num_skills\n",
    "\n",
    "        # 因为flat_logits的长度为batch_size * num_steps * num_skills，我们要根据每一步的target_id将其长度变成batch_size * num_steps\n",
    "        flat_base_target_id = tf.reshape(self.target_id, [-1])\n",
    "\n",
    "        flat_target_id = flat_base_target_id + flat_base_target_index\n",
    "        # gather是从一个tensor中切片一个子集\n",
    "        flat_target_logits = tf.gather(flat_logits, flat_target_id)\n",
    "\n",
    "        # 对切片后的数据进行sigmoid转换\n",
    "        self.pred = tf.sigmoid(tf.reshape(flat_target_logits, [batch_size, self.max_steps]), name=\"pred\")\n",
    "        # 将sigmoid后的值表示为0或1\n",
    "        self.binary_pred = tf.cast(tf.greater_equal(self.pred, 0.5), tf.float32, name=\"binary_pred\")\n",
    "\n",
    "        # 定义损失函数\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # flat_target_logits_sigmoid = tf.nn.log_softmax(flat_target_logits)\n",
    "            # self.loss = -tf.reduce_mean(flat_target_correctness * flat_target_logits_sigmoid)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=flat_target_correctness,\n",
    "                                                                               logits=flat_target_logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def gen_metrics(sequence_len, binary_pred, pred, target_correctness):\n",
    "    \"\"\"\n",
    "    生成auc和accuracy的指标值\n",
    "    :param sequence_len: 每一个batch中各序列的长度组成的列表\n",
    "    :param binary_pred:\n",
    "    :param pred:\n",
    "    :param target_correctness:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    binary_preds = []\n",
    "    preds = []\n",
    "    target_correctnesses = []\n",
    "    for seq_idx, seq_len in enumerate(sequence_len):\n",
    "        binary_preds.append(binary_pred[seq_idx, :seq_len])\n",
    "        preds.append(pred[seq_idx, :seq_len])\n",
    "        target_correctnesses.append(target_correctness[seq_idx, :seq_len])\n",
    "\n",
    "    new_binary_pred = np.concatenate(binary_preds)\n",
    "    new_pred = np.concatenate(preds)\n",
    "    new_target_correctness = np.concatenate(target_correctnesses)\n",
    "\n",
    "    auc = roc_auc_score(new_target_correctness, new_pred)\n",
    "    accuracy = accuracy_score(new_target_correctness, new_binary_pred)\n",
    "    precision = precision_score(new_target_correctness, new_binary_pred)\n",
    "    recall = recall_score(new_target_correctness, new_binary_pred)\n",
    "\n",
    "    return auc, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using dkt/rnn/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dkt/W:0/grad/hist is illegal; using dkt/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dkt/W:0/grad/sparsity is illegal; using dkt/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dkt/b:0/grad/hist is illegal; using dkt/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dkt/b:0/grad/sparsity is illegal; using dkt/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangxinyang848/anaconda3/envs/jiang/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to /data4T/share/jiangxinyang848/dkt/runs/1546588631\n",
      "初始化完毕，开始训练\n",
      "train: 2019-01-04T15:57:12.891540: step 1, loss 0.6649993062019348, acc 0.55418480890179, auc: 0.5592457988797013, precision: 0.8141906873614191, recall: 0.5631901840490797\n",
      "train: 2019-01-04T15:57:14.277169: step 2, loss 0.6512970328330994, acc 0.7151464782879701, auc: 0.7212137920681752, precision: 0.7187121762285161, recall: 0.934235368156073\n",
      "train: 2019-01-04T15:57:15.152428: step 3, loss 0.6603468656539917, acc 0.5564334085778782, auc: 0.5053623836658082, precision: 0.6254653760238272, recall: 0.7479964381121995\n",
      "train: 2019-01-04T15:57:16.714156: step 4, loss 0.6549214720726013, acc 0.5791875855773619, auc: 0.5602000760642198, precision: 0.6704304869442484, recall: 0.6761565836298933\n",
      "train: 2019-01-04T15:57:17.870086: step 5, loss 0.6498287916183472, acc 0.5918482875742994, auc: 0.5252767258138828, precision: 0.6996792301523657, recall: 0.7157506152584086\n",
      "train: 2019-01-04T15:57:18.848666: step 6, loss 0.6481996774673462, acc 0.5923945335710041, auc: 0.5111233753719214, precision: 0.652228657768824, recall: 0.7927762473217018\n",
      "train: 2019-01-04T15:57:20.045639: step 7, loss 0.6358292698860168, acc 0.6268140981340705, auc: 0.5774775523932778, precision: 0.6627685613271689, recall: 0.8651047213347532\n",
      "train: 2019-01-04T15:57:21.333621: step 8, loss 0.6335188150405884, acc 0.6318559556786704, auc: 0.5456510302317297, precision: 0.6805229634596044, recall: 0.8437240232751455\n",
      "train: 2019-01-04T15:57:22.661886: step 9, loss 0.6362234950065613, acc 0.549512987012987, auc: 0.4349435281920991, precision: 0.6877101232723197, recall: 0.6895131086142322\n",
      "train: 2019-01-04T15:57:23.814159: step 10, loss 0.6195808053016663, acc 0.6738126649076517, auc: 0.5858367706547967, precision: 0.7126990003702333, recall: 0.9003741814780168\n",
      "train: 2019-01-04T15:57:24.216370: step 11, loss 0.6385108828544617, acc 0.5627906976744186, auc: 0.5805430167345054, precision: 0.5569823434991974, recall: 0.9028620988725065\n",
      "train: 2019-01-04T15:57:24.945921: step 12, loss 0.6181644201278687, acc 0.6051430941518042, auc: 0.6175634800829128, precision: 0.6265270506108203, recall: 0.7764960346070656\n",
      "train: 2019-01-04T15:57:27.783798: step 13, loss 0.6132564544677734, acc 0.5785958904109589, auc: 0.5253330682959243, precision: 0.7429034874290349, recall: 0.6453734147487084\n",
      "train: 2019-01-04T15:57:29.142612: step 14, loss 0.5994375944137573, acc 0.6866197183098591, auc: 0.6764129793854757, precision: 0.7634691195795007, recall: 0.7988084326306141\n",
      "train: 2019-01-04T15:57:30.352938: step 15, loss 0.6019908785820007, acc 0.6254310344827586, auc: 0.5734142378885384, precision: 0.6599362380446334, recall: 0.8443235893949694\n",
      "train: 2019-01-04T15:57:31.106648: step 16, loss 0.5959258079528809, acc 0.6505404014410705, auc: 0.6026274280382414, precision: 0.7250341997264022, recall: 0.7928197456993269\n",
      "train: 2019-01-04T15:57:32.507720: step 17, loss 0.5833973288536072, acc 0.7204561161022806, auc: 0.7175717448551473, precision: 0.7538523925385239, recall: 0.9019893255701116\n",
      "train: 2019-01-04T15:57:33.917946: step 18, loss 0.5908789038658142, acc 0.6266203703703703, auc: 0.6064431214991641, precision: 0.6722027972027972, recall: 0.8254025044722719\n",
      "train: 2019-01-04T15:57:34.934324: step 19, loss 0.5874905586242676, acc 0.6407706650093226, auc: 0.5579071235347159, precision: 0.6982823002240478, recall: 0.8431018935978359\n",
      "train: 2019-01-04T15:57:36.561726: step 20, loss 0.5733164548873901, acc 0.699566682715455, auc: 0.6822449721931032, precision: 0.7237091105704244, recall: 0.9221495005167069\n",
      "train: 2019-01-04T15:57:38.487123: step 21, loss 0.5658208727836609, acc 0.7509787234042553, auc: 0.6963796804601161, precision: 0.7729760332138139, recall: 0.9403122130394858\n",
      "train: 2019-01-04T15:57:39.805065: step 22, loss 0.5737107396125793, acc 0.6915373059589384, auc: 0.5988272703641392, precision: 0.7264908256880734, recall: 0.9011379800853485\n",
      "train: 2019-01-04T15:57:41.210442: step 23, loss 0.569153904914856, acc 0.6568583069262109, auc: 0.5926805841016068, precision: 0.7207258293166997, recall: 0.8272046859746176\n",
      "train: 2019-01-04T15:57:42.162453: step 24, loss 0.5540058612823486, acc 0.7265185185185186, auc: 0.6856845884742302, precision: 0.7474975783015821, recall: 0.9425895765472313\n",
      "train: 2019-01-04T15:57:46.014731: step 25, loss 0.5593082904815674, acc 0.6642365887207703, auc: 0.5775333748262488, precision: 0.6798933965057744, recall: 0.9427222336275919\n",
      "train: 2019-01-04T15:57:47.585643: step 26, loss 0.5493620038032532, acc 0.6856864179468423, auc: 0.7167584688162688, precision: 0.6986374565856265, recall: 0.9420028818443804\n",
      "train: 2019-01-04T15:57:49.626807: step 27, loss 0.5482551455497742, acc 0.6769881933615505, auc: 0.6826484217595702, precision: 0.686029764762362, recall: 0.9526666666666667\n",
      "train: 2019-01-04T15:57:50.626835: step 28, loss 0.5553146600723267, acc 0.6194073213248111, auc: 0.6255689327635119, precision: 0.6205443940375891, recall: 0.9323271665043817\n",
      "train: 2019-01-04T15:57:51.848529: step 29, loss 0.5373167395591736, acc 0.7190780465540849, auc: 0.6865959466186364, precision: 0.7404351087771943, recall: 0.9388078630310717\n",
      "train: 2019-01-04T15:57:55.109045: step 30, loss 0.5410811901092529, acc 0.6462124938514511, auc: 0.6591668382376232, precision: 0.6525500322788896, recall: 0.9645038167938931\n",
      "train: 2019-01-04T15:57:56.930228: step 31, loss 0.5294234752655029, acc 0.7213851440655564, auc: 0.7531036557806623, precision: 0.7158071748878924, recall: 0.9845797995373939\n",
      "train: 2019-01-04T15:57:58.574187: step 32, loss 0.5251958966255188, acc 0.7281337047353761, auc: 0.7522245871559634, precision: 0.7337423312883435, recall: 0.9568\n",
      "train: 2019-01-04T15:57:59.744833: step 33, loss 0.5160923600196838, acc 0.7740564858785304, auc: 0.7333053755407057, precision: 0.7990235964198535, recall: 0.9475715664200708\n",
      "train: 2019-01-04T15:58:00.976613: step 34, loss 0.5231925249099731, acc 0.6596126255380201, auc: 0.7093520503084438, precision: 0.665518725785622, recall: 0.89988358556461\n",
      "train: 2019-01-04T15:58:02.528751: step 35, loss 0.511877715587616, acc 0.7487932843651627, auc: 0.7536894599230749, precision: 0.7649289099526067, recall: 0.9402854646082144\n",
      "train: 2019-01-04T15:58:03.408162: step 36, loss 0.5193893909454346, acc 0.7232704402515723, auc: 0.6484143803744279, precision: 0.7441781977725278, recall: 0.9299873471109237\n",
      "train: 2019-01-04T15:58:04.630818: step 37, loss 0.5082966089248657, acc 0.7326629680998613, auc: 0.6783665416689987, precision: 0.747122168585221, recall: 0.9571836346336822\n",
      "train: 2019-01-04T15:58:05.665538: step 38, loss 0.5107213854789734, acc 0.7123158884362268, auc: 0.6320510909712789, precision: 0.730218281036835, recall: 0.9440035273368607\n",
      "train: 2019-01-04T15:58:07.133729: step 39, loss 0.512769341468811, acc 0.6545695950431512, auc: 0.6289386583682917, precision: 0.6606268364348677, recall: 0.939088061260007\n",
      "train: 2019-01-04T15:58:07.706947: step 40, loss 0.5135883092880249, acc 0.6393270241850684, auc: 0.6401617314444161, precision: 0.6401766004415012, recall: 0.9715242881072027\n",
      "train: 2019-01-04T15:58:09.262971: step 41, loss 0.4922371506690979, acc 0.7405489383738996, auc: 0.763373762175715, precision: 0.7446491717848501, recall: 0.968296224588577\n",
      "train: 2019-01-04T15:58:10.252955: step 42, loss 0.5112868547439575, acc 0.5934131736526946, auc: 0.6769291113345585, precision: 0.5789290681502086, recall: 0.9188741721854304\n",
      "train: 2019-01-04T15:58:11.709650: step 43, loss 0.4883778989315033, acc 0.6953010279001468, auc: 0.8198082991208867, precision: 0.6765445904389152, recall: 0.9649122807017544\n",
      "train: 2019-01-04T15:58:12.538865: step 44, loss 0.49183812737464905, acc 0.6978998384491115, auc: 0.6829483491087982, precision: 0.707764705882353, recall: 0.9221336603310852\n",
      "train: 2019-01-04T15:58:14.343101: step 45, loss 0.46839404106140137, acc 0.7915702904137754, auc: 0.8212192888799295, precision: 0.8172092312394926, recall: 0.926208210635718\n",
      "train: 2019-01-04T15:58:16.247835: step 46, loss 0.48397913575172424, acc 0.7014544526664965, auc: 0.7329371099326644, precision: 0.7107413893753649, recall: 0.931522570772762\n",
      "train: 2019-01-04T15:58:18.054108: step 47, loss 0.4779048264026642, acc 0.7026647966339411, auc: 0.7273217104324307, precision: 0.7130801687763713, recall: 0.9095801937567277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T15:58:18.999843: step 48, loss 0.4762658476829529, acc 0.7151714077315828, auc: 0.7181867295319483, precision: 0.7369353410097431, recall: 0.8989735278227985\n",
      "train: 2019-01-04T15:58:20.171124: step 49, loss 0.4784357249736786, acc 0.6981175923774111, auc: 0.6922217400003674, precision: 0.7173243772739994, recall: 0.8986676016830295\n",
      "train: 2019-01-04T15:58:21.560071: step 50, loss 0.46058666706085205, acc 0.7652120467117394, auc: 0.7640798273361131, precision: 0.8028134853262188, recall: 0.9085918199286303\n",
      "train: 2019-01-04T15:58:22.580115: step 51, loss 0.4674523174762726, acc 0.6918044077134986, auc: 0.7021816831830733, precision: 0.7109621451104101, recall: 0.917557251908397\n",
      "train: 2019-01-04T15:58:23.534580: step 52, loss 0.46458229422569275, acc 0.723998653651969, auc: 0.7074849397590361, precision: 0.745021475985943, recall: 0.9195180722891566\n",
      "train: 2019-01-04T15:58:24.884757: step 53, loss 0.4617909789085388, acc 0.7369510232237296, auc: 0.7346573485665343, precision: 0.7433920704845814, recall: 0.9271978021978022\n",
      "train: 2019-01-04T15:58:25.872961: step 54, loss 0.45508328080177307, acc 0.7254098360655737, auc: 0.6906094466910335, precision: 0.7333722287047841, recall: 0.9408682634730539\n",
      "train: 2019-01-04T15:58:26.479197: step 55, loss 0.4709426462650299, acc 0.6705165222970805, auc: 0.7692673203938369, precision: 0.6432058584214809, recall: 0.9133448873483535\n",
      "train: 2019-01-04T15:58:27.191535: step 56, loss 0.43783336877822876, acc 0.80763299922899, auc: 0.841489181156444, precision: 0.8222438076736279, recall: 0.9271631982475356\n",
      "train: 2019-01-04T15:58:28.215817: step 57, loss 0.4500141143798828, acc 0.6762849413886384, auc: 0.6742346071697742, precision: 0.701617735170761, recall: 0.8418404025880661\n",
      "train: 2019-01-04T15:58:28.681583: step 58, loss 0.4672403335571289, acc 0.6350288411116938, auc: 0.6379210557462421, precision: 0.6682027649769585, recall: 0.8409279204639603\n",
      "train: 2019-01-04T15:58:30.356887: step 59, loss 0.41876015067100525, acc 0.844039971448965, auc: 0.9037709088003409, precision: 0.8537898363479759, recall: 0.953125\n",
      "train: 2019-01-04T15:58:31.220302: step 60, loss 0.444674015045166, acc 0.7300242130750605, auc: 0.771368948413291, precision: 0.7358708189158016, recall: 0.9032562529495045\n",
      "train: 2019-01-04T15:58:32.202554: step 61, loss 0.4139553904533386, acc 0.8465753424657534, auc: 0.9139282290917804, precision: 0.8539748953974895, recall: 0.9537383177570093\n",
      "train: 2019-01-04T15:58:33.212454: step 62, loss 0.44423097372055054, acc 0.6735768344850279, auc: 0.6731651579054215, precision: 0.687980574666127, recall: 0.8849557522123894\n",
      "train: 2019-01-04T15:58:34.432726: step 63, loss 0.43439656496047974, acc 0.6855775803144224, auc: 0.718551299733508, precision: 0.6877240143369175, recall: 0.873151308304892\n",
      "train: 2019-01-04T15:58:36.015426: step 64, loss 0.4169299006462097, acc 0.7671713846829055, auc: 0.8585925893700537, precision: 0.778125, recall: 0.9072540576349785\n",
      "train: 2019-01-04T15:58:36.749858: step 65, loss 0.4241308867931366, acc 0.7513487003433056, auc: 0.7541668624515852, precision: 0.7718978102189781, recall: 0.9057815845824411\n",
      "train: 2019-01-04T15:58:38.317222: step 66, loss 0.4115550220012665, acc 0.7730715287517531, auc: 0.8204730861792968, precision: 0.7955465587044535, recall: 0.9207340882467786\n",
      "train: 2019-01-04T15:58:45.656668: step 67, loss 0.4036789536476135, acc 0.8709229701596114, auc: 0.8734035012082932, precision: 0.8777021617293835, recall: 0.9705621956617972\n",
      "train: 2019-01-04T15:58:46.529715: step 68, loss 0.4257388710975647, acc 0.73297692740574, auc: 0.689208689637827, precision: 0.7662809917355372, recall: 0.90546875\n",
      "train: 2019-01-04T15:58:47.751665: step 69, loss 0.4185705780982971, acc 0.6915947587088527, auc: 0.7250253907229999, precision: 0.692063492063492, recall: 0.9022245214692188\n",
      "train: 2019-01-04T15:58:49.582368: step 70, loss 0.39872461557388306, acc 0.7958211706867414, auc: 0.8668478254913011, precision: 0.8097965633023899, recall: 0.9180474697716077\n",
      "train: 2019-01-04T15:58:50.258868: step 71, loss 0.40934038162231445, acc 0.7422528283325135, auc: 0.7410755558851521, precision: 0.8041561712846348, recall: 0.8570469798657718\n",
      "train: 2019-01-04T15:58:51.440394: step 72, loss 0.40980905294418335, acc 0.7406849020030153, auc: 0.7697241260718901, precision: 0.7524986849026828, recall: 0.9158130601792573\n",
      "train: 2019-01-04T15:58:53.033293: step 73, loss 0.40182438492774963, acc 0.735683920980245, auc: 0.8271637535913079, precision: 0.6942537043729671, recall: 0.901031894934334\n",
      "train: 2019-01-04T15:58:54.566985: step 74, loss 0.3926764726638794, acc 0.8055444754686877, auc: 0.7864355228834712, precision: 0.8305747126436782, recall: 0.9381978706829395\n",
      "train: 2019-01-04T15:58:56.022979: step 75, loss 0.3969670236110687, acc 0.7433090024330901, auc: 0.7928093112244897, precision: 0.7495169082125603, recall: 0.9312725090036015\n",
      "train: 2019-01-04T15:58:57.238039: step 76, loss 0.40744319558143616, acc 0.7048458149779736, auc: 0.6995055625317455, precision: 0.7084485407066052, recall: 0.9238782051282052\n",
      "train: 2019-01-04T15:58:58.907921: step 77, loss 0.38247790932655334, acc 0.7947067238912733, auc: 0.8497592469210993, precision: 0.7888124439126533, recall: 0.9444842406876791\n",
      "train: 2019-01-04T15:58:59.176991: step 78, loss 0.4187895357608795, acc 0.7178770949720671, auc: 0.7177156584407075, precision: 0.7415929203539823, recall: 0.8821052631578947\n",
      "train: 2019-01-04T15:59:00.369722: step 79, loss 0.39709192514419556, acc 0.7153163152053275, auc: 0.7262995756590371, precision: 0.7230079337702656, recall: 0.9038378611470461\n",
      "train: 2019-01-04T15:59:01.132821: step 80, loss 0.3860481381416321, acc 0.7821403752605977, auc: 0.8128688092652829, precision: 0.7968617472434266, recall: 0.9269856931425753\n",
      "train: 2019-01-04T15:59:02.139681: step 81, loss 0.37871578335762024, acc 0.7786330049261084, auc: 0.7968185231783558, precision: 0.8165971959075408, recall: 0.9016736401673641\n",
      "train: 2019-01-04T15:59:03.326258: step 82, loss 0.38273683190345764, acc 0.7513684210526316, auc: 0.7796125913867203, precision: 0.776358627598297, recall: 0.9149940968122786\n",
      "train: 2019-01-04T15:59:04.314803: step 83, loss 0.3720589280128479, acc 0.7775974025974026, auc: 0.8375230563405925, precision: 0.7937474624441738, recall: 0.9169793621013134\n",
      "train: 2019-01-04T15:59:06.059188: step 84, loss 0.3773345351219177, acc 0.6366830639494027, auc: 0.7138485728526596, precision: 0.605089058524173, recall: 0.8217000691085004\n",
      "train: 2019-01-04T15:59:07.677037: step 85, loss 0.38483989238739014, acc 0.6429863084474297, auc: 0.660555925132021, precision: 0.6554469779225479, recall: 0.8081213743864346\n",
      "train: 2019-01-04T15:59:08.727001: step 86, loss 0.3748839497566223, acc 0.7517795637198622, auc: 0.7662078616999383, precision: 0.8009545199326221, recall: 0.8846511627906977\n",
      "train: 2019-01-04T15:59:10.246778: step 87, loss 0.3591032326221466, acc 0.7817531305903399, auc: 0.8636095605618072, precision: 0.7991017964071856, recall: 0.8974445191661062\n",
      "train: 2019-01-04T15:59:12.019761: step 88, loss 0.3727169930934906, acc 0.6606076343806804, auc: 0.7199459813345686, precision: 0.6531187122736418, recall: 0.784816247582205\n",
      "train: 2019-01-04T15:59:13.184604: step 89, loss 0.3772197365760803, acc 0.6988603988603989, auc: 0.6798701938240128, precision: 0.7568671121009651, recall: 0.8353133961491193\n",
      "train: 2019-01-04T15:59:14.642621: step 90, loss 0.34379109740257263, acc 0.8299062049062049, auc: 0.8974321340892633, precision: 0.858276394093013, recall: 0.9284692417739628\n",
      "train: 2019-01-04T15:59:15.883266: step 91, loss 0.35173770785331726, acc 0.7945879458794588, auc: 0.8421476105619727, precision: 0.8135973104221144, recall: 0.927993182786536\n",
      "train: 2019-01-04T15:59:16.187199: step 92, loss 0.4094388782978058, acc 0.6705750977107761, auc: 0.7152811015794707, precision: 0.6878228782287823, recall: 0.8480436760691538\n",
      "train: 2019-01-04T15:59:16.518656: step 93, loss 0.40346547961235046, acc 0.6936656282450675, auc: 0.7343612954800065, precision: 0.6954140999315537, recall: 0.8751076658053403\n",
      "train: 2019-01-04T15:59:18.176106: step 94, loss 0.35571157932281494, acc 0.6665541680728991, auc: 0.7281653725568492, precision: 0.6305858987090367, recall: 0.8388375165125496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T15:59:27.301158: step 95, loss 0.33343759179115295, acc 0.9041342121030557, auc: 0.9294695085185142, precision: 0.9158997722095672, recall: 0.9657027572293208\n",
      "train: 2019-01-04T15:59:28.165821: step 96, loss 0.34603238105773926, acc 0.7722457627118644, auc: 0.7786174408540941, precision: 0.827891156462585, recall: 0.8730272596843616\n",
      "train: 2019-01-04T15:59:28.430915: step 97, loss 0.3814060688018799, acc 0.7122676579925651, auc: 0.7078177278401999, precision: 0.7487875848690592, recall: 0.8577777777777778\n",
      "train: 2019-01-04T15:59:29.009505: step 98, loss 0.37157732248306274, acc 0.6977186311787072, auc: 0.7095417506352404, precision: 0.7246184472461845, recall: 0.8316831683168316\n",
      "train: 2019-01-04T15:59:30.565164: step 99, loss 0.35502901673316956, acc 0.6731681602530311, auc: 0.6582868485511423, precision: 0.6997667444185272, recall: 0.8610086100861009\n",
      "train: 2019-01-04T15:59:33.259114: step 100, loss 0.31888821721076965, acc 0.8605289421157685, auc: 0.9448328326173767, precision: 0.8665480427046264, recall: 0.9297441771668575\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T15:59:45.742692, step: 100, loss: 0.3442270721380527, acc: 0.7544095282824337, auc: 0.7821470450972814, precision: 0.770528533708399, recall: 0.8975237642771297\n",
      "Saved model checkpoint to model/my-model-100\n",
      "\n",
      "train: 2019-01-04T15:59:47.145857: step 101, loss 0.33094730973243713, acc 0.7901134521880064, auc: 0.8513480381736294, precision: 0.8071689346166612, recall: 0.9254185692541856\n",
      "train: 2019-01-04T15:59:48.423164: step 102, loss 0.34635448455810547, acc 0.6538461538461539, auc: 0.6833489987928381, precision: 0.6801099908340972, recall: 0.8109289617486338\n",
      "train: 2019-01-04T15:59:50.594140: step 103, loss 0.3209507167339325, acc 0.8280254777070064, auc: 0.9004972946006119, precision: 0.8235103626943006, recall: 0.9233841684822077\n",
      "train: 2019-01-04T15:59:51.978583: step 104, loss 0.33922120928764343, acc 0.7013433302093096, auc: 0.6919412405776042, precision: 0.743921568627451, recall: 0.8622727272727273\n",
      "train: 2019-01-04T15:59:53.322328: step 105, loss 0.32823342084884644, acc 0.8043129566922117, auc: 0.836009941964946, precision: 0.8407217643127646, recall: 0.9078662496993024\n",
      "train: 2019-01-04T15:59:54.335904: step 106, loss 0.3472921550273895, acc 0.7444499259990133, auc: 0.748735223041534, precision: 0.7791310072416063, recall: 0.866398243045388\n",
      "train: 2019-01-04T15:59:56.195121: step 107, loss 0.31313356757164, acc 0.8288850087303566, auc: 0.8793606992537992, precision: 0.8518635502210992, recall: 0.925531914893617\n",
      "train: 2019-01-04T15:59:57.553005: step 108, loss 0.3310692608356476, acc 0.7382753403933434, auc: 0.8082933011785689, precision: 0.7452100840336134, recall: 0.8878654385262315\n",
      "train: 2019-01-04T15:59:58.653003: step 109, loss 0.3313882052898407, acc 0.7448509068552106, auc: 0.7379576811488359, precision: 0.7694704049844237, recall: 0.8925022583559169\n",
      "train: 2019-01-04T15:59:59.414921: step 110, loss 0.33353936672210693, acc 0.7186086956521739, auc: 0.7773153605463993, precision: 0.7527624309392266, recall: 0.8573675930781332\n",
      "train: 2019-01-04T16:00:00.611702: step 111, loss 0.31842896342277527, acc 0.8056122448979591, auc: 0.7358464970642896, precision: 0.8326359832635983, recall: 0.9485224022878932\n",
      "train: 2019-01-04T16:00:01.944797: step 112, loss 0.3358081579208374, acc 0.6715470961444607, auc: 0.6937979664990825, precision: 0.6769327467001885, recall: 0.8713592233009708\n",
      "train: 2019-01-04T16:00:03.466914: step 113, loss 0.3265431523323059, acc 0.6737530662305805, auc: 0.6993944448923147, precision: 0.6777933942375264, recall: 0.87324581258488\n",
      "train: 2019-01-04T16:00:05.711556: step 114, loss 0.29827699065208435, acc 0.8404078728954233, auc: 0.9136849206570321, precision: 0.841399607586658, recall: 0.9319087287214777\n",
      "train: 2019-01-04T16:00:07.189453: step 115, loss 0.32050707936286926, acc 0.6949286846275753, auc: 0.704865098757734, precision: 0.7185209860093271, recall: 0.8743413052290231\n",
      "train: 2019-01-04T16:00:07.596445: step 116, loss 0.3414532244205475, acc 0.6483669214732453, auc: 0.6823524944642957, precision: 0.6795005202913632, recall: 0.7673325499412456\n",
      "train: 2019-01-04T16:00:11.199876: step 117, loss 0.30134016275405884, acc 0.7321643458827575, auc: 0.8390680288934769, precision: 0.7257155247181266, recall: 0.9195054945054945\n",
      "train: 2019-01-04T16:00:12.688391: step 118, loss 0.315246045589447, acc 0.6748137108792847, auc: 0.7031302366869265, precision: 0.6945098039215686, recall: 0.8502160345655305\n",
      "train: 2019-01-04T16:00:13.586286: step 119, loss 0.33071643114089966, acc 0.677070457354759, auc: 0.7029115955073768, precision: 0.6966653274407393, recall: 0.8567193675889329\n",
      "train: 2019-01-04T16:00:14.627153: step 120, loss 0.30788561701774597, acc 0.7704494018752021, auc: 0.8019515500758096, precision: 0.7888527257933279, recall: 0.9103286384976526\n",
      "train: 2019-01-04T16:00:15.758566: step 121, loss 0.3149198889732361, acc 0.7109917877447883, auc: 0.7081953270860262, precision: 0.7318587504850601, recall: 0.8938388625592417\n",
      "train: 2019-01-04T16:00:17.358272: step 122, loss 0.27880603075027466, acc 0.8762120232708468, auc: 0.9362835650208655, precision: 0.8939984411535464, recall: 0.9538461538461539\n",
      "train: 2019-01-04T16:00:18.123847: step 123, loss 0.3052982687950134, acc 0.7572209211553473, auc: 0.8153464437772764, precision: 0.7578084997439836, recall: 0.9085328422344997\n",
      "train: 2019-01-04T16:00:19.196044: step 124, loss 0.3227737843990326, acc 0.7143181818181819, auc: 0.6746471525560738, precision: 0.7376705141657922, recall: 0.9162593678722711\n",
      "train: 2019-01-04T16:00:21.054816: step 125, loss 0.2972269356250763, acc 0.6999090633525311, auc: 0.7510743809817657, precision: 0.6909888357256778, recall: 0.8896303901437371\n",
      "train: 2019-01-04T16:00:30.362960: step 126, loss 0.2730104923248291, acc 0.9205178501373088, auc: 0.9483292814624239, precision: 0.9241424188900251, recall: 0.9805681594002762\n",
      "train: 2019-01-04T16:00:31.484130: step 127, loss 0.28453779220581055, acc 0.8108031674208145, auc: 0.8898930682995393, precision: 0.8148558758314856, recall: 0.9292035398230089\n",
      "train: 2019-01-04T16:00:32.665265: step 128, loss 0.3079541027545929, acc 0.6750585480093677, auc: 0.6983089897456323, precision: 0.6814678899082569, recall: 0.8847070033349214\n",
      "train: 2019-01-04T16:00:34.269698: step 129, loss 0.27615341544151306, acc 0.8282943525385055, auc: 0.9148664814644796, precision: 0.8354157690650582, recall: 0.8981009726725335\n",
      "train: 2019-01-04T16:00:35.353200: step 130, loss 0.3072890043258667, acc 0.6675385062481837, auc: 0.6988826304945538, precision: 0.6903279369032793, recall: 0.8068898592916061\n",
      "train: 2019-01-04T16:00:35.818270: step 131, loss 0.31376609206199646, acc 0.6828087167070218, auc: 0.73639259851406, precision: 0.684154175588865, recall: 0.7361751152073732\n",
      "train: 2019-01-04T16:00:37.851382: step 132, loss 0.267703115940094, acc 0.843915343915344, auc: 0.912438431510294, precision: 0.8572561459159397, recall: 0.9345821325648415\n",
      "train: 2019-01-04T16:00:39.551895: step 133, loss 0.2731449604034424, acc 0.8225716928769657, auc: 0.8845143538934306, precision: 0.8420217588395286, recall: 0.934121196882072\n",
      "train: 2019-01-04T16:00:40.924614: step 134, loss 0.269184947013855, acc 0.8364500792393027, auc: 0.8751318104315249, precision: 0.828228476821192, recall: 0.9519505233111323\n",
      "train: 2019-01-04T16:00:42.564908: step 135, loss 0.2984760105609894, acc 0.6936336924583741, auc: 0.7031175772267613, precision: 0.7111886369299776, recall: 0.8757287511506597\n",
      "train: 2019-01-04T16:00:43.741090: step 136, loss 0.30593472719192505, acc 0.6941681735985533, auc: 0.6888234966949369, precision: 0.7148571428571429, recall: 0.8757437871893594\n",
      "train: 2019-01-04T16:00:44.133104: step 137, loss 0.2956986129283905, acc 0.6570327552986512, auc: 0.7282861037750217, precision: 0.6223662884927067, recall: 0.757396449704142\n",
      "train: 2019-01-04T16:00:45.504978: step 138, loss 0.2711779475212097, acc 0.7994632837277384, auc: 0.863276032649023, precision: 0.8227228207639569, recall: 0.9003215434083601\n",
      "train: 2019-01-04T16:00:46.600151: step 139, loss 0.28276321291923523, acc 0.7640034119988627, auc: 0.8405193116533323, precision: 0.7705064993276558, recall: 0.8438880706921944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:00:47.122343: step 140, loss 0.30807214975357056, acc 0.737490377213241, auc: 0.7613077849756877, precision: 0.7783585990590696, recall: 0.8523182598740698\n",
      "train: 2019-01-04T16:00:48.152935: step 141, loss 0.2701229453086853, acc 0.812448132780083, auc: 0.8486853836330623, precision: 0.8324118866620595, recall: 0.925826287471176\n",
      "train: 2019-01-04T16:00:49.259717: step 142, loss 0.2753676474094391, acc 0.6902268760907504, auc: 0.6870523232592197, precision: 0.7072120559741658, recall: 0.8878378378378379\n",
      "train: 2019-01-04T16:00:50.017256: step 143, loss 0.30024442076683044, acc 0.6879090595935239, auc: 0.7467521731603752, precision: 0.6878048780487804, recall: 0.8412887828162291\n",
      "train: 2019-01-04T16:00:51.483599: step 144, loss 0.27432698011398315, acc 0.7447161210111893, auc: 0.8295283119680359, precision: 0.7206703910614525, recall: 0.8749058025621703\n",
      "train: 2019-01-04T16:00:52.341019: step 145, loss 0.27109575271606445, acc 0.7524707996406109, auc: 0.7509244317729552, precision: 0.7593732512590935, recall: 0.9181326116373477\n",
      "train: 2019-01-04T16:00:53.582806: step 146, loss 0.26702162623405457, acc 0.739288307915759, auc: 0.742080239707738, precision: 0.7515865820489573, recall: 0.9070021881838074\n",
      "train: 2019-01-04T16:00:56.610615: step 147, loss 0.24956373870372772, acc 0.8368313081899953, auc: 0.8787159665162798, precision: 0.8529469355786176, recall: 0.9389954731623195\n",
      "train: 2019-01-04T16:00:57.328217: step 148, loss 0.3013661205768585, acc 0.665266106442577, auc: 0.7110006850403132, precision: 0.6765163297045101, recall: 0.7971899816737935\n",
      "train: 2019-01-04T16:00:59.081784: step 149, loss 0.262370765209198, acc 0.7652414885193982, auc: 0.8113002355013498, precision: 0.7968058968058968, recall: 0.9003331482509717\n",
      "train: 2019-01-04T16:01:00.667166: step 150, loss 0.26004835963249207, acc 0.758770924099977, auc: 0.7912285476893104, precision: 0.7898332877835474, recall: 0.9108099590293098\n",
      "train: 2019-01-04T16:01:02.412414: step 151, loss 0.26240360736846924, acc 0.7736539466806064, auc: 0.842854686183458, precision: 0.7775398524863193, recall: 0.8997797356828194\n",
      "train: 2019-01-04T16:01:03.801510: step 152, loss 0.2440463900566101, acc 0.8106087575649698, auc: 0.8953029221478181, precision: 0.8274161735700197, recall: 0.9021505376344086\n",
      "train: 2019-01-04T16:01:05.173473: step 153, loss 0.26715773344039917, acc 0.7644379668427764, auc: 0.7709014837535659, precision: 0.7871707731520816, recall: 0.9271953965474106\n",
      "train: 2019-01-04T16:01:06.660205: step 154, loss 0.259328156709671, acc 0.7964349376114082, auc: 0.8279313644630547, precision: 0.8008493518104605, recall: 0.9345331246739698\n",
      "train: 2019-01-04T16:01:08.112512: step 155, loss 0.24433493614196777, acc 0.8226718885987816, auc: 0.87414805683052, precision: 0.8463136033229491, recall: 0.9359747344243469\n",
      "train: 2019-01-04T16:01:09.032511: step 156, loss 0.2680513858795166, acc 0.7315541601255887, auc: 0.7161911921776658, precision: 0.7648419429452583, recall: 0.8900852400179453\n",
      "train: 2019-01-04T16:01:09.922222: step 157, loss 0.2528951168060303, acc 0.8078799249530957, auc: 0.8103052717220143, precision: 0.831511839708561, recall: 0.9278455284552846\n",
      "train: 2019-01-04T16:01:11.836677: step 158, loss 0.24458639323711395, acc 0.7585908796196239, auc: 0.8517581394137147, precision: 0.7665615141955836, recall: 0.8981854838709677\n",
      "train: 2019-01-04T16:01:12.667603: step 159, loss 0.2803442180156708, acc 0.7328529490258874, auc: 0.7576670178830702, precision: 0.730459165790396, recall: 0.8998272884283247\n",
      "train: 2019-01-04T16:01:13.783712: step 160, loss 0.260649710893631, acc 0.7406774836322232, auc: 0.732013256742254, precision: 0.7620981387478849, recall: 0.9154471544715447\n",
      "train: 2019-01-04T16:01:14.724364: step 161, loss 0.27895841002464294, acc 0.7015840572304548, auc: 0.6841372066532208, precision: 0.73502722323049, recall: 0.8927259368111683\n",
      "train: 2019-01-04T16:01:15.713377: step 162, loss 0.24732515215873718, acc 0.7594011142061281, auc: 0.820014192710965, precision: 0.7936434822662367, recall: 0.8763987792472024\n",
      "train: 2019-01-04T16:01:16.618957: step 163, loss 0.2557356655597687, acc 0.7107474568774879, auc: 0.7092035134471755, precision: 0.7341072415699281, recall: 0.8847435043304464\n",
      "train: 2019-01-04T16:01:17.918697: step 164, loss 0.2743207514286041, acc 0.7137895812053116, auc: 0.7387884461028453, precision: 0.7227827934147637, recall: 0.8840532640467684\n",
      "train: 2019-01-04T16:01:18.910189: step 165, loss 0.25273561477661133, acc 0.7606611570247934, auc: 0.7876288046587774, precision: 0.7601908065915004, recall: 0.9111226611226612\n",
      "train: 2019-01-04T16:01:20.472239: step 166, loss 0.25760146975517273, acc 0.7100731812311666, auc: 0.7487574485373782, precision: 0.7076307516433267, recall: 0.8842857142857142\n",
      "train: 2019-01-04T16:01:21.981495: step 167, loss 0.2527426481246948, acc 0.6987745098039215, auc: 0.6903573893404691, precision: 0.7171165996553704, recall: 0.9109813936519519\n",
      "train: 2019-01-04T16:01:23.521026: step 168, loss 0.23386259377002716, acc 0.7476525821596244, auc: 0.8324705615942029, precision: 0.7620528771384136, recall: 0.8876811594202898\n",
      "train: 2019-01-04T16:01:24.861615: step 169, loss 0.22278529405593872, acc 0.8660355029585799, auc: 0.9123937324226596, precision: 0.8886657101865136, recall: 0.945648854961832\n",
      "train: 2019-01-04T16:01:26.386980: step 170, loss 0.22657373547554016, acc 0.8490636009642129, auc: 0.9113537960894624, precision: 0.8825059101654846, recall: 0.9217283950617284\n",
      "train: 2019-01-04T16:01:28.293721: step 171, loss 0.21091659367084503, acc 0.8948315762406789, auc: 0.9612654069300889, precision: 0.9245350500715308, recall: 0.9288537549407114\n",
      "train: 2019-01-04T16:01:29.517802: step 172, loss 0.25666821002960205, acc 0.7730685017203628, auc: 0.7948192293986764, precision: 0.8057908213511916, recall: 0.8979367866549605\n",
      "train: 2019-01-04T16:01:31.224859: step 173, loss 0.249246284365654, acc 0.7755404204915606, auc: 0.7395645556843542, precision: 0.8079201218480284, recall: 0.9260911736178468\n",
      "train: 2019-01-04T16:01:32.604932: step 174, loss 0.2478644996881485, acc 0.7255096966683242, auc: 0.7230875127740188, precision: 0.7383411580594679, recall: 0.8979824895317853\n",
      "train: 2019-01-04T16:01:32.866274: step 175, loss 0.2836134731769562, acc 0.6669491525423729, auc: 0.7320535352169799, precision: 0.6493150684931507, recall: 0.7757774140752864\n",
      "train: 2019-01-04T16:01:34.105138: step 176, loss 0.2615106701850891, acc 0.6943015840362066, auc: 0.7322356217198094, precision: 0.7015409570154095, recall: 0.8716829022505879\n",
      "train: 2019-01-04T16:01:35.369377: step 177, loss 0.23285305500030518, acc 0.7181818181818181, auc: 0.7364240330763573, precision: 0.7414658634538153, recall: 0.865767878077374\n",
      "train: 2019-01-04T16:01:36.544728: step 178, loss 0.2240123599767685, acc 0.7760869565217391, auc: 0.7092763316093429, precision: 0.8076728924785462, recall: 0.922722029988466\n",
      "train: 2019-01-04T16:01:37.776463: step 179, loss 0.22969606518745422, acc 0.6642335766423357, auc: 0.6992279038790665, precision: 0.689922480620155, recall: 0.8153629316420014\n",
      "train: 2019-01-04T16:01:39.637882: step 180, loss 0.22750650346279144, acc 0.7964699393270822, auc: 0.8075459000208328, precision: 0.8224480968858131, recall: 0.9300562484715089\n",
      "train: 2019-01-04T16:01:41.088612: step 181, loss 0.23510611057281494, acc 0.668814055636896, auc: 0.7085598297681839, precision: 0.6839464882943144, recall: 0.8135256091496768\n",
      "train: 2019-01-04T16:01:42.010476: step 182, loss 0.23820042610168457, acc 0.7210965435041716, auc: 0.7039115040387369, precision: 0.7408464098906324, recall: 0.9084548104956268\n",
      "train: 2019-01-04T16:01:43.186401: step 183, loss 0.22458122670650482, acc 0.6995846792801107, auc: 0.7049724009201596, precision: 0.7256484149855907, recall: 0.8779637377963738\n",
      "train: 2019-01-04T16:01:44.171052: step 184, loss 0.21162351965904236, acc 0.8195897865215571, auc: 0.8514665938742045, precision: 0.8414334806087383, recall: 0.9407244785949506\n",
      "train: 2019-01-04T16:01:45.574507: step 185, loss 0.2345302551984787, acc 0.7662009762666218, auc: 0.8387031374777146, precision: 0.7757945145842403, recall: 0.9084884017333673\n",
      "train: 2019-01-04T16:01:46.904026: step 186, loss 0.21018631756305695, acc 0.8476738369184592, auc: 0.9138423823470126, precision: 0.8552331268340397, recall: 0.9408177905308465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:01:48.710415: step 187, loss 0.2195623815059662, acc 0.7514906303236797, auc: 0.8258588127201265, precision: 0.7552570093457944, recall: 0.8871355060034305\n",
      "train: 2019-01-04T16:01:49.805682: step 188, loss 0.22704535722732544, acc 0.7577068685776095, auc: 0.7984653109586964, precision: 0.7935858068918458, recall: 0.8888039740160489\n",
      "train: 2019-01-04T16:01:50.937196: step 189, loss 0.23451527953147888, acc 0.753687660969328, auc: 0.8000901514910854, precision: 0.7695924764890282, recall: 0.8856421356421357\n",
      "train: 2019-01-04T16:01:51.907406: step 190, loss 0.22662858664989471, acc 0.6235616438356164, auc: 0.6789980185437146, precision: 0.622568093385214, recall: 0.7984031936127745\n",
      "train: 2019-01-04T16:01:53.385565: step 191, loss 0.23115701973438263, acc 0.7609092417454787, auc: 0.8018438973815591, precision: 0.7749793559042114, recall: 0.9144945188794154\n",
      "train: 2019-01-04T16:01:54.952817: step 192, loss 0.20152547955513, acc 0.873011403632268, auc: 0.92704703132377, precision: 0.8869233437391606, recall: 0.953392990305742\n",
      "train: 2019-01-04T16:01:55.742259: step 193, loss 0.23387770354747772, acc 0.7065435568044962, auc: 0.6944820503919696, precision: 0.7329974811083123, recall: 0.8786231884057971\n",
      "train: 2019-01-04T16:01:57.456659: step 194, loss 0.21484392881393433, acc 0.8333576110706482, auc: 0.8785559354021268, precision: 0.8424581005586592, recall: 0.9381999170468686\n",
      "train: 2019-01-04T16:02:00.385150: step 195, loss 0.1914910525083542, acc 0.8555815768930523, auc: 0.9285599502650257, precision: 0.8611838658983761, recall: 0.9399656946826758\n",
      "train: 2019-01-04T16:02:02.168087: step 196, loss 0.19507454335689545, acc 0.8533048026628626, auc: 0.9062669869349609, precision: 0.8723026899201892, recall: 0.9410076530612245\n",
      "train: 2019-01-04T16:02:03.404815: step 197, loss 0.23452945053577423, acc 0.7558247903075489, auc: 0.7494651751281212, precision: 0.7775109170305677, recall: 0.9244548286604362\n",
      "train: 2019-01-04T16:02:04.243416: step 198, loss 0.22492648661136627, acc 0.7282608695652174, auc: 0.7413741169055954, precision: 0.744196843082637, recall: 0.8895671476137624\n",
      "train: 2019-01-04T16:02:04.868773: step 199, loss 0.2460458129644394, acc 0.7025695177754312, auc: 0.7592686712296832, precision: 0.7150943396226415, recall: 0.8628343767785999\n",
      "train: 2019-01-04T16:02:12.211073: step 200, loss 0.1852722465991974, acc 0.8945250272490987, auc: 0.9328667353226973, precision: 0.9039728488720303, recall: 0.9683490162532079\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:02:25.133862, step: 200, loss: 0.21619637539753547, acc: 0.7672264584425936, auc: 0.7997905258487208, precision: 0.7809138951853807, recall: 0.9032355516509958\n",
      "Saved model checkpoint to model/my-model-200\n",
      "\n",
      "train: 2019-01-04T16:02:26.759016: step 201, loss 0.2048313170671463, acc 0.7474747474747475, auc: 0.8201767602710958, precision: 0.7591822591822592, recall: 0.8957481602616517\n",
      "train: 2019-01-04T16:02:29.988434: step 202, loss 0.19769170880317688, acc 0.7972508591065293, auc: 0.8722537603179873, precision: 0.789594651065608, recall: 0.9023400191021967\n",
      "train: 2019-01-04T16:02:31.954234: step 203, loss 0.1904151439666748, acc 0.8223251279551548, auc: 0.871647630908266, precision: 0.8448067632850241, recall: 0.9286425489545304\n",
      "train: 2019-01-04T16:02:33.008828: step 204, loss 0.19370537996292114, acc 0.8320732192598488, auc: 0.8273336225246402, precision: 0.8630385487528345, recall: 0.9406821552150272\n",
      "train: 2019-01-04T16:02:34.152235: step 205, loss 0.21672411262989044, acc 0.718019680196802, auc: 0.692145465734403, precision: 0.739670932358318, recall: 0.9079892280071813\n",
      "train: 2019-01-04T16:02:35.565087: step 206, loss 0.20814815163612366, acc 0.7893370607028753, auc: 0.7977022684646269, precision: 0.8005905064728595, recall: 0.9521880064829822\n",
      "train: 2019-01-04T16:02:36.765725: step 207, loss 0.1996380239725113, acc 0.8196941239602897, auc: 0.8181495873274602, precision: 0.8324734446130501, recall: 0.958085923856095\n",
      "train: 2019-01-04T16:02:37.300489: step 208, loss 0.22278714179992676, acc 0.6940667490729295, auc: 0.7131575849530437, precision: 0.6987032799389779, recall: 0.9015748031496063\n",
      "train: 2019-01-04T16:02:38.215411: step 209, loss 0.21769243478775024, acc 0.7840416305290546, auc: 0.7715383135910577, precision: 0.7972486079266295, recall: 0.9500390320062451\n",
      "train: 2019-01-04T16:02:39.472644: step 210, loss 0.19693268835544586, acc 0.8006109414051652, auc: 0.8635505397575681, precision: 0.8081761006289309, recall: 0.9319097502014504\n",
      "train: 2019-01-04T16:02:40.408856: step 211, loss 0.196254163980484, acc 0.829409757761856, auc: 0.875973858197495, precision: 0.8228187919463087, recall: 0.9464745239320638\n",
      "train: 2019-01-04T16:02:41.608428: step 212, loss 0.19735011458396912, acc 0.8158487167942369, auc: 0.8445094438475775, precision: 0.8325123152709359, recall: 0.9491575524682234\n",
      "train: 2019-01-04T16:02:42.868910: step 213, loss 0.18138808012008667, acc 0.8291873963515755, auc: 0.894038440553583, precision: 0.8426082068577853, recall: 0.9190680564071122\n",
      "train: 2019-01-04T16:02:43.844397: step 214, loss 0.20750269293785095, acc 0.7630040911747517, auc: 0.7747066816288718, precision: 0.7858126721763086, recall: 0.9235127478753541\n",
      "train: 2019-01-04T16:02:45.103311: step 215, loss 0.20516833662986755, acc 0.7678948603614175, auc: 0.7776115803131944, precision: 0.7940453428025129, recall: 0.9252068746021642\n",
      "train: 2019-01-04T16:02:46.278708: step 216, loss 0.2257290780544281, acc 0.7074217951339195, auc: 0.7066707261277528, precision: 0.7210513265559138, recall: 0.9047915370255134\n",
      "train: 2019-01-04T16:02:46.729136: step 217, loss 0.21638862788677216, acc 0.6652142338416849, auc: 0.7351665915679757, precision: 0.6044776119402985, recall: 0.7726550079491256\n",
      "train: 2019-01-04T16:02:56.558898: step 218, loss 0.16680577397346497, acc 0.9223344759155692, auc: 0.9628702634350916, precision: 0.9292242532322782, recall: 0.9736073805909145\n",
      "train: 2019-01-04T16:02:57.736020: step 219, loss 0.20012103021144867, acc 0.7097371343946687, auc: 0.7335987043204526, precision: 0.7246039142590867, recall: 0.8895881006864989\n",
      "train: 2019-01-04T16:02:59.329980: step 220, loss 0.19586913287639618, acc 0.7650578380534503, auc: 0.8329126749447402, precision: 0.7724611860524306, recall: 0.9144320578487496\n",
      "train: 2019-01-04T16:03:00.671164: step 221, loss 0.20459291338920593, acc 0.7466379774072082, auc: 0.7154370290053048, precision: 0.7694235588972431, recall: 0.9226145755071374\n",
      "train: 2019-01-04T16:03:02.842843: step 222, loss 0.1824009269475937, acc 0.813092633114515, auc: 0.8722955464090378, precision: 0.8137137377127787, recall: 0.9319055464030752\n",
      "train: 2019-01-04T16:03:03.245130: step 223, loss 0.243692547082901, acc 0.6943933823529411, auc: 0.7581893838582261, precision: 0.6644042232277526, recall: 0.8001816530426885\n",
      "train: 2019-01-04T16:03:04.294214: step 224, loss 0.20509381592273712, acc 0.7815674891146589, auc: 0.7727809119193112, precision: 0.8044506258692629, recall: 0.9353169469598965\n",
      "train: 2019-01-04T16:03:05.721896: step 225, loss 0.17820072174072266, acc 0.7844579469139751, auc: 0.8807604266237361, precision: 0.7738037699371677, recall: 0.8859988931931378\n",
      "train: 2019-01-04T16:03:06.801136: step 226, loss 0.18402424454689026, acc 0.8028588445503275, auc: 0.8669942412110856, precision: 0.8215085884988798, recall: 0.9228187919463087\n",
      "train: 2019-01-04T16:03:07.925646: step 227, loss 0.20493870973587036, acc 0.7295987072448155, auc: 0.7324745019483956, precision: 0.7594454072790294, recall: 0.8760495801679328\n",
      "train: 2019-01-04T16:03:09.725836: step 228, loss 0.18605558574199677, acc 0.7555859553123575, auc: 0.8239509445826034, precision: 0.7571695760598504, recall: 0.8923585598824394\n",
      "train: 2019-01-04T16:03:10.111618: step 229, loss 0.2117638885974884, acc 0.7545098039215686, auc: 0.8188752378959534, precision: 0.7365269461077845, recall: 0.8686440677966102\n",
      "train: 2019-01-04T16:03:12.006339: step 230, loss 0.178458109498024, acc 0.743631881676253, auc: 0.7727464903802824, precision: 0.7668161434977578, recall: 0.9200614911606456\n",
      "train: 2019-01-04T16:03:13.739482: step 231, loss 0.18247278034687042, acc 0.8284581060981656, auc: 0.8967514047123681, precision: 0.8305467690917306, recall: 0.9273461150353178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:03:17.498152: step 232, loss 0.1609184294939041, acc 0.9012, auc: 0.957236948608708, precision: 0.9081059390048154, recall: 0.9575458392101551\n",
      "train: 2019-01-04T16:03:19.356662: step 233, loss 0.16590936481952667, acc 0.8884699968583097, auc: 0.93202171687992, precision: 0.899625468164794, recall: 0.9650462032944958\n",
      "train: 2019-01-04T16:03:20.997836: step 234, loss 0.19495607912540436, acc 0.7104910714285714, auc: 0.7379523776729451, precision: 0.7353025523372526, recall: 0.8727025187202179\n",
      "train: 2019-01-04T16:03:22.342050: step 235, loss 0.17135384678840637, acc 0.8553430821147356, auc: 0.9070425393892059, precision: 0.8773662551440329, recall: 0.9422510312315852\n",
      "train: 2019-01-04T16:03:23.204154: step 236, loss 0.19826877117156982, acc 0.726742064939803, auc: 0.755937022075316, precision: 0.7362637362637363, recall: 0.8866513233601842\n",
      "train: 2019-01-04T16:03:26.418379: step 237, loss 0.15361268818378448, acc 0.8965416178194607, auc: 0.9544023334463989, precision: 0.8891954022988505, recall: 0.9452590420332356\n",
      "train: 2019-01-04T16:03:27.898619: step 238, loss 0.19234345853328705, acc 0.709480122324159, auc: 0.7550076819770171, precision: 0.7153846153846154, recall: 0.8625632377740303\n",
      "train: 2019-01-04T16:03:29.215647: step 239, loss 0.16413173079490662, acc 0.8260869565217391, auc: 0.888668757265809, precision: 0.8579881656804734, recall: 0.9146045608927705\n",
      "train: 2019-01-04T16:03:30.813312: step 240, loss 0.18655571341514587, acc 0.7334885690093141, auc: 0.7630003229297584, precision: 0.7717981888745149, recall: 0.887797619047619\n",
      "train: 2019-01-04T16:03:32.125320: step 241, loss 0.19141888618469238, acc 0.6962095875139354, auc: 0.6709825911218914, precision: 0.7345907713034692, recall: 0.878372935964559\n",
      "train: 2019-01-04T16:03:33.253151: step 242, loss 0.17524299025535583, acc 0.7683873915029694, auc: 0.745279717790088, precision: 0.7953906689151209, recall: 0.9082156611039794\n",
      "train: 2019-01-04T16:03:34.113130: step 243, loss 0.18922460079193115, acc 0.7268211920529801, auc: 0.743853404521785, precision: 0.7494897959183674, recall: 0.8968253968253969\n",
      "train: 2019-01-04T16:03:35.089273: step 244, loss 0.1901121288537979, acc 0.7517385257301809, auc: 0.7762395355282032, precision: 0.7538665488289881, recall: 0.9157273215244229\n",
      "train: 2019-01-04T16:03:36.594282: step 245, loss 0.17359031736850739, acc 0.8200248241621845, auc: 0.8710851694999379, precision: 0.8245525586085203, recall: 0.9494920174165458\n",
      "train: 2019-01-04T16:03:37.654921: step 246, loss 0.1806439757347107, acc 0.8022202746129127, auc: 0.7702706199572515, precision: 0.8228008891711655, recall: 0.9560885608856089\n",
      "train: 2019-01-04T16:03:38.785005: step 247, loss 0.19321101903915405, acc 0.7077717235445942, auc: 0.7109669906621704, precision: 0.7285308729595458, recall: 0.8899003034243607\n",
      "train: 2019-01-04T16:03:39.333965: step 248, loss 0.21459847688674927, acc 0.6925411968777103, auc: 0.7620788116765302, precision: 0.6817528735632183, recall: 0.7810699588477367\n",
      "train: 2019-01-04T16:03:40.890535: step 249, loss 0.15109558403491974, acc 0.8542399374755764, auc: 0.9319246160321204, precision: 0.8604389420371412, recall: 0.9244256348246674\n",
      "train: 2019-01-04T16:03:42.385273: step 250, loss 0.17463593184947968, acc 0.7551503670376509, auc: 0.8394797295103416, precision: 0.766967569374791, recall: 0.8719118206005321\n",
      "train: 2019-01-04T16:03:43.340154: step 251, loss 0.20831434428691864, acc 0.7356375525455394, auc: 0.7753847494177006, precision: 0.7614788312462731, recall: 0.884961884961885\n",
      "train: 2019-01-04T16:03:45.045505: step 252, loss 0.165262833237648, acc 0.807013414921158, auc: 0.8226083056781818, precision: 0.8276520316334879, recall: 0.9416692522494571\n",
      "train: 2019-01-04T16:03:46.127043: step 253, loss 0.16242250800132751, acc 0.8081418782748891, auc: 0.8911390975241741, precision: 0.8133640552995391, recall: 0.9028132992327366\n",
      "train: 2019-01-04T16:03:47.048124: step 254, loss 0.19156597554683685, acc 0.7037037037037037, auc: 0.6883474870921197, precision: 0.7240932642487047, recall: 0.8891834570519618\n",
      "train: 2019-01-04T16:03:47.405185: step 255, loss 0.24252644181251526, acc 0.6989453499520614, auc: 0.7492120623764635, precision: 0.7008837525492863, recall: 0.8457752255947498\n",
      "train: 2019-01-04T16:03:48.586240: step 256, loss 0.1955135017633438, acc 0.7063731170336037, auc: 0.689142038861319, precision: 0.7323119777158774, recall: 0.8957410562180579\n",
      "train: 2019-01-04T16:03:49.752402: step 257, loss 0.16373670101165771, acc 0.7857142857142857, auc: 0.8209606903892944, precision: 0.8205233033524121, recall: 0.9156021897810219\n",
      "train: 2019-01-04T16:03:50.158856: step 258, loss 0.23076196014881134, acc 0.7500986971969996, auc: 0.7580330298900193, precision: 0.8018200202224469, recall: 0.8680897646414888\n",
      "train: 2019-01-04T16:03:51.478381: step 259, loss 0.21025614440441132, acc 0.7339722405816259, auc: 0.701374632984491, precision: 0.7426610751048418, recall: 0.9374398460057748\n",
      "train: 2019-01-04T16:03:52.613686: step 260, loss 0.17447087168693542, acc 0.7738708699605941, auc: 0.7888940081467632, precision: 0.777932474765054, recall: 0.9539052496798975\n",
      "train: 2019-01-04T16:03:53.065766: step 261, loss 0.22262771427631378, acc 0.7073509015256588, auc: 0.7521230396750446, precision: 0.7028248587570621, recall: 0.92079940784604\n",
      "train: 2019-01-04T16:03:54.614247: step 262, loss 0.14679883420467377, acc 0.8603289657095066, auc: 0.9121563009305503, precision: 0.863066538090646, recall: 0.9728260869565217\n",
      "train: 2019-01-04T16:03:56.122227: step 263, loss 0.16501276195049286, acc 0.7674586033117351, auc: 0.8387726709960449, precision: 0.7854477611940298, recall: 0.9005347593582887\n",
      "train: 2019-01-04T16:03:57.685224: step 264, loss 0.1595408171415329, acc 0.7577480807506397, auc: 0.8366336815692825, precision: 0.8015904572564613, recall: 0.8509919797382862\n",
      "train: 2019-01-04T16:03:59.377134: step 265, loss 0.15838021039962769, acc 0.7932618683001531, auc: 0.8456143383911465, precision: 0.8328120557293147, recall: 0.8913572732805843\n",
      "train: 2019-01-04T16:04:00.371549: step 266, loss 0.18420442938804626, acc 0.744920993227991, auc: 0.8246485041793888, precision: 0.7756381549484997, recall: 0.8112412177985948\n",
      "train: 2019-01-04T16:04:01.453890: step 267, loss 0.1721835732460022, acc 0.7738629083920564, auc: 0.7514262799670525, precision: 0.8023082650781832, recall: 0.924892703862661\n",
      "train: 2019-01-04T16:04:02.350906: step 268, loss 0.17206971347332, acc 0.7751937984496124, auc: 0.7998817626605248, precision: 0.8052653572921019, recall: 0.9055451127819549\n",
      "train: 2019-01-04T16:04:03.496749: step 269, loss 0.1714879721403122, acc 0.7935389846975953, auc: 0.7992214048487398, precision: 0.8046763615625891, recall: 0.9447606293940408\n",
      "train: 2019-01-04T16:04:04.613562: step 270, loss 0.18023498356342316, acc 0.7091013824884793, auc: 0.723821426580023, precision: 0.7120608475190149, recall: 0.901421366345713\n",
      "train: 2019-01-04T16:04:05.944810: step 271, loss 0.16357991099357605, acc 0.7060691601976006, auc: 0.7821742335199315, precision: 0.65882996172772, recall: 0.8521923620933521\n",
      "train: 2019-01-04T16:04:07.077908: step 272, loss 0.18667760491371155, acc 0.7374159127812573, auc: 0.686515010255454, precision: 0.7584033613445378, recall: 0.9316129032258065\n",
      "train: 2019-01-04T16:04:08.530476: step 273, loss 0.1768941432237625, acc 0.7099577266322217, auc: 0.686209942590783, precision: 0.7200111327581409, recall: 0.9186789772727273\n",
      "train: 2019-01-04T16:04:09.222560: step 274, loss 0.1872304528951645, acc 0.7371601208459214, auc: 0.7922835409434348, precision: 0.7419028340080972, recall: 0.887409200968523\n",
      "train: 2019-01-04T16:04:10.456419: step 275, loss 0.17800705134868622, acc 0.6938997821350763, auc: 0.6936301092459543, precision: 0.7186981069412155, recall: 0.8865219172470299\n",
      "train: 2019-01-04T16:04:11.430227: step 276, loss 0.1617674082517624, acc 0.7943060498220641, auc: 0.7912924539957238, precision: 0.8197406942701798, recall: 0.9302325581395349\n",
      "train: 2019-01-04T16:04:12.534883: step 277, loss 0.17255432903766632, acc 0.7271914132379249, auc: 0.784680397070057, precision: 0.7194907813871817, recall: 0.8558746736292429\n",
      "train: 2019-01-04T16:04:13.693669: step 278, loss 0.15255995094776154, acc 0.827671913835957, auc: 0.9009039657523301, precision: 0.8310228550018733, recall: 0.9276453366792137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:04:15.267972: step 279, loss 0.17214913666248322, acc 0.7231732776617954, auc: 0.7571358456375641, precision: 0.7191374663072776, recall: 0.9037940379403794\n",
      "train: 2019-01-04T16:04:16.534305: step 280, loss 0.20870311558246613, acc 0.6751568844592101, auc: 0.681748081472165, precision: 0.6919954904171364, recall: 0.8862258157666763\n",
      "train: 2019-01-04T16:04:17.786175: step 281, loss 0.15776368975639343, acc 0.8035215804165772, auc: 0.8872905996205274, precision: 0.8091780424048794, recall: 0.9152431011826544\n",
      "train: 2019-01-04T16:04:19.116546: step 282, loss 0.1532628834247589, acc 0.810786313165647, auc: 0.8826022360248448, precision: 0.8361714621256606, recall: 0.91136\n",
      "train: 2019-01-04T16:04:20.134731: step 283, loss 0.1580197662115097, acc 0.7068709836875927, auc: 0.6985851041447506, precision: 0.734248284466625, recall: 0.8757440476190477\n",
      "train: 2019-01-04T16:04:21.352276: step 284, loss 0.17383641004562378, acc 0.737518910741301, auc: 0.6905342000997211, precision: 0.7626233313987232, recall: 0.9217818309365136\n",
      "train: 2019-01-04T16:04:29.204596: step 285, loss 0.1265496164560318, acc 0.9035763266167317, auc: 0.9510979320761422, precision: 0.9124341005529124, recall: 0.9682084868331287\n",
      "train: 2019-01-04T16:04:30.785105: step 286, loss 0.14860005676746368, acc 0.8102605863192183, auc: 0.8903370831518502, precision: 0.8237840590781768, recall: 0.9309352517985612\n",
      "train: 2019-01-04T16:04:33.637244: step 287, loss 0.1298556625843048, acc 0.8485310119695321, auc: 0.9298202782730229, precision: 0.8456826137689615, recall: 0.9455316373124593\n",
      "train: 2019-01-04T16:04:35.406363: step 288, loss 0.17517708241939545, acc 0.7412252712188896, auc: 0.7511277713296043, precision: 0.7589302069976186, recall: 0.9312204989885368\n",
      "train: 2019-01-04T16:04:37.143635: step 289, loss 0.1688385009765625, acc 0.7367804551539491, auc: 0.8167126293694855, precision: 0.7142857142857143, recall: 0.8817365269461078\n",
      "train: 2019-01-04T16:04:39.079840: step 290, loss 0.1328190416097641, acc 0.8464332036316472, auc: 0.9147686026261647, precision: 0.8551724137931035, recall: 0.9351432880844646\n",
      "train: 2019-01-04T16:04:40.050202: step 291, loss 0.15283040702342987, acc 0.7505050505050505, auc: 0.7219435394151303, precision: 0.776969696969697, recall: 0.9105113636363636\n",
      "train: 2019-01-04T16:04:40.851978: step 292, loss 0.17586550116539001, acc 0.6800997091815538, auc: 0.6872940640687888, precision: 0.709246392303581, recall: 0.8544752092723761\n",
      "train: 2019-01-04T16:04:41.839144: step 293, loss 0.17894035577774048, acc 0.6943501080580426, auc: 0.7064655757939629, precision: 0.721539071796906, recall: 0.863312766967252\n",
      "train: 2019-01-04T16:04:43.007403: step 294, loss 0.1514989584684372, acc 0.7198237885462555, auc: 0.7068740320082603, precision: 0.7369589345172031, recall: 0.8912751677852349\n",
      "train: 2019-01-04T16:04:45.034420: step 295, loss 0.14172512292861938, acc 0.8507038035339922, auc: 0.893086156350327, precision: 0.8540630182421227, recall: 0.9576446280991735\n",
      "train: 2019-01-04T16:04:46.635705: step 296, loss 0.1543455421924591, acc 0.7858328721638074, auc: 0.8110087689770088, precision: 0.7951651197927908, recall: 0.9455852156057495\n",
      "train: 2019-01-04T16:04:47.293461: step 297, loss 0.17478807270526886, acc 0.6446194225721785, auc: 0.7077700386954118, precision: 0.6353135313531353, recall: 0.7661691542288557\n",
      "train: 2019-01-04T16:04:49.498931: step 298, loss 0.143607497215271, acc 0.8151366503153469, auc: 0.8876073304148528, precision: 0.8074406431108618, recall: 0.9372829861111112\n",
      "train: 2019-01-04T16:04:52.277329: step 299, loss 0.12312944233417511, acc 0.9094953426942861, auc: 0.9522133685954545, precision: 0.9027529495888452, recall: 0.9792515028117122\n",
      "train: 2019-01-04T16:04:52.552517: step 300, loss 0.21443913877010345, acc 0.6724791508718726, auc: 0.738240416040459, precision: 0.696329254727475, recall: 0.7974522292993631\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:05:05.421320, step: 300, loss: 0.15777513671379823, acc: 0.7713213328264851, auc: 0.8042566280325475, precision: 0.7801965535023047, recall: 0.9143636858698664\n",
      "Saved model checkpoint to model/my-model-300\n",
      "\n",
      "train: 2019-01-04T16:05:06.367902: step 301, loss 0.1501247137784958, acc 0.7309851965288412, auc: 0.7250668175437185, precision: 0.748585795097423, recall: 0.9036418816388467\n",
      "train: 2019-01-04T16:05:07.928655: step 302, loss 0.17515182495117188, acc 0.7104031677465803, auc: 0.7356453741474811, precision: 0.7109015639374425, recall: 0.8977635782747604\n",
      "train: 2019-01-04T16:05:09.567770: step 303, loss 0.1514231413602829, acc 0.664064602960969, auc: 0.6899493661524884, precision: 0.6759520451339915, recall: 0.8535173642030276\n",
      "train: 2019-01-04T16:05:10.881362: step 304, loss 0.15394935011863708, acc 0.6612377850162866, auc: 0.6632001548492589, precision: 0.6781467814678147, recall: 0.8664222105814563\n",
      "train: 2019-01-04T16:05:11.987727: step 305, loss 0.14803332090377808, acc 0.7505526897568165, auc: 0.7887679340275389, precision: 0.7444499259990133, recall: 0.9046762589928058\n",
      "train: 2019-01-04T16:05:13.527222: step 306, loss 0.12384859472513199, acc 0.8895378364652108, auc: 0.9174987559797231, precision: 0.8972320376914017, recall: 0.972550271305458\n",
      "train: 2019-01-04T16:05:15.384843: step 307, loss 0.14677025377750397, acc 0.8300909550277918, auc: 0.8781516429916117, precision: 0.8378947368421052, recall: 0.9542729919506765\n",
      "train: 2019-01-04T16:05:16.380407: step 308, loss 0.16891911625862122, acc 0.7770956316410862, auc: 0.7976939367629651, precision: 0.7903367496339678, recall: 0.9221045439016058\n",
      "train: 2019-01-04T16:05:17.674739: step 309, loss 0.17114895582199097, acc 0.707758425695544, auc: 0.7290638138131363, precision: 0.72334455667789, recall: 0.8938973647711512\n",
      "train: 2019-01-04T16:05:19.135614: step 310, loss 0.15873020887374878, acc 0.8158288450029922, auc: 0.8397463432955372, precision: 0.8264420622766717, recall: 0.9583662194159431\n",
      "train: 2019-01-04T16:05:19.732429: step 311, loss 0.16665351390838623, acc 0.7567221510883483, auc: 0.8150051285772237, precision: 0.7851851851851852, recall: 0.8771483131763208\n",
      "train: 2019-01-04T16:05:21.217782: step 312, loss 0.158763125538826, acc 0.6690611840475601, auc: 0.6971188137154501, precision: 0.6762476310802275, recall: 0.8731647634584013\n",
      "train: 2019-01-04T16:05:21.575320: step 313, loss 0.22550614178180695, acc 0.6922690763052208, auc: 0.72114896354662, precision: 0.7160409556313994, recall: 0.8418940609951846\n",
      "train: 2019-01-04T16:05:23.017076: step 314, loss 0.13696405291557312, acc 0.8220726136090406, auc: 0.8342230763045964, precision: 0.8433701657458563, recall: 0.9463732176069436\n",
      "train: 2019-01-04T16:05:24.947502: step 315, loss 0.14891047775745392, acc 0.7214098237720284, auc: 0.7867122041327939, precision: 0.7328109201213346, recall: 0.87109375\n",
      "train: 2019-01-04T16:05:25.787249: step 316, loss 0.14351466298103333, acc 0.743065693430657, auc: 0.7939957931037235, precision: 0.7477966101694915, recall: 0.8760921366163622\n",
      "train: 2019-01-04T16:05:26.829276: step 317, loss 0.1513795405626297, acc 0.7577319587628866, auc: 0.7768250600736052, precision: 0.7772200772200772, recall: 0.920018281535649\n",
      "train: 2019-01-04T16:05:28.253128: step 318, loss 0.12069752812385559, acc 0.8736737400530504, auc: 0.8710867276465085, precision: 0.8798831690397956, recall: 0.9788789601949635\n",
      "train: 2019-01-04T16:05:29.425391: step 319, loss 0.181605264544487, acc 0.7375928096912857, auc: 0.6734165130211631, precision: 0.7622256031297544, recall: 0.9337060702875399\n",
      "train: 2019-01-04T16:05:31.354037: step 320, loss 0.13600237667560577, acc 0.8154583582983823, auc: 0.8985984724157997, precision: 0.8329094110741538, recall: 0.9184466019417475\n",
      "train: 2019-01-04T16:05:32.451669: step 321, loss 0.1632736623287201, acc 0.770779676890658, auc: 0.7903538362261162, precision: 0.7937689550592777, recall: 0.9257234726688103\n",
      "train: 2019-01-04T16:05:33.770706: step 322, loss 0.16055984795093536, acc 0.709279845335911, auc: 0.7318398273584592, precision: 0.7355345911949686, recall: 0.8659755646057016\n",
      "train: 2019-01-04T16:05:35.928429: step 323, loss 0.11284816265106201, acc 0.9038306451612903, auc: 0.9555172381215271, precision: 0.911093627065303, recall: 0.9617940199335548\n",
      "train: 2019-01-04T16:05:36.826702: step 324, loss 0.14652685821056366, acc 0.797806107322858, auc: 0.8424410835842603, precision: 0.8208791208791208, recall: 0.9207066557107642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:05:38.178242: step 325, loss 0.14704826474189758, acc 0.7553220156292104, auc: 0.6926004886250685, precision: 0.7615062761506276, recall: 0.9586155003762227\n",
      "train: 2019-01-04T16:05:39.662493: step 326, loss 0.13872931897640228, acc 0.6762225969645869, auc: 0.7095506337940574, precision: 0.6787797218483625, recall: 0.861126920887877\n",
      "train: 2019-01-04T16:05:40.651861: step 327, loss 0.13896003365516663, acc 0.8482758620689655, auc: 0.9018457300275482, precision: 0.8436923076923077, recall: 0.9621052631578947\n",
      "train: 2019-01-04T16:05:41.689321: step 328, loss 0.1326570212841034, acc 0.7964661909616038, auc: 0.8559812843306044, precision: 0.8114193807800563, recall: 0.9394785847299814\n",
      "train: 2019-01-04T16:05:42.887995: step 329, loss 0.1535102128982544, acc 0.7601776461880089, auc: 0.7619532524455512, precision: 0.7641509433962265, recall: 0.9377713458755427\n",
      "train: 2019-01-04T16:05:43.402876: step 330, loss 0.1914796233177185, acc 0.7052631578947368, auc: 0.7444009639764286, precision: 0.7197286012526096, recall: 0.8629536921151439\n",
      "train: 2019-01-04T16:05:44.489937: step 331, loss 0.14610107243061066, acc 0.69613457408733, auc: 0.6737487031937792, precision: 0.738374619730552, recall: 0.8730729701952723\n",
      "train: 2019-01-04T16:05:45.949775: step 332, loss 0.1414433866739273, acc 0.7968099861303745, auc: 0.8812507851246898, precision: 0.8051366635249765, recall: 0.9082934609250398\n",
      "train: 2019-01-04T16:05:47.235094: step 333, loss 0.13873304426670074, acc 0.739524702939337, auc: 0.7599706403125207, precision: 0.7565497783151954, recall: 0.8912630579297246\n",
      "train: 2019-01-04T16:05:49.029795: step 334, loss 0.14646779000759125, acc 0.7336214347450303, auc: 0.7428092083209397, precision: 0.7614091273018415, recall: 0.915964363111004\n",
      "train: 2019-01-04T16:05:49.500906: step 335, loss 0.1498134881258011, acc 0.7082683307332294, auc: 0.7202412367506708, precision: 0.7257462686567164, recall: 0.9067599067599068\n",
      "train: 2019-01-04T16:05:50.689718: step 336, loss 0.13045282661914825, acc 0.7112802148612355, auc: 0.7631002415308314, precision: 0.702846975088968, recall: 0.891647855530474\n",
      "train: 2019-01-04T16:05:51.338239: step 337, loss 0.1868949681520462, acc 0.6978074356530028, auc: 0.7161954777911113, precision: 0.7039128680919726, recall: 0.8893985728848114\n",
      "train: 2019-01-04T16:05:53.346848: step 338, loss 0.0994502454996109, acc 0.944687682428488, auc: 0.9873719873719874, precision: 0.9573820395738204, recall: 0.970117601696549\n",
      "train: 2019-01-04T16:05:54.407150: step 339, loss 0.1305343210697174, acc 0.8025830258302583, auc: 0.8405814987309842, precision: 0.8054930211616389, recall: 0.9455602536997886\n",
      "train: 2019-01-04T16:05:55.623373: step 340, loss 0.1425826996564865, acc 0.6918663516706041, auc: 0.6992266184044569, precision: 0.7094682230869002, recall: 0.8719447396386822\n",
      "train: 2019-01-04T16:05:57.449836: step 341, loss 0.11023425310850143, acc 0.8402420574886535, auc: 0.9116091366508473, precision: 0.8518228145825166, recall: 0.9354283254412398\n",
      "train: 2019-01-04T16:05:57.706240: step 342, loss 0.2196343094110489, acc 0.7380660954712362, auc: 0.8118361665589852, precision: 0.7417840375586855, recall: 0.8377518557794273\n",
      "train: 2019-01-04T16:05:58.916114: step 343, loss 0.14881137013435364, acc 0.7106658809664114, auc: 0.7373372290315812, precision: 0.728565623730191, recall: 0.8509729473184623\n",
      "train: 2019-01-04T16:06:00.494553: step 344, loss 0.13323302567005157, acc 0.7092261904761905, auc: 0.7110758164516998, precision: 0.7247673586256264, recall: 0.9068517689207345\n",
      "train: 2019-01-04T16:06:00.992897: step 345, loss 0.1513133943080902, acc 0.7654623392529087, auc: 0.8199508878441244, precision: 0.7542448614834674, recall: 0.8865546218487395\n",
      "train: 2019-01-04T16:06:01.452059: step 346, loss 0.19307827949523926, acc 0.6917744916820703, auc: 0.7371402770215535, precision: 0.7122252747252747, recall: 0.8070038910505837\n",
      "train: 2019-01-04T16:06:02.771888: step 347, loss 0.12106523662805557, acc 0.7945205479452054, auc: 0.8533181447008488, precision: 0.79669573198715, recall: 0.9243876464323749\n",
      "train: 2019-01-04T16:06:04.241218: step 348, loss 0.10898030549287796, acc 0.8580906745300405, auc: 0.9053330105911207, precision: 0.8662063012516185, recall: 0.9639769452449568\n",
      "train: 2019-01-04T16:06:11.806653: step 349, loss 0.1021704375743866, acc 0.8880589868811645, auc: 0.9372877601174321, precision: 0.8924940020564378, recall: 0.9716417910447761\n",
      "train: 2019-01-04T16:06:12.940798: step 350, loss 0.16433008015155792, acc 0.713322435635472, auc: 0.7788383181987117, precision: 0.7366389548693587, recall: 0.8278278278278278\n",
      "train: 2019-01-04T16:06:13.616583: step 351, loss 0.1539691984653473, acc 0.739524838012959, auc: 0.8222980551246868, precision: 0.733370288248337, recall: 0.9155709342560554\n",
      "train: 2019-01-04T16:06:15.226253: step 352, loss 0.12686681747436523, acc 0.7990634312473393, auc: 0.821279199044689, precision: 0.8016686531585221, recall: 0.9677697841726619\n",
      "train: 2019-01-04T16:06:17.203959: step 353, loss 0.10735280811786652, acc 0.842817679558011, auc: 0.9243369094296372, precision: 0.8380881808077065, recall: 0.9448621553884712\n",
      "train: 2019-01-04T16:06:18.503669: step 354, loss 0.131418839097023, acc 0.7763503649635036, auc: 0.8318048226950356, precision: 0.768451519536903, recall: 0.944\n",
      "train: 2019-01-04T16:06:19.732693: step 355, loss 0.1548074334859848, acc 0.7019138755980862, auc: 0.6983202982450321, precision: 0.72105561861521, recall: 0.9062054208273894\n",
      "train: 2019-01-04T16:06:20.936642: step 356, loss 0.135428786277771, acc 0.7431506849315068, auc: 0.744122092752977, precision: 0.7621212121212121, recall: 0.9108193752829334\n",
      "train: 2019-01-04T16:06:21.359336: step 357, loss 0.18418043851852417, acc 0.6783144912641316, auc: 0.7657940051020408, precision: 0.6229508196721312, recall: 0.7633928571428571\n",
      "train: 2019-01-04T16:06:22.997797: step 358, loss 0.13938100636005402, acc 0.7945806842264608, auc: 0.8325019208732212, precision: 0.8126596132798248, recall: 0.9310344827586207\n",
      "train: 2019-01-04T16:06:24.437988: step 359, loss 0.13209529221057892, acc 0.7342503438789546, auc: 0.737454209209637, precision: 0.7567475230611548, recall: 0.8971243418388012\n",
      "train: 2019-01-04T16:06:25.587965: step 360, loss 0.13710550963878632, acc 0.7678258694020516, auc: 0.8274058809437193, precision: 0.7803908578999669, recall: 0.8988935520793591\n",
      "train: 2019-01-04T16:06:26.693362: step 361, loss 0.13702163100242615, acc 0.7195636875439831, auc: 0.7121285311497857, precision: 0.7570179475379659, recall: 0.8594566353187043\n",
      "train: 2019-01-04T16:06:28.557181: step 362, loss 0.1341068595647812, acc 0.6956420955030135, auc: 0.7465192710020315, precision: 0.7013446567586695, recall: 0.8086495308037536\n",
      "train: 2019-01-04T16:06:29.669650: step 363, loss 0.14247938990592957, acc 0.7313691507798961, auc: 0.7220756206719344, precision: 0.7581265291855994, recall: 0.9011217282924803\n",
      "train: 2019-01-04T16:06:31.056444: step 364, loss 0.1031801700592041, acc 0.8705489614243324, auc: 0.9052186458217464, precision: 0.8913138367525099, recall: 0.9533146591970122\n",
      "train: 2019-01-04T16:06:32.547839: step 365, loss 0.15359574556350708, acc 0.6978216347048967, auc: 0.688252891277926, precision: 0.7209771402958315, recall: 0.8948539638386648\n",
      "train: 2019-01-04T16:06:33.667044: step 366, loss 0.14653022587299347, acc 0.7498790517658442, auc: 0.7916818035505357, precision: 0.7649741562785041, recall: 0.9060136838314728\n",
      "train: 2019-01-04T16:06:34.657174: step 367, loss 0.1459231823682785, acc 0.7731492501785289, auc: 0.8410026515234009, precision: 0.8058518744285279, recall: 0.8932432432432432\n",
      "train: 2019-01-04T16:06:36.375394: step 368, loss 0.1305164098739624, acc 0.7976963969285292, auc: 0.8752650033544727, precision: 0.8180935740632886, recall: 0.9068216053367764\n",
      "train: 2019-01-04T16:06:37.687650: step 369, loss 0.14084556698799133, acc 0.6805476326297776, auc: 0.7027211487170933, precision: 0.6935541951746489, recall: 0.8774487471526196\n",
      "train: 2019-01-04T16:06:39.371455: step 370, loss 0.13415156304836273, acc 0.6838344011828487, auc: 0.7314079067778065, precision: 0.6852827537759044, recall: 0.8344739093242087\n",
      "train: 2019-01-04T16:06:40.534458: step 371, loss 0.17104122042655945, acc 0.7329257641921397, auc: 0.7202893679210816, precision: 0.7660980810234541, recall: 0.8926708074534162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:06:44.514649: step 372, loss 0.10401125997304916, acc 0.8758260498827543, auc: 0.9387969147443298, precision: 0.876898596846521, recall: 0.9507528230865746\n",
      "train: 2019-01-04T16:06:45.837220: step 373, loss 0.14622612297534943, acc 0.7320990408208788, auc: 0.6728608235116328, precision: 0.7593020272004106, recall: 0.9183736809435133\n",
      "train: 2019-01-04T16:06:46.943333: step 374, loss 0.12003070116043091, acc 0.7326687811508835, auc: 0.7477173534319929, precision: 0.7422048997772829, recall: 0.913013698630137\n",
      "train: 2019-01-04T16:06:47.931747: step 375, loss 0.13405348360538483, acc 0.7488727020464794, auc: 0.738327302276281, precision: 0.7691648822269808, recall: 0.9066128218071681\n",
      "train: 2019-01-04T16:06:49.190081: step 376, loss 0.14391443133354187, acc 0.7826654240447344, auc: 0.8040704915879224, precision: 0.8043960923623446, recall: 0.9270726714431935\n",
      "train: 2019-01-04T16:06:50.509070: step 377, loss 0.12722639739513397, acc 0.7418077061577242, auc: 0.6919499562748649, precision: 0.7540506855006232, recall: 0.9355670103092784\n",
      "train: 2019-01-04T16:06:51.072146: step 378, loss 0.16102644801139832, acc 0.6742539200809307, auc: 0.7432120751170225, precision: 0.6594473487677371, recall: 0.8244631185807656\n",
      "train: 2019-01-04T16:06:51.661725: step 379, loss 0.15708637237548828, acc 0.6900089206066012, auc: 0.7305923746424972, precision: 0.6934889434889435, recall: 0.8520754716981133\n",
      "train: 2019-01-04T16:06:52.057238: step 380, loss 0.16557864844799042, acc 0.6955696202531646, auc: 0.7228872724460753, precision: 0.7010050251256281, recall: 0.8709677419354839\n",
      "train: 2019-01-04T16:06:53.038647: step 381, loss 0.15029969811439514, acc 0.7638535841382816, auc: 0.7688791653796319, precision: 0.7699425654116145, recall: 0.9206409767264403\n",
      "train: 2019-01-04T16:06:54.460281: step 382, loss 0.13619373738765717, acc 0.7205417607223477, auc: 0.7054956818170509, precision: 0.746922024623803, recall: 0.8971409792967466\n",
      "train: 2019-01-04T16:06:54.920141: step 383, loss 0.17298701405525208, acc 0.7174787662047385, auc: 0.7755044626511205, precision: 0.741106719367589, recall: 0.8247800586510264\n",
      "train: 2019-01-04T16:07:04.316205: step 384, loss 0.08580994606018066, acc 0.9484663843850046, auc: 0.9824341721293205, precision: 0.9567110337106606, recall: 0.9807447079826574\n",
      "train: 2019-01-04T16:07:05.686734: step 385, loss 0.15152373909950256, acc 0.7527093596059113, auc: 0.7721053144229262, precision: 0.7638583638583638, recall: 0.9156908665105387\n",
      "train: 2019-01-04T16:07:06.438919: step 386, loss 0.1444319188594818, acc 0.7001321003963011, auc: 0.7550543447194572, precision: 0.7038527889591719, recall: 0.8805755395683453\n",
      "train: 2019-01-04T16:07:07.604749: step 387, loss 0.12313388288021088, acc 0.7559291504052837, auc: 0.7668819031435854, precision: 0.77443857331572, recall: 0.9474747474747475\n",
      "train: 2019-01-04T16:07:09.363874: step 388, loss 0.11684007197618484, acc 0.831765087605451, auc: 0.8809161771826776, precision: 0.8357857563272513, recall: 0.9551569506726457\n",
      "train: 2019-01-04T16:07:10.677839: step 389, loss 0.11472975462675095, acc 0.7306176084099869, auc: 0.7199707301073467, precision: 0.7531914893617021, recall: 0.9036375239310785\n",
      "train: 2019-01-04T16:07:12.378797: step 390, loss 0.13272619247436523, acc 0.7675684873234644, auc: 0.8355344482867926, precision: 0.7812712056095906, recall: 0.8964443290942123\n",
      "train: 2019-01-04T16:07:13.479159: step 391, loss 0.1640348732471466, acc 0.7461040098056383, auc: 0.7608217934610475, precision: 0.7630255941499086, recall: 0.8899253731343284\n",
      "train: 2019-01-04T16:07:14.381727: step 392, loss 0.1395622342824936, acc 0.7289177193581428, auc: 0.7049976419846048, precision: 0.7588163761653831, recall: 0.9039111540318686\n",
      "train: 2019-01-04T16:07:15.645177: step 393, loss 0.12674416601657867, acc 0.7458033573141487, auc: 0.6955655075990896, precision: 0.7670765027322405, recall: 0.9311774461028193\n",
      "train: 2019-01-04T16:07:17.189320: step 394, loss 0.11420222371816635, acc 0.7166386554621849, auc: 0.6920894770125646, precision: 0.7346859149434257, recall: 0.9203323558162267\n",
      "train: 2019-01-04T16:07:18.224284: step 395, loss 0.13026317954063416, acc 0.7978494623655914, auc: 0.8603851646014873, precision: 0.8095804819994049, recall: 0.9296207721216262\n",
      "train: 2019-01-04T16:07:19.525959: step 396, loss 0.125873863697052, acc 0.733273596176822, auc: 0.7472134146795921, precision: 0.7327773749093546, recall: 0.9283417547083141\n",
      "train: 2019-01-04T16:07:20.969061: step 397, loss 0.11824329942464828, acc 0.701280227596017, auc: 0.6781553877931145, precision: 0.7161447742984953, recall: 0.9253809774040987\n",
      "train: 2019-01-04T16:07:21.521451: step 398, loss 0.14668656885623932, acc 0.6694214876033058, auc: 0.7255535675136667, precision: 0.6619483763530392, recall: 0.8377239199157007\n",
      "train: 2019-01-04T16:07:23.070863: step 399, loss 0.12885302305221558, acc 0.6943076081007116, auc: 0.7202005161645587, precision: 0.6976256983240223, recall: 0.8883948421520675\n",
      "train: 2019-01-04T16:07:24.206364: step 400, loss 0.13097384572029114, acc 0.7921855921855921, auc: 0.838711871498237, precision: 0.7964519140989729, recall: 0.9285195936139332\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:07:36.827954, step: 400, loss: 0.12714135790100464, acc: 0.7730531232970103, auc: 0.8064023666349782, precision: 0.7825257718327424, recall: 0.9126521340681797\n",
      "Saved model checkpoint to model/my-model-400\n",
      "\n",
      "train: 2019-01-04T16:07:38.188929: step 401, loss 0.10932280123233795, acc 0.7949748743718593, auc: 0.8716894303137174, precision: 0.8025962399283796, recall: 0.9129327902240326\n",
      "train: 2019-01-04T16:07:39.621941: step 402, loss 0.14054043591022491, acc 0.7185249836351735, auc: 0.7194688002431738, precision: 0.7368989735278229, recall: 0.8961892247043364\n",
      "train: 2019-01-04T16:07:41.441315: step 403, loss 0.11184649914503098, acc 0.8614063777596075, auc: 0.9214546137793733, precision: 0.879394449116905, recall: 0.9457308248914617\n",
      "train: 2019-01-04T16:07:42.789499: step 404, loss 0.11722838878631592, acc 0.7350906095551895, auc: 0.7745436169889628, precision: 0.7443130118289354, recall: 0.8711395101171459\n",
      "train: 2019-01-04T16:07:44.446214: step 405, loss 0.12056616693735123, acc 0.7661386138613862, auc: 0.8514341989951315, precision: 0.7773144286905754, recall: 0.8803526448362721\n",
      "train: 2019-01-04T16:07:45.599348: step 406, loss 0.12028088420629501, acc 0.8047058823529412, auc: 0.8858420734859236, precision: 0.8055, recall: 0.8541887592788971\n",
      "train: 2019-01-04T16:07:46.045280: step 407, loss 0.1816202700138092, acc 0.6957787481804949, auc: 0.7113935286048576, precision: 0.71024512884978, recall: 0.8719135802469136\n",
      "train: 2019-01-04T16:07:47.781432: step 408, loss 0.10016641765832901, acc 0.852963967454475, auc: 0.9284496270997225, precision: 0.8668230388324212, recall: 0.9306099608282037\n",
      "train: 2019-01-04T16:07:49.104778: step 409, loss 0.12210377305746078, acc 0.832299340829779, auc: 0.8607710179507295, precision: 0.8410518966720968, recall: 0.952054794520548\n",
      "train: 2019-01-04T16:07:50.705654: step 410, loss 0.10186431556940079, acc 0.8158315565031983, auc: 0.8775069153660703, precision: 0.8285811269517991, recall: 0.929196802436239\n",
      "train: 2019-01-04T16:07:53.967971: step 411, loss 0.09274087846279144, acc 0.8302372631966157, auc: 0.9170042499085812, precision: 0.8204819277108434, recall: 0.8928220255653884\n",
      "train: 2019-01-04T16:07:55.506807: step 412, loss 0.1159374788403511, acc 0.771117166212534, auc: 0.8568519351935193, precision: 0.7505307855626328, recall: 0.875\n",
      "train: 2019-01-04T16:07:56.779346: step 413, loss 0.10016759485006332, acc 0.8369938229238161, auc: 0.855168777007437, precision: 0.8552, recall: 0.9498000888494003\n",
      "train: 2019-01-04T16:07:58.507125: step 414, loss 0.1191595047712326, acc 0.7404046133484591, auc: 0.8317351054348668, precision: 0.7493727348759409, recall: 0.8500948766603416\n",
      "train: 2019-01-04T16:08:01.441847: step 415, loss 0.07952164858579636, acc 0.9268193796878167, auc: 0.9757696906209459, precision: 0.9399265477439664, recall: 0.9644683714670256\n",
      "train: 2019-01-04T16:08:03.141896: step 416, loss 0.12092170119285583, acc 0.7366771159874608, auc: 0.7517283721587646, precision: 0.7489948305571511, recall: 0.8962199312714777\n",
      "train: 2019-01-04T16:08:05.923795: step 417, loss 0.07985524088144302, acc 0.9094650205761317, auc: 0.9676617609376679, precision: 0.9127749903512158, recall: 0.9578776832725799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:08:06.519888: step 418, loss 0.15212474763393402, acc 0.7625688073394495, auc: 0.8363868904320719, precision: 0.783342119135477, recall: 0.8629500580720093\n",
      "train: 2019-01-04T16:08:08.054895: step 419, loss 0.13562509417533875, acc 0.8048657506617926, auc: 0.8336440995841897, precision: 0.8257314974182444, recall: 0.945320197044335\n",
      "train: 2019-01-04T16:08:09.445121: step 420, loss 0.11923951655626297, acc 0.8229005000862217, auc: 0.885558312209811, precision: 0.8378437363596682, recall: 0.9311181178753335\n",
      "train: 2019-01-04T16:08:09.987817: step 421, loss 0.14279644191265106, acc 0.7505957108816521, auc: 0.8461965357126568, precision: 0.7473684210526316, recall: 0.8458304134548003\n",
      "train: 2019-01-04T16:08:11.006368: step 422, loss 0.11060132086277008, acc 0.7975843398583924, auc: 0.8512666939163658, precision: 0.7533068783068783, recall: 0.9097444089456869\n",
      "train: 2019-01-04T16:08:12.219435: step 423, loss 0.09079939126968384, acc 0.8398617511520737, auc: 0.9136229186245451, precision: 0.8670605612998523, recall: 0.9229559748427673\n",
      "train: 2019-01-04T16:08:13.360560: step 424, loss 0.11369506269693375, acc 0.7486378496185979, auc: 0.7872650164925022, precision: 0.7455179282868526, recall: 0.8921334922526818\n",
      "train: 2019-01-04T16:08:13.976066: step 425, loss 0.12127949297428131, acc 0.7703818369453045, auc: 0.8484774482420998, precision: 0.7875375375375375, recall: 0.8662262592898431\n",
      "train: 2019-01-04T16:08:15.894367: step 426, loss 0.11901143193244934, acc 0.7630926813150504, auc: 0.819219192289784, precision: 0.7763819095477387, recall: 0.9101620029455081\n",
      "train: 2019-01-04T16:08:17.098573: step 427, loss 0.12412925809621811, acc 0.7166617166617166, auc: 0.7449585682882908, precision: 0.7263002582073036, recall: 0.9027968821641449\n",
      "train: 2019-01-04T16:08:18.919053: step 428, loss 0.11450599879026413, acc 0.7593844719048729, auc: 0.7187993850629815, precision: 0.7773075905845015, recall: 0.9392777245126238\n",
      "train: 2019-01-04T16:08:20.548969: step 429, loss 0.09644478559494019, acc 0.8244545915778793, auc: 0.8877079888614019, precision: 0.8332296204107031, recall: 0.944954128440367\n",
      "train: 2019-01-04T16:08:22.021948: step 430, loss 0.09260280430316925, acc 0.8645587541293063, auc: 0.928780884939643, precision: 0.8607250755287009, recall: 0.9618501012829169\n",
      "train: 2019-01-04T16:08:22.975471: step 431, loss 0.11685487627983093, acc 0.7799442896935933, auc: 0.8251250583985688, precision: 0.8055805580558055, recall: 0.8994974874371859\n",
      "train: 2019-01-04T16:08:23.869170: step 432, loss 0.11344922333955765, acc 0.8238557558945908, auc: 0.8117193425960529, precision: 0.833932135728543, recall: 0.9578175149014214\n",
      "train: 2019-01-04T16:08:25.364270: step 433, loss 0.11901181936264038, acc 0.7311056290648127, auc: 0.7585763176572391, precision: 0.7542901716068643, recall: 0.9194928684627576\n",
      "train: 2019-01-04T16:08:26.604885: step 434, loss 0.09885277599096298, acc 0.7984625668449198, auc: 0.8624555453016245, precision: 0.8004237288135593, recall: 0.9346857991093518\n",
      "train: 2019-01-04T16:08:28.137731: step 435, loss 0.10948006808757782, acc 0.7022114510754317, auc: 0.7115769287123407, precision: 0.7170930663700408, recall: 0.8978644382544104\n",
      "train: 2019-01-04T16:08:29.769233: step 436, loss 0.10265659540891647, acc 0.8324641460234681, auc: 0.8563738286627041, precision: 0.8442791174961519, recall: 0.9525325615050652\n",
      "train: 2019-01-04T16:08:31.439873: step 437, loss 0.1303493082523346, acc 0.711835657745948, auc: 0.7123666296606836, precision: 0.7331058020477815, recall: 0.9005030743432085\n",
      "train: 2019-01-04T16:08:32.629308: step 438, loss 0.14310432970523834, acc 0.7192943770672546, auc: 0.7401995747191477, precision: 0.7207282913165266, recall: 0.9031239031239031\n",
      "train: 2019-01-04T16:08:33.559897: step 439, loss 0.11149770766496658, acc 0.7553667262969589, auc: 0.7493685713239869, precision: 0.78101802757158, recall: 0.9166148102053516\n",
      "train: 2019-01-04T16:08:34.741271: step 440, loss 0.12022245675325394, acc 0.6994652406417112, auc: 0.749529127751931, precision: 0.7022222222222222, recall: 0.855595667870036\n",
      "train: 2019-01-04T16:08:35.812026: step 441, loss 0.1268938034772873, acc 0.7476661951909477, auc: 0.752902618830096, precision: 0.7581967213114754, recall: 0.9234608985024958\n",
      "train: 2019-01-04T16:08:39.461918: step 442, loss 0.08441068232059479, acc 0.8859914467508084, auc: 0.9535989870899997, precision: 0.8844356323466931, recall: 0.9575283655320453\n",
      "train: 2019-01-04T16:08:41.604021: step 443, loss 0.08286498486995697, acc 0.8922413793103449, auc: 0.9625802633653489, precision: 0.8965920155793573, recall: 0.9595664860358483\n",
      "train: 2019-01-04T16:08:42.174624: step 444, loss 0.14593961834907532, acc 0.6865364850976362, auc: 0.7423229525703023, precision: 0.6986301369863014, recall: 0.8109540636042403\n",
      "train: 2019-01-04T16:08:43.205116: step 445, loss 0.11253908276557922, acc 0.7684054753977062, auc: 0.7939452519656617, precision: 0.7784228350266086, recall: 0.9054586381541925\n",
      "train: 2019-01-04T16:08:44.359046: step 446, loss 0.11182595044374466, acc 0.7473190348525469, auc: 0.7159126757727289, precision: 0.764750378214826, recall: 0.9387186629526463\n",
      "train: 2019-01-04T16:08:45.941577: step 447, loss 0.15097279846668243, acc 0.6924119241192412, auc: 0.6979883850253236, precision: 0.7073260073260074, recall: 0.8966798235430694\n",
      "train: 2019-01-04T16:08:47.203198: step 448, loss 0.11827630549669266, acc 0.7772163527790538, auc: 0.8023356706242633, precision: 0.7948582030214684, recall: 0.9386541471048513\n",
      "train: 2019-01-04T16:08:48.170322: step 449, loss 0.11200150102376938, acc 0.8362989323843416, auc: 0.8794316724624656, precision: 0.8410174880763116, recall: 0.9483685908927931\n",
      "train: 2019-01-04T16:08:48.586869: step 450, loss 0.15374650061130524, acc 0.7032013022246337, auc: 0.7594858253566583, precision: 0.7234387672343877, recall: 0.8123861566484517\n",
      "train: 2019-01-04T16:08:49.824319: step 451, loss 0.11547327041625977, acc 0.8162182644941266, auc: 0.8508211830800176, precision: 0.8312931226352104, recall: 0.9462883202432227\n",
      "train: 2019-01-04T16:08:51.390042: step 452, loss 0.08439760655164719, acc 0.8995756718528995, auc: 0.9601719417846195, precision: 0.9139426284189459, recall: 0.9578186902819855\n",
      "train: 2019-01-04T16:08:52.745620: step 453, loss 0.11755253374576569, acc 0.7995142002989537, auc: 0.8378200915960621, precision: 0.8104408352668213, recall: 0.9317151240330754\n",
      "train: 2019-01-04T16:08:53.841738: step 454, loss 0.1609582155942917, acc 0.7175834084761046, auc: 0.7098891086992668, precision: 0.7320675105485233, recall: 0.9214020180562932\n",
      "train: 2019-01-04T16:08:54.749650: step 455, loss 0.10954621434211731, acc 0.8089041095890411, auc: 0.8209276936949021, precision: 0.827683615819209, recall: 0.9399633363886343\n",
      "train: 2019-01-04T16:08:56.063307: step 456, loss 0.12491362541913986, acc 0.6923965796163624, auc: 0.7208320511152918, precision: 0.7142857142857143, recall: 0.8392390898918314\n",
      "train: 2019-01-04T16:08:57.723227: step 457, loss 0.11039597541093826, acc 0.7162449751714354, auc: 0.7758842834955388, precision: 0.713608172199927, recall: 0.8249683677773092\n",
      "train: 2019-01-04T16:08:58.984085: step 458, loss 0.10196321457624435, acc 0.8054067839836776, auc: 0.841156680763425, precision: 0.8177429088714544, recall: 0.9445799930289299\n",
      "train: 2019-01-04T16:08:59.778935: step 459, loss 0.13855229318141937, acc 0.7097097097097097, auc: 0.7269603114449115, precision: 0.7366959740860712, recall: 0.8409931325937665\n",
      "train: 2019-01-04T16:09:00.893406: step 460, loss 0.10762376338243484, acc 0.778547975191536, auc: 0.7385385878489327, precision: 0.8009397693293464, recall: 0.9300595238095238\n",
      "train: 2019-01-04T16:09:02.278174: step 461, loss 0.09744302928447723, acc 0.7915144703568419, auc: 0.8446587131640665, precision: 0.824991370383155, recall: 0.9104761904761904\n",
      "train: 2019-01-04T16:09:03.484490: step 462, loss 0.1163010448217392, acc 0.7859657357427834, auc: 0.8555307555145455, precision: 0.7770426728415482, recall: 0.908001546192501\n",
      "train: 2019-01-04T16:09:05.228077: step 463, loss 0.09809868037700653, acc 0.7800308573947542, auc: 0.8488019810796349, precision: 0.7884671109823239, recall: 0.9103379056540649\n",
      "train: 2019-01-04T16:09:06.610141: step 464, loss 0.11548673361539841, acc 0.6893229863526893, auc: 0.6867621709151691, precision: 0.7088135593220339, recall: 0.8737985791893021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:09:07.491276: step 465, loss 0.15112890303134918, acc 0.75229769110065, auc: 0.7723563804094545, precision: 0.7572204746925937, recall: 0.9118457300275482\n",
      "train: 2019-01-04T16:09:09.126105: step 466, loss 0.0738774910569191, acc 0.9020240539747727, auc: 0.9552838071889912, precision: 0.9150552486187845, recall: 0.9678597516435354\n",
      "train: 2019-01-04T16:09:10.361605: step 467, loss 0.10749567300081253, acc 0.702401082177883, auc: 0.7573931220305017, precision: 0.7142857142857143, recall: 0.8495821727019499\n",
      "train: 2019-01-04T16:09:11.622810: step 468, loss 0.11359207332134247, acc 0.7434494673193205, auc: 0.7480004336911285, precision: 0.7513774104683195, recall: 0.9281156954487452\n",
      "train: 2019-01-04T16:09:12.829440: step 469, loss 0.0987536832690239, acc 0.8367612600885186, auc: 0.8896454964805031, precision: 0.843613077182339, recall: 0.9388597149287322\n",
      "train: 2019-01-04T16:09:14.204473: step 470, loss 0.10546468198299408, acc 0.7906925934017915, auc: 0.8635296877743164, precision: 0.7798313878080415, recall: 0.8960506706408345\n",
      "train: 2019-01-04T16:09:15.951229: step 471, loss 0.07921504229307175, acc 0.8522954091816367, auc: 0.928760074512554, precision: 0.8591942560829677, recall: 0.9288486416558862\n",
      "train: 2019-01-04T16:09:17.280572: step 472, loss 0.09525109082460403, acc 0.7264033264033264, auc: 0.714167862266858, precision: 0.7425889328063241, recall: 0.9164634146341464\n",
      "train: 2019-01-04T16:09:18.301262: step 473, loss 0.08944524824619293, acc 0.8135113268608414, auc: 0.8970979809293498, precision: 0.8364130434782608, recall: 0.9058269570335491\n",
      "train: 2019-01-04T16:09:21.223197: step 474, loss 0.07662712037563324, acc 0.9021460386017006, auc: 0.9629680837634947, precision: 0.9023041474654377, recall: 0.9616895874263262\n",
      "train: 2019-01-04T16:09:22.148728: step 475, loss 0.10689077526330948, acc 0.7492612916842549, auc: 0.771069901038141, precision: 0.7808870116156283, recall: 0.8920386007237636\n",
      "train: 2019-01-04T16:09:24.324916: step 476, loss 0.0843871682882309, acc 0.8684294871794872, auc: 0.9402028911674126, precision: 0.870995859664415, recall: 0.9458116422148604\n",
      "train: 2019-01-04T16:09:33.392453: step 477, loss 0.06961962580680847, acc 0.9085332284703107, auc: 0.951791760841049, precision: 0.9180022363026463, recall: 0.972076961026147\n",
      "train: 2019-01-04T16:09:33.996958: step 478, loss 0.14915335178375244, acc 0.731128074639525, auc: 0.7257232537283688, precision: 0.7502564102564102, recall: 0.908695652173913\n",
      "train: 2019-01-04T16:09:34.261742: step 479, loss 0.19621926546096802, acc 0.719592570401438, auc: 0.7843991587926897, precision: 0.7206896551724138, recall: 0.8530612244897959\n",
      "train: 2019-01-04T16:09:35.602604: step 480, loss 0.12835869193077087, acc 0.7201012444631935, auc: 0.7149448859266856, precision: 0.7366204162537165, recall: 0.9184430027803522\n",
      "train: 2019-01-04T16:09:36.415074: step 481, loss 0.08898337185382843, acc 0.8144329896907216, auc: 0.854091077433377, precision: 0.8380493678506924, recall: 0.9386378961564397\n",
      "train: 2019-01-04T16:09:37.350888: step 482, loss 0.1041974425315857, acc 0.7784938941655359, auc: 0.8306784572231639, precision: 0.7926829268292683, recall: 0.9113670505758638\n",
      "train: 2019-01-04T16:09:38.993313: step 483, loss 0.09078534692525864, acc 0.8444623764373613, auc: 0.9019046321825565, precision: 0.8552857493254844, recall: 0.9506543075245365\n",
      "train: 2019-01-04T16:09:39.539055: step 484, loss 0.15587057173252106, acc 0.6733084267330842, auc: 0.6879459209279148, precision: 0.6985334057577404, recall: 0.847167325428195\n",
      "train: 2019-01-04T16:09:40.770124: step 485, loss 0.11027082800865173, acc 0.6942175759555701, auc: 0.6943040071273052, precision: 0.721277478224803, recall: 0.8681977034448327\n",
      "train: 2019-01-04T16:09:41.196881: step 486, loss 0.1357818841934204, acc 0.7523860021208908, auc: 0.8235293452051602, precision: 0.7395470383275261, recall: 0.8348082595870207\n",
      "train: 2019-01-04T16:09:42.100849: step 487, loss 0.14249438047409058, acc 0.6997275204359673, auc: 0.6882211414435184, precision: 0.7204850031908104, recall: 0.9090177133655395\n",
      "train: 2019-01-04T16:09:43.261457: step 488, loss 0.10103912651538849, acc 0.6998491704374057, auc: 0.7550391998427638, precision: 0.6944858420268256, recall: 0.8853704876504117\n",
      "train: 2019-01-04T16:09:44.821232: step 489, loss 0.11655227094888687, acc 0.7099122243630914, auc: 0.7157051292513231, precision: 0.7191154041687849, recall: 0.9188048067554401\n",
      "train: 2019-01-04T16:09:46.084422: step 490, loss 0.11107664555311203, acc 0.8160377358490566, auc: 0.7845388615007165, precision: 0.8257439134355276, recall: 0.9672564034856087\n",
      "train: 2019-01-04T16:09:49.267010: step 491, loss 0.0819275751709938, acc 0.821439416945035, auc: 0.8933051148258961, precision: 0.8156043046357616, recall: 0.932560340747752\n",
      "train: 2019-01-04T16:09:52.138573: step 492, loss 0.07611370831727982, acc 0.8766103635843114, auc: 0.9467757052547318, precision: 0.8827425009738995, recall: 0.9457429048414023\n",
      "train: 2019-01-04T16:09:53.045434: step 493, loss 0.12044098228216171, acc 0.732994923857868, auc: 0.7292765806438471, precision: 0.754885993485342, recall: 0.9083782459578638\n",
      "train: 2019-01-04T16:09:54.126729: step 494, loss 0.10450895875692368, acc 0.8128865979381443, auc: 0.8536624647525054, precision: 0.8095716198125836, recall: 0.9390527950310559\n",
      "train: 2019-01-04T16:09:55.622837: step 495, loss 0.12023601680994034, acc 0.7515696589173596, auc: 0.8009462761519811, precision: 0.7668988119623106, recall: 0.9199017199017199\n",
      "train: 2019-01-04T16:09:57.554378: step 496, loss 0.06935790926218033, acc 0.9017857142857143, auc: 0.9644245486685806, precision: 0.9273631840796019, recall: 0.9404641775983855\n",
      "train: 2019-01-04T16:09:58.795860: step 497, loss 0.10912235081195831, acc 0.6932948347770292, auc: 0.6889157379172502, precision: 0.7206591639871383, recall: 0.8729308666017527\n",
      "train: 2019-01-04T16:10:00.008353: step 498, loss 0.12953148782253265, acc 0.6921463528315078, auc: 0.7365376453999097, precision: 0.7009316770186336, recall: 0.8630975143403442\n",
      "train: 2019-01-04T16:10:01.600187: step 499, loss 0.10969927906990051, acc 0.6833824975417896, auc: 0.6992759655620939, precision: 0.6975211797929087, recall: 0.872791519434629\n",
      "train: 2019-01-04T16:10:02.899223: step 500, loss 0.11103875935077667, acc 0.8193581780538303, auc: 0.8365531056728894, precision: 0.8356245090337785, recall: 0.9529675251959686\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:10:15.516674, step: 500, loss: 0.1087328762962268, acc: 0.7740789570815422, auc: 0.8101349996511978, precision: 0.7805211896022869, recall: 0.919977173575174\n",
      "Saved model checkpoint to model/my-model-500\n",
      "\n",
      "train: 2019-01-04T16:10:16.598219: step 501, loss 0.10539916902780533, acc 0.7446564885496183, auc: 0.6876995816797808, precision: 0.770392749244713, recall: 0.9287200832466181\n",
      "train: 2019-01-04T16:10:18.126550: step 502, loss 0.07587789744138718, acc 0.8664187643020596, auc: 0.9400180458636731, precision: 0.8804554079696395, recall: 0.9385113268608414\n",
      "train: 2019-01-04T16:10:19.787291: step 503, loss 0.07034972310066223, acc 0.8936905790838375, auc: 0.9536554768986627, precision: 0.9018691588785047, recall: 0.963147792706334\n",
      "train: 2019-01-04T16:10:21.206110: step 504, loss 0.10240984708070755, acc 0.7863032524049474, auc: 0.8635168905504053, precision: 0.7665051483949121, recall: 0.9398440401039733\n",
      "train: 2019-01-04T16:10:22.260528: step 505, loss 0.10135897994041443, acc 0.7631229235880399, auc: 0.8064409656733309, precision: 0.7692929292929293, recall: 0.9305962854349951\n",
      "train: 2019-01-04T16:10:22.657649: step 506, loss 0.14796249568462372, acc 0.712403951701427, auc: 0.6980220207692048, precision: 0.7346547314578005, recall: 0.9133545310015898\n",
      "train: 2019-01-04T16:10:23.743582: step 507, loss 0.11068397760391235, acc 0.7200129324280634, auc: 0.798065737118964, precision: 0.6996587030716723, recall: 0.8516320474777448\n",
      "train: 2019-01-04T16:10:25.773530: step 508, loss 0.10967139154672623, acc 0.6927791610802528, auc: 0.7177979871881035, precision: 0.7116733466933868, recall: 0.8624772313296903\n",
      "train: 2019-01-04T16:10:26.886315: step 509, loss 0.11397553235292435, acc 0.7398561151079137, auc: 0.7501128143417797, precision: 0.7614085519223859, recall: 0.8982619754133108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:10:27.790135: step 510, loss 0.1201729029417038, acc 0.7036479879654005, auc: 0.6880229070446462, precision: 0.7375790424570913, recall: 0.8875\n",
      "train: 2019-01-04T16:10:28.964765: step 511, loss 0.11575430631637573, acc 0.7063969382176053, auc: 0.7389344699056776, precision: 0.7280606717226435, recall: 0.8626444159178434\n",
      "train: 2019-01-04T16:10:30.703275: step 512, loss 0.08741830289363861, acc 0.8337591240875912, auc: 0.9077317987852478, precision: 0.8387171740646061, recall: 0.943282801881861\n",
      "train: 2019-01-04T16:10:31.871204: step 513, loss 0.08286784589290619, acc 0.8443606444977717, auc: 0.8830525956876841, precision: 0.8501026694045175, recall: 0.9587772116720704\n",
      "train: 2019-01-04T16:10:33.763876: step 514, loss 0.11231787502765656, acc 0.7368421052631579, auc: 0.8002034794871979, precision: 0.7493355142097731, recall: 0.9062809099901088\n",
      "train: 2019-01-04T16:10:34.178317: step 515, loss 0.16355931758880615, acc 0.691812865497076, auc: 0.7434144442061781, precision: 0.7009267059814659, recall: 0.8286852589641435\n",
      "train: 2019-01-04T16:10:34.992353: step 516, loss 0.0896638035774231, acc 0.8662524525833878, auc: 0.9375835588766147, precision: 0.8771428571428571, recall: 0.9521488701816571\n",
      "train: 2019-01-04T16:10:36.327641: step 517, loss 0.09444104135036469, acc 0.8053375196232339, auc: 0.8208152960476689, precision: 0.8153507392261717, recall: 0.9428883230265551\n",
      "train: 2019-01-04T16:10:43.758990: step 518, loss 0.06353800743818283, acc 0.9092983456930975, auc: 0.9583872170195397, precision: 0.9189403710685584, recall: 0.9690311780707261\n",
      "train: 2019-01-04T16:10:44.975470: step 519, loss 0.111238993704319, acc 0.74366576819407, auc: 0.7209726688803478, precision: 0.776925535314797, recall: 0.9057377049180327\n",
      "train: 2019-01-04T16:10:45.779212: step 520, loss 0.12484129518270493, acc 0.7282786885245902, auc: 0.7375790899158016, precision: 0.7462284482758621, recall: 0.8782498414711477\n",
      "train: 2019-01-04T16:10:47.343882: step 521, loss 0.10652146488428116, acc 0.7593823718636071, auc: 0.7740698332447491, precision: 0.77518327450448, recall: 0.9066370276278184\n",
      "train: 2019-01-04T16:10:47.718305: step 522, loss 0.1445716917514801, acc 0.7219787100814026, auc: 0.7722938039902107, precision: 0.7292817679558011, recall: 0.8407643312101911\n",
      "train: 2019-01-04T16:10:48.985486: step 523, loss 0.12953831255435944, acc 0.6750342309447741, auc: 0.6952426414216775, precision: 0.6867401752897936, recall: 0.8848816029143898\n",
      "train: 2019-01-04T16:10:50.614990: step 524, loss 0.10487302392721176, acc 0.6651066042641706, auc: 0.704408279304019, precision: 0.6713735558408216, recall: 0.8879456706281834\n",
      "train: 2019-01-04T16:10:52.436240: step 525, loss 0.09298577904701233, acc 0.6772456786625106, auc: 0.7551191442148997, precision: 0.6457013574660634, recall: 0.8003365114974762\n",
      "train: 2019-01-04T16:10:53.694871: step 526, loss 0.15078431367874146, acc 0.7366105295099378, auc: 0.7104900743516096, precision: 0.7478595142407828, recall: 0.9359282746555871\n",
      "train: 2019-01-04T16:10:55.438039: step 527, loss 0.08381463587284088, acc 0.7302371541501976, auc: 0.8060445072535831, precision: 0.7208538587848933, recall: 0.8099630996309963\n",
      "train: 2019-01-04T16:10:57.095083: step 528, loss 0.11123841255903244, acc 0.7740772779700116, auc: 0.8692128913552817, precision: 0.7787270397266126, recall: 0.8728752693320565\n",
      "train: 2019-01-04T16:10:58.729390: step 529, loss 0.0891457200050354, acc 0.690700104493208, auc: 0.709614867394778, precision: 0.7200923787528868, recall: 0.8468223791417708\n",
      "train: 2019-01-04T16:10:59.855049: step 530, loss 0.10284675657749176, acc 0.7188488298545225, auc: 0.7846495547287449, precision: 0.7150308202939782, recall: 0.8396436525612472\n",
      "train: 2019-01-04T16:11:01.424837: step 531, loss 0.07871104031801224, acc 0.8398318935325706, auc: 0.9142238160937121, precision: 0.8667287977632805, recall: 0.9156547423695438\n",
      "train: 2019-01-04T16:11:02.347390: step 532, loss 0.09425918012857437, acc 0.8121468926553672, auc: 0.8788771962820057, precision: 0.8373295721673719, recall: 0.9054397559735639\n",
      "train: 2019-01-04T16:11:04.276687: step 533, loss 0.09321204572916031, acc 0.6601204503796806, auc: 0.6850484590545926, precision: 0.687186349949816, recall: 0.8498138187836161\n",
      "train: 2019-01-04T16:11:05.777683: step 534, loss 0.10961513221263885, acc 0.7332444782939832, auc: 0.7791470435845088, precision: 0.7531411677753141, recall: 0.8845486111111112\n",
      "train: 2019-01-04T16:11:07.410285: step 535, loss 0.08844269067049026, acc 0.7940058152538582, auc: 0.8369134953521433, precision: 0.8159311087190527, recall: 0.9275007647598654\n",
      "train: 2019-01-04T16:11:08.280741: step 536, loss 0.11773599684238434, acc 0.7269979852249832, auc: 0.7299742392045226, precision: 0.7458208315473639, recall: 0.8877551020408163\n",
      "train: 2019-01-04T16:11:09.527034: step 537, loss 0.09384075552225113, acc 0.8307764928015106, auc: 0.884215673868801, precision: 0.8336816960286653, recall: 0.94579945799458\n",
      "train: 2019-01-04T16:11:09.853667: step 538, loss 0.1373833864927292, acc 0.7355907780979827, auc: 0.7987641498546986, precision: 0.7186813186813187, recall: 0.8549019607843137\n",
      "train: 2019-01-04T16:11:10.931399: step 539, loss 0.13717269897460938, acc 0.7575257731958763, auc: 0.7542628711563677, precision: 0.7661728395061729, recall: 0.9312725090036015\n",
      "train: 2019-01-04T16:11:18.520429: step 540, loss 0.06073381379246712, acc 0.9051340223510629, auc: 0.9449075584942604, precision: 0.9101375743975022, recall: 0.9781879194630873\n",
      "train: 2019-01-04T16:11:19.740737: step 541, loss 0.10147029906511307, acc 0.8052526151791676, auc: 0.821623442334283, precision: 0.8182746299132211, recall: 0.9516176907094093\n",
      "train: 2019-01-04T16:11:21.513696: step 542, loss 0.0929660052061081, acc 0.6908594815825375, auc: 0.7251648450680084, precision: 0.6907739101047651, recall: 0.9036251105216623\n",
      "train: 2019-01-04T16:11:23.164574: step 543, loss 0.12142074108123779, acc 0.7355221158230734, auc: 0.7536120826564069, precision: 0.7407610684229784, recall: 0.92612076852699\n",
      "train: 2019-01-04T16:11:23.644020: step 544, loss 0.1003093272447586, acc 0.6999109528049866, auc: 0.7549150851773105, precision: 0.70828025477707, recall: 0.8373493975903614\n",
      "train: 2019-01-04T16:11:24.640956: step 545, loss 0.10777260363101959, acc 0.7446153846153846, auc: 0.7410288759620866, precision: 0.7617706237424547, recall: 0.92431640625\n",
      "train: 2019-01-04T16:11:25.729368: step 546, loss 0.08485311269760132, acc 0.7527147087857848, auc: 0.7677222920623032, precision: 0.7644201578627808, recall: 0.9176384839650146\n",
      "train: 2019-01-04T16:11:27.073741: step 547, loss 0.09231877326965332, acc 0.7769351464435147, auc: 0.8515782335999291, precision: 0.8007434944237918, recall: 0.8717118575475516\n",
      "train: 2019-01-04T16:11:27.815962: step 548, loss 0.10752256214618683, acc 0.7153987167736022, auc: 0.767729666549873, precision: 0.740473061760841, recall: 0.8329637841832964\n",
      "train: 2019-01-04T16:11:30.557977: step 549, loss 0.07094033807516098, acc 0.8563287342531494, auc: 0.9286422564373475, precision: 0.8659262197540658, recall: 0.939328743545611\n",
      "train: 2019-01-04T16:11:32.509441: step 550, loss 0.08550285547971725, acc 0.8691931540342298, auc: 0.9037892626591436, precision: 0.8651612903225806, recall: 0.9768712438535786\n",
      "train: 2019-01-04T16:11:33.533285: step 551, loss 0.09171999990940094, acc 0.7960884899006092, auc: 0.822501695179022, precision: 0.8271792919680243, recall: 0.9227176220806794\n",
      "train: 2019-01-04T16:11:34.119153: step 552, loss 0.13381153345108032, acc 0.7048528241845664, auc: 0.7635651413567153, precision: 0.6998112020138452, recall: 0.8075526506899056\n",
      "train: 2019-01-04T16:11:36.028552: step 553, loss 0.06887151300907135, acc 0.8589543937708565, auc: 0.9214460951160588, precision: 0.8791964777105118, recall: 0.9424778761061947\n",
      "train: 2019-01-04T16:11:37.919404: step 554, loss 0.08504046499729156, acc 0.8216742157356148, auc: 0.8796167708230003, precision: 0.816264449401744, recall: 0.9624581539933046\n",
      "train: 2019-01-04T16:11:39.409232: step 555, loss 0.09352172911167145, acc 0.7267775229357798, auc: 0.7513599258697381, precision: 0.7334579439252337, recall: 0.8910081743869209\n",
      "train: 2019-01-04T16:11:40.683387: step 556, loss 0.09337734431028366, acc 0.7093862815884476, auc: 0.7117125584349474, precision: 0.7340280924331672, recall: 0.8813928182807399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:11:41.728907: step 557, loss 0.094634510576725, acc 0.8387949260042283, auc: 0.8639372930776523, precision: 0.8503732554365466, recall: 0.9461899602744673\n",
      "train: 2019-01-04T16:11:42.896607: step 558, loss 0.10488149523735046, acc 0.7693817468105987, auc: 0.7984600554297636, precision: 0.7908790879087909, recall: 0.9155956929489406\n",
      "train: 2019-01-04T16:11:43.652930: step 559, loss 0.11697373539209366, acc 0.7382995319812793, auc: 0.7704309126396554, precision: 0.7615347255949491, recall: 0.897025171624714\n",
      "train: 2019-01-04T16:11:44.679740: step 560, loss 0.11601527780294418, acc 0.7381552419354839, auc: 0.8211974238858476, precision: 0.741189035243859, recall: 0.8696741854636592\n",
      "train: 2019-01-04T16:11:45.785460: step 561, loss 0.1248917207121849, acc 0.7366922234392114, auc: 0.7042308349274146, precision: 0.7562003968253969, recall: 0.9329865361077111\n",
      "train: 2019-01-04T16:11:46.578106: step 562, loss 0.11587654054164886, acc 0.7915567282321899, auc: 0.8093715438879416, precision: 0.8140756302521008, recall: 0.9281437125748503\n",
      "train: 2019-01-04T16:11:47.772905: step 563, loss 0.11534958332777023, acc 0.7211287645245436, auc: 0.7382338412101224, precision: 0.7360781990521327, recall: 0.8971119133574007\n",
      "train: 2019-01-04T16:11:48.707589: step 564, loss 0.10993022471666336, acc 0.7586977338014682, auc: 0.7953885048094408, precision: 0.7677392739273927, recall: 0.9060370009737099\n",
      "train: 2019-01-04T16:11:49.632178: step 565, loss 0.10953377187252045, acc 0.7422885572139304, auc: 0.7710542863842215, precision: 0.7546117546117546, recall: 0.8956211812627292\n",
      "train: 2019-01-04T16:11:50.821106: step 566, loss 0.10109101980924606, acc 0.7076271186440678, auc: 0.731396583385149, precision: 0.7181895815542272, recall: 0.8764981761334029\n",
      "train: 2019-01-04T16:11:52.555472: step 567, loss 0.06627415865659714, acc 0.8511018463371054, auc: 0.9388889009704908, precision: 0.863265306122449, recall: 0.8794178794178794\n",
      "train: 2019-01-04T16:11:53.971235: step 568, loss 0.0719059482216835, acc 0.8690828402366864, auc: 0.932361875113341, precision: 0.8978214176127647, recall: 0.9366197183098591\n",
      "train: 2019-01-04T16:11:55.634442: step 569, loss 0.08278094232082367, acc 0.8123938879456706, auc: 0.8790656708576936, precision: 0.830805439330544, recall: 0.9305799648506151\n",
      "train: 2019-01-04T16:11:56.693203: step 570, loss 0.07932184636592865, acc 0.8335574983187626, auc: 0.9065000449519015, precision: 0.8566820276497696, recall: 0.9099363680861479\n",
      "train: 2019-01-04T16:11:57.545896: step 571, loss 0.12100780010223389, acc 0.7661602621388144, auc: 0.7951072489422619, precision: 0.7540387722132472, recall: 0.9138521781693588\n",
      "train: 2019-01-04T16:11:58.903981: step 572, loss 0.09239514917135239, acc 0.7967418546365915, auc: 0.8033724308794552, precision: 0.802910855063675, recall: 0.9426842292630829\n",
      "train: 2019-01-04T16:11:59.891575: step 573, loss 0.08798545598983765, acc 0.739193083573487, auc: 0.7151619860012233, precision: 0.7559055118110236, recall: 0.9249827942188575\n",
      "train: 2019-01-04T16:12:01.324402: step 574, loss 0.08992725610733032, acc 0.7226837060702875, auc: 0.7327557365133692, precision: 0.7365853658536585, recall: 0.9220291216533584\n",
      "train: 2019-01-04T16:12:02.714956: step 575, loss 0.07511577755212784, acc 0.8364914425427873, auc: 0.8701215748644544, precision: 0.8569861009509876, recall: 0.9420989143546441\n",
      "train: 2019-01-04T16:12:03.638132: step 576, loss 0.10844258964061737, acc 0.7411411411411412, auc: 0.8291157494284517, precision: 0.7395309882747069, recall: 0.8803589232303091\n",
      "train: 2019-01-04T16:12:04.658111: step 577, loss 0.08135172724723816, acc 0.7865853658536586, auc: 0.8528882522437672, precision: 0.8034447821681864, recall: 0.9204875217643644\n",
      "train: 2019-01-04T16:12:06.356479: step 578, loss 0.07500800490379333, acc 0.8721361185983828, auc: 0.9401320218149577, precision: 0.8843736908253037, recall: 0.953262587491533\n",
      "train: 2019-01-04T16:12:07.225118: step 579, loss 0.09577339142560959, acc 0.7433826828174069, auc: 0.74037151408618, precision: 0.756827731092437, recall: 0.9296774193548387\n",
      "train: 2019-01-04T16:12:08.624941: step 580, loss 0.06841270625591278, acc 0.8441907320349228, auc: 0.8837323328145985, precision: 0.8626524990161354, recall: 0.9501517121803208\n",
      "train: 2019-01-04T16:12:09.665948: step 581, loss 0.10870932042598724, acc 0.8005945575120055, auc: 0.789091420257259, precision: 0.8143148242485991, recall: 0.957185628742515\n",
      "train: 2019-01-04T16:12:13.458001: step 582, loss 0.05988314002752304, acc 0.8635773864689528, auc: 0.9318578946687416, precision: 0.8546962804388547, recall: 0.9430174195453204\n",
      "train: 2019-01-04T16:12:15.684147: step 583, loss 0.07281042635440826, acc 0.840395243677776, auc: 0.9217131135925327, precision: 0.8314713572617067, recall: 0.9347942276857295\n",
      "train: 2019-01-04T16:12:17.066948: step 584, loss 0.07912507653236389, acc 0.8391043397968606, auc: 0.8947523584905661, precision: 0.854815661617605, recall: 0.940566037735849\n",
      "train: 2019-01-04T16:12:17.666226: step 585, loss 0.14147426187992096, acc 0.709353673223605, auc: 0.7591679585075776, precision: 0.7331825890333522, recall: 0.8373143963847643\n",
      "train: 2019-01-04T16:12:18.728242: step 586, loss 0.10275658965110779, acc 0.7278933680104032, auc: 0.7577221398272866, precision: 0.7285652735889703, recall: 0.8909378292939937\n",
      "train: 2019-01-04T16:12:19.053936: step 587, loss 0.14075641334056854, acc 0.7371392722710163, auc: 0.7564885093822475, precision: 0.7550200803212851, recall: 0.8918406072106262\n",
      "train: 2019-01-04T16:12:20.602166: step 588, loss 0.10993271321058273, acc 0.6941526263627353, auc: 0.7139255537274809, precision: 0.7146766169154228, recall: 0.8788620373202815\n",
      "train: 2019-01-04T16:12:21.514016: step 589, loss 0.06950029730796814, acc 0.8259791769955379, auc: 0.8895825695404083, precision: 0.8359621451104101, recall: 0.9357344632768362\n",
      "train: 2019-01-04T16:12:23.234842: step 590, loss 0.08616530895233154, acc 0.7753419147224457, auc: 0.8535112188281202, precision: 0.773955773955774, recall: 0.9074903969270166\n",
      "train: 2019-01-04T16:12:23.533828: step 591, loss 0.16748271882534027, acc 0.6831882116543871, auc: 0.6998206166724956, precision: 0.704052780395853, recall: 0.8245033112582781\n",
      "train: 2019-01-04T16:12:24.917243: step 592, loss 0.07361412048339844, acc 0.8626028059990324, auc: 0.9116824314794646, precision: 0.8741445998214817, recall: 0.9529678884203697\n",
      "train: 2019-01-04T16:12:26.582580: step 593, loss 0.08283299952745438, acc 0.8570279832743648, auc: 0.8933518105688453, precision: 0.8606198897128732, recall: 0.9666809055958991\n",
      "train: 2019-01-04T16:12:28.051830: step 594, loss 0.08856699615716934, acc 0.7858158910790489, auc: 0.8362903574449551, precision: 0.804920049200492, recall: 0.9261251061420889\n",
      "train: 2019-01-04T16:12:30.992294: step 595, loss 0.06767358630895615, acc 0.8294726466239527, auc: 0.8882970806904265, precision: 0.8341873334699733, recall: 0.9467317980925797\n",
      "train: 2019-01-04T16:12:31.789868: step 596, loss 0.10427958518266678, acc 0.7757009345794392, auc: 0.7526970077688662, precision: 0.7883877159309021, recall: 0.9367160775370581\n",
      "train: 2019-01-04T16:12:33.134573: step 597, loss 0.10717371851205826, acc 0.7728246892413202, auc: 0.7436784298321115, precision: 0.7827618600281823, recall: 0.9610726643598616\n",
      "train: 2019-01-04T16:12:34.323615: step 598, loss 0.07847503572702408, acc 0.8423153692614771, auc: 0.899678213970094, precision: 0.8489443948855189, recall: 0.9583752937227258\n",
      "train: 2019-01-04T16:12:35.882255: step 599, loss 0.10135523974895477, acc 0.7729591836734694, auc: 0.8434840538724564, precision: 0.782147315855181, recall: 0.9088491295938105\n",
      "train: 2019-01-04T16:12:37.998260: step 600, loss 0.08495502918958664, acc 0.7546974659541459, auc: 0.8479156126496703, precision: 0.7411880700024649, recall: 0.8896449704142012\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:12:50.689213, step: 600, loss: 0.09700312193196553, acc: 0.7752041872638549, auc: 0.811424021218468, precision: 0.7770628473936354, recall: 0.9303863792094901\n",
      "Saved model checkpoint to model/my-model-600\n",
      "\n",
      "train: 2019-01-04T16:12:51.700914: step 601, loss 0.09947989135980606, acc 0.6960742929506121, auc: 0.7427196658938233, precision: 0.6940463065049615, recall: 0.8841292134831461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:12:54.875355: step 602, loss 0.05778505653142929, acc 0.8920618731939487, auc: 0.9514959627606965, precision: 0.888677901732732, recall: 0.957544757033248\n",
      "train: 2019-01-04T16:12:55.891112: step 603, loss 0.09929025918245316, acc 0.7782888684452622, auc: 0.7835574156665208, precision: 0.7913074712643678, recall: 0.9394456289978678\n",
      "train: 2019-01-04T16:12:57.312550: step 604, loss 0.10787048935890198, acc 0.7307279364397681, auc: 0.7479954839342726, precision: 0.7364072494669509, recall: 0.9124834874504624\n",
      "train: 2019-01-04T16:12:58.474586: step 605, loss 0.1012277603149414, acc 0.7163790296320417, auc: 0.7498798506944628, precision: 0.7093658126837463, recall: 0.9041755888650964\n",
      "train: 2019-01-04T16:12:59.460916: step 606, loss 0.09153982996940613, acc 0.7654916512059369, auc: 0.7196645757996424, precision: 0.7857142857142857, recall: 0.9387550200803213\n",
      "train: 2019-01-04T16:12:59.896631: step 607, loss 0.10750787705183029, acc 0.7292817679558011, auc: 0.748534302081878, precision: 0.7502507522567703, recall: 0.8883610451306413\n",
      "train: 2019-01-04T16:13:01.436462: step 608, loss 0.06718471646308899, acc 0.8656927426955702, auc: 0.9302817130136639, precision: 0.8733950432965064, recall: 0.9524584825789645\n",
      "train: 2019-01-04T16:13:02.435325: step 609, loss 0.08876481652259827, acc 0.7436219155165202, auc: 0.7527174487101361, precision: 0.7557213930348259, recall: 0.9256550883607556\n",
      "train: 2019-01-04T16:13:03.560470: step 610, loss 0.1192866712808609, acc 0.7155603917301414, auc: 0.7455107888183874, precision: 0.7344664251880747, recall: 0.8816053511705686\n",
      "train: 2019-01-04T16:13:12.809139: step 611, loss 0.04968724772334099, acc 0.9271981776765376, auc: 0.9627210197875956, precision: 0.9299171062547099, recall: 0.9831550193489642\n",
      "train: 2019-01-04T16:13:13.605706: step 612, loss 0.12018035352230072, acc 0.6898892461593427, auc: 0.7354350220264317, precision: 0.6898854961832062, recall: 0.8689903846153846\n",
      "train: 2019-01-04T16:13:14.568776: step 613, loss 0.09077707678079605, acc 0.807983682983683, auc: 0.8206286780282107, precision: 0.8187035786630655, recall: 0.9521005104043974\n",
      "train: 2019-01-04T16:13:15.181811: step 614, loss 0.11331470310688019, acc 0.6754257907542579, auc: 0.7329756173792399, precision: 0.6659125188536953, recall: 0.7976513098464318\n",
      "train: 2019-01-04T16:13:16.651152: step 615, loss 0.11219947785139084, acc 0.7065644286515803, auc: 0.7360022485935731, precision: 0.72265625, recall: 0.8723843206601827\n",
      "train: 2019-01-04T16:13:17.613936: step 616, loss 0.10042554885149002, acc 0.6813620071684587, auc: 0.7046647432905708, precision: 0.7057115198451114, recall: 0.8384128809660725\n",
      "train: 2019-01-04T16:13:18.966887: step 617, loss 0.09971559047698975, acc 0.7114939573155052, auc: 0.6949429953232236, precision: 0.7323943661971831, recall: 0.8888888888888888\n",
      "train: 2019-01-04T16:13:20.358899: step 618, loss 0.0934685543179512, acc 0.7307901907356948, auc: 0.7369704258575064, precision: 0.761114797611148, recall: 0.8953942232630757\n",
      "train: 2019-01-04T16:13:21.279376: step 619, loss 0.1098875105381012, acc 0.7744505494505495, auc: 0.7920248850623541, precision: 0.7948634590377113, recall: 0.9278937381404174\n",
      "train: 2019-01-04T16:13:21.614262: step 620, loss 0.12857942283153534, acc 0.7407137654770576, auc: 0.8215952869418216, precision: 0.6997422680412371, recall: 0.8153153153153153\n",
      "train: 2019-01-04T16:13:22.726302: step 621, loss 0.10142078250646591, acc 0.7481137550783518, auc: 0.7733653534267116, precision: 0.7468643101482326, recall: 0.906783571758191\n",
      "train: 2019-01-04T16:13:24.517455: step 622, loss 0.10152991116046906, acc 0.7484398216939079, auc: 0.7775678062289226, precision: 0.7652279846849983, recall: 0.9274414680447163\n",
      "train: 2019-01-04T16:13:25.657014: step 623, loss 0.10594792664051056, acc 0.7391790002792517, auc: 0.7501479607183441, precision: 0.7452120383036935, recall: 0.9201858108108109\n",
      "train: 2019-01-04T16:13:27.435943: step 624, loss 0.08216910064220428, acc 0.7239136091595108, auc: 0.7890762372675556, precision: 0.7337142857142858, recall: 0.8417832167832168\n",
      "train: 2019-01-04T16:13:28.526127: step 625, loss 0.1014624834060669, acc 0.7626755534396572, auc: 0.8154475976918092, precision: 0.7666136724960254, recall: 0.9016454749439042\n",
      "train: 2019-01-04T16:13:29.970238: step 626, loss 0.0817289873957634, acc 0.8175895765472313, auc: 0.8902877876092163, precision: 0.8208685162846804, recall: 0.9347527472527473\n",
      "train: 2019-01-04T16:13:31.251796: step 627, loss 0.10632622241973877, acc 0.75767987065481, auc: 0.8214027558924308, precision: 0.7634060900026947, recall: 0.8982244768547876\n",
      "train: 2019-01-04T16:13:33.019062: step 628, loss 0.07020052522420883, acc 0.8513914656771799, auc: 0.9099236089284739, precision: 0.866351138156412, recall: 0.9486673247778875\n",
      "train: 2019-01-04T16:13:33.354938: step 629, loss 0.1182631105184555, acc 0.7303958177744585, auc: 0.7802068619215445, precision: 0.743863393810032, recall: 0.8520782396088019\n",
      "train: 2019-01-04T16:13:34.333338: step 630, loss 0.08773826062679291, acc 0.830820770519263, auc: 0.9103959532530962, precision: 0.8379552288673572, recall: 0.9186813186813186\n",
      "train: 2019-01-04T16:13:35.502181: step 631, loss 0.08807670325040817, acc 0.7997333333333333, auc: 0.7899031300107212, precision: 0.8234036052551176, recall: 0.939679218967922\n",
      "train: 2019-01-04T16:13:36.565725: step 632, loss 0.08438637852668762, acc 0.7097933513027853, auc: 0.7242272129157457, precision: 0.7201101928374656, recall: 0.9044982698961938\n",
      "train: 2019-01-04T16:13:37.704764: step 633, loss 0.09070012718439102, acc 0.7369990062934747, auc: 0.7026797769764745, precision: 0.7534039334341907, recall: 0.9334582942830365\n",
      "train: 2019-01-04T16:13:38.897754: step 634, loss 0.08955417573451996, acc 0.7141848976711362, auc: 0.7247624180889654, precision: 0.7191989551589029, recall: 0.9091909741331866\n",
      "train: 2019-01-04T16:13:40.004143: step 635, loss 0.08471798151731491, acc 0.7913399249914763, auc: 0.762115890357767, precision: 0.807909604519774, recall: 0.9363891487371375\n",
      "train: 2019-01-04T16:13:40.998472: step 636, loss 0.08212434500455856, acc 0.7042726836293807, auc: 0.7274179408016763, precision: 0.713422007255139, recall: 0.8925869894099848\n",
      "train: 2019-01-04T16:13:42.881890: step 637, loss 0.06970423460006714, acc 0.8094736842105263, auc: 0.8758906229503842, precision: 0.8161745144985368, recall: 0.9347958561852528\n",
      "train: 2019-01-04T16:13:44.136826: step 638, loss 0.0823398008942604, acc 0.7790190735694823, auc: 0.8396951080360658, precision: 0.7827278958190541, recall: 0.9280780170662333\n",
      "train: 2019-01-04T16:13:45.510864: step 639, loss 0.08031916618347168, acc 0.8091658583899127, auc: 0.86363077265459, precision: 0.8163771712158809, recall: 0.9310222851078882\n",
      "train: 2019-01-04T16:13:46.459463: step 640, loss 0.09505113214254379, acc 0.8246753246753247, auc: 0.8705187570755393, precision: 0.833752444816988, recall: 0.9488076311605723\n",
      "train: 2019-01-04T16:13:47.602255: step 641, loss 0.07832197099924088, acc 0.7774597495527729, auc: 0.8273180032524785, precision: 0.7903854482459939, recall: 0.9296994396332144\n",
      "train: 2019-01-04T16:13:49.197361: step 642, loss 0.05844205990433693, acc 0.8907641819034224, auc: 0.9495388466855024, precision: 0.9076023391812865, recall: 0.9539028887523049\n",
      "train: 2019-01-04T16:13:51.114118: step 643, loss 0.06153133884072304, acc 0.860862320349927, auc: 0.934680258530069, precision: 0.8771025501899078, recall: 0.9376450116009281\n",
      "train: 2019-01-04T16:13:52.906484: step 644, loss 0.0566609762609005, acc 0.8682089940213154, auc: 0.9411399180343445, precision: 0.8703130374957, recall: 0.9511278195488722\n",
      "train: 2019-01-04T16:13:54.087116: step 645, loss 0.11168771237134933, acc 0.7951666341843695, auc: 0.7730410939261864, precision: 0.816941694169417, recall: 0.9443031536113937\n",
      "train: 2019-01-04T16:13:55.072484: step 646, loss 0.08893580734729767, acc 0.7147360126083531, auc: 0.7398852755785296, precision: 0.719856336582863, recall: 0.8874130297280203\n",
      "train: 2019-01-04T16:13:58.371207: step 647, loss 0.06970516592264175, acc 0.8329297820823245, auc: 0.8914533929356245, precision: 0.8275591785546323, recall: 0.9496312286382443\n",
      "train: 2019-01-04T16:13:58.589797: step 648, loss 0.1292911171913147, acc 0.7471022128556375, auc: 0.773655966648162, precision: 0.7703703703703704, recall: 0.9203539823008849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:14:00.496419: step 649, loss 0.0849335789680481, acc 0.7453610141466104, auc: 0.8030819015525215, precision: 0.7575357535753575, recall: 0.9237993023879796\n",
      "train: 2019-01-04T16:14:01.758787: step 650, loss 0.08920568972826004, acc 0.7366536458333334, auc: 0.7194167154548666, precision: 0.7411373707533235, recall: 0.948936170212766\n",
      "train: 2019-01-04T16:14:02.602551: step 651, loss 0.08586824685335159, acc 0.7704194260485652, auc: 0.8532384158585602, precision: 0.7761664564943254, recall: 0.8818051575931232\n",
      "train: 2019-01-04T16:14:03.486535: step 652, loss 0.08262622356414795, acc 0.7471461187214612, auc: 0.7555331670184866, precision: 0.7588703837798697, recall: 0.9050086355785838\n",
      "train: 2019-01-04T16:14:04.382828: step 653, loss 0.08697233349084854, acc 0.7920059215396003, auc: 0.8521453754920609, precision: 0.8227294803302574, recall: 0.8958223162347964\n",
      "train: 2019-01-04T16:14:05.549943: step 654, loss 0.10176432132720947, acc 0.7283051834595224, auc: 0.752092591266309, precision: 0.7468680913780398, recall: 0.891772987241531\n",
      "train: 2019-01-04T16:14:07.284477: step 655, loss 0.08978250622749329, acc 0.8172268907563025, auc: 0.8752315374769882, precision: 0.8235599377270368, recall: 0.9435196195005945\n",
      "train: 2019-01-04T16:14:08.158544: step 656, loss 0.10345916450023651, acc 0.7500720668780628, auc: 0.7420969432893378, precision: 0.77338736913205, recall: 0.9211584875301689\n",
      "train: 2019-01-04T16:14:09.465049: step 657, loss 0.09353218227624893, acc 0.8089430894308943, auc: 0.8723456184841403, precision: 0.8277925531914894, recall: 0.9247338450111414\n",
      "train: 2019-01-04T16:14:12.241629: step 658, loss 0.08230499923229218, acc 0.7973905542376537, auc: 0.8661379529941233, precision: 0.80471659081506, recall: 0.9264845982851699\n",
      "train: 2019-01-04T16:14:13.820007: step 659, loss 0.08093510568141937, acc 0.7910846343467544, auc: 0.8851412309558229, precision: 0.7867111650485437, recall: 0.891984864121087\n",
      "train: 2019-01-04T16:14:14.993875: step 660, loss 0.08083245903253555, acc 0.775386480751743, auc: 0.8229436255692852, precision: 0.7838627236217598, recall: 0.9351045296167247\n",
      "train: 2019-01-04T16:14:15.300031: step 661, loss 0.1516779065132141, acc 0.7422062350119905, auc: 0.8058854093690159, precision: 0.7283393501805054, recall: 0.8621794871794872\n",
      "train: 2019-01-04T16:14:16.335548: step 662, loss 0.10656146705150604, acc 0.7418978782933084, auc: 0.8301590247662045, precision: 0.758502232909653, recall: 0.8453292496171516\n",
      "train: 2019-01-04T16:14:17.332350: step 663, loss 0.08513616770505905, acc 0.8216610257745379, auc: 0.8782580945185527, precision: 0.8394736842105263, recall: 0.9283375773008367\n",
      "train: 2019-01-04T16:14:19.055735: step 664, loss 0.11018694192171097, acc 0.7277657266811279, auc: 0.799781631342325, precision: 0.7568192325473879, recall: 0.822819803970847\n",
      "train: 2019-01-04T16:14:20.168976: step 665, loss 0.08964769542217255, acc 0.7951132603715958, auc: 0.8314318567833278, precision: 0.8201768752047167, recall: 0.9072463768115943\n",
      "train: 2019-01-04T16:14:21.692379: step 666, loss 0.06567186117172241, acc 0.7962962962962963, auc: 0.8930539288781918, precision: 0.7812660833762224, recall: 0.8659440958357102\n",
      "train: 2019-01-04T16:14:23.148774: step 667, loss 0.10945099592208862, acc 0.7618832050701675, auc: 0.8324559912935348, precision: 0.7766785939802147, recall: 0.8771095792726409\n",
      "train: 2019-01-04T16:14:24.293584: step 668, loss 0.10664796829223633, acc 0.7871714758316486, auc: 0.8038414472720719, precision: 0.8027812895069533, recall: 0.9449404761904762\n",
      "train: 2019-01-04T16:14:25.429978: step 669, loss 0.0889742448925972, acc 0.725997506234414, auc: 0.7333470437657434, precision: 0.7482705611068409, recall: 0.8968217411331184\n",
      "train: 2019-01-04T16:14:27.578379: step 670, loss 0.05551261082291603, acc 0.8547409040793826, auc: 0.9237342150810544, precision: 0.8411764705882353, recall: 0.9601342845153168\n",
      "train: 2019-01-04T16:14:28.989096: step 671, loss 0.060701508074998856, acc 0.8756388415672913, auc: 0.9447219767129078, precision: 0.8932651834034877, recall: 0.9501119283658459\n",
      "train: 2019-01-04T16:14:30.483925: step 672, loss 0.07712423801422119, acc 0.8537727666955768, auc: 0.8889791577271794, precision: 0.8509771986970684, recall: 0.9741319039850851\n",
      "train: 2019-01-04T16:14:31.877504: step 673, loss 0.07884487509727478, acc 0.662528216704289, auc: 0.6782501157528414, precision: 0.6691176470588235, recall: 0.8916105327617881\n",
      "train: 2019-01-04T16:14:32.979747: step 674, loss 0.08561493456363678, acc 0.7253333333333334, auc: 0.7309743724883061, precision: 0.7291483757682178, recall: 0.9410764872521247\n",
      "train: 2019-01-04T16:14:34.063067: step 675, loss 0.08563075214624405, acc 0.7688412272862675, auc: 0.8331214419690691, precision: 0.7691723614274867, recall: 0.9234275296262534\n",
      "train: 2019-01-04T16:14:43.568416: step 676, loss 0.04200338199734688, acc 0.9554381788424342, auc: 0.9870783942462976, precision: 0.956702025072324, recall: 0.9851057491808162\n",
      "train: 2019-01-04T16:14:44.470926: step 677, loss 0.0989251583814621, acc 0.7777777777777778, auc: 0.8003543372153237, precision: 0.7768560763168809, recall: 0.9203931203931204\n",
      "train: 2019-01-04T16:14:46.194949: step 678, loss 0.07179822027683258, acc 0.7261715014344916, auc: 0.784977599673574, precision: 0.7292899408284024, recall: 0.8267188373392957\n",
      "train: 2019-01-04T16:14:47.759905: step 679, loss 0.07746347784996033, acc 0.8263927447714233, auc: 0.8644563379517582, precision: 0.8401200171453065, recall: 0.953307392996109\n",
      "train: 2019-01-04T16:14:49.085632: step 680, loss 0.09413094073534012, acc 0.6780618311533888, auc: 0.6951920724247613, precision: 0.7010231148162183, recall: 0.8628731343283582\n",
      "train: 2019-01-04T16:14:50.770355: step 681, loss 0.06601923704147339, acc 0.843421052631579, auc: 0.9131432551733905, precision: 0.8674425205790519, recall: 0.9252194974265819\n",
      "train: 2019-01-04T16:14:51.765409: step 682, loss 0.08234726637601852, acc 0.7787709497206704, auc: 0.7681040802385831, precision: 0.7917026793431288, recall: 0.9423868312757202\n",
      "train: 2019-01-04T16:14:53.286688: step 683, loss 0.06886454671621323, acc 0.8124669836238775, auc: 0.8824014229757378, precision: 0.811577752553916, recall: 0.9100551548578701\n",
      "train: 2019-01-04T16:14:53.670255: step 684, loss 0.10252755880355835, acc 0.7441673370876911, auc: 0.7491703196265994, precision: 0.7687564234326825, recall: 0.8894173602853745\n",
      "train: 2019-01-04T16:14:54.797954: step 685, loss 0.11062745749950409, acc 0.7656110615521855, auc: 0.7111510438281701, precision: 0.7848230201171378, recall: 0.9373479318734793\n",
      "train: 2019-01-04T16:14:55.886948: step 686, loss 0.09488841891288757, acc 0.6998112020138452, auc: 0.7382615350218157, precision: 0.7097495527728086, recall: 0.8387949260042283\n",
      "train: 2019-01-04T16:14:57.644317: step 687, loss 0.08733393251895905, acc 0.7323221971715516, auc: 0.774298719722414, precision: 0.743103928670939, recall: 0.8741396263520157\n",
      "train: 2019-01-04T16:14:58.716990: step 688, loss 0.05308028310537338, acc 0.8735868448098664, auc: 0.9121969021854446, precision: 0.8862973760932945, recall: 0.9675366008911521\n",
      "train: 2019-01-04T16:15:01.654855: step 689, loss 0.04769337549805641, acc 0.8977009445731599, auc: 0.9681166055055219, precision: 0.9027916251246261, recall: 0.9516552811350499\n",
      "train: 2019-01-04T16:15:02.759363: step 690, loss 0.09300311654806137, acc 0.7011939335269441, auc: 0.6944528346831824, precision: 0.7131952017448201, recall: 0.9347308242020009\n",
      "train: 2019-01-04T16:15:04.223870: step 691, loss 0.09829625487327576, acc 0.784833091436865, auc: 0.7893775378724394, precision: 0.8003236900667611, recall: 0.9521058965102286\n",
      "train: 2019-01-04T16:15:05.517466: step 692, loss 0.0813804641366005, acc 0.834114139693356, auc: 0.901486898716321, precision: 0.8475129111171514, recall: 0.934652278177458\n",
      "train: 2019-01-04T16:15:06.876594: step 693, loss 0.08937069773674011, acc 0.7698467169983986, auc: 0.8211111183193514, precision: 0.7799879445449066, recall: 0.9036312849162011\n",
      "train: 2019-01-04T16:15:07.729957: step 694, loss 0.09204152226448059, acc 0.7603719599427754, auc: 0.7877906238089187, precision: 0.767379679144385, recall: 0.9208556149732621\n",
      "train: 2019-01-04T16:15:08.894787: step 695, loss 0.08962468802928925, acc 0.6794529686457639, auc: 0.7094517191079913, precision: 0.6926464112725671, recall: 0.8567538126361656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:15:12.678358: step 696, loss 0.053115569055080414, acc 0.8762729898160815, auc: 0.9240052757392944, precision: 0.8730292200966996, recall: 0.9518679807471923\n",
      "train: 2019-01-04T16:15:13.138247: step 697, loss 0.10824133455753326, acc 0.776875298614429, auc: 0.854515591788662, precision: 0.715929203539823, recall: 0.8471204188481676\n",
      "train: 2019-01-04T16:15:14.649805: step 698, loss 0.061638254672288895, acc 0.8140963465890722, auc: 0.8732349007369331, precision: 0.8376518218623482, recall: 0.9224253232278199\n",
      "train: 2019-01-04T16:15:22.311856: step 699, loss 0.046561870723962784, acc 0.8859633517167764, auc: 0.9408062910738073, precision: 0.8992248062015504, recall: 0.9582315299592786\n",
      "train: 2019-01-04T16:15:23.256098: step 700, loss 0.11792723089456558, acc 0.6969525959367946, auc: 0.7690924399380599, precision: 0.6899521531100479, recall: 0.771948608137045\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:15:35.966478, step: 700, loss: 0.0887234963190097, acc: 0.7772196787273354, auc: 0.8136167687921321, precision: 0.7888952335381496, recall: 0.9083254866876858\n",
      "Saved model checkpoint to model/my-model-700\n",
      "\n",
      "train: 2019-01-04T16:15:37.598371: step 701, loss 0.07516863942146301, acc 0.6854381872709097, auc: 0.7212179578298981, precision: 0.7097209720972097, recall: 0.8406183368869936\n",
      "train: 2019-01-04T16:15:38.170877: step 702, loss 0.11845580488443375, acc 0.718343765045739, auc: 0.7803657480611399, precision: 0.7177177177177178, recall: 0.8206008583690987\n",
      "train: 2019-01-04T16:15:39.759710: step 703, loss 0.09259311109781265, acc 0.7722394033108968, auc: 0.796435572335231, precision: 0.7859678429734809, recall: 0.9431220245552493\n",
      "train: 2019-01-04T16:15:40.283906: step 704, loss 0.1456841379404068, acc 0.7200779727095517, auc: 0.7738372144554759, precision: 0.7250274423710209, recall: 0.8589076723016905\n",
      "train: 2019-01-04T16:15:41.298556: step 705, loss 0.10119178146123886, acc 0.7277568922305765, auc: 0.6877926187798844, precision: 0.7410201149425287, recall: 0.9330619629127092\n",
      "train: 2019-01-04T16:15:43.058130: step 706, loss 0.07659102976322174, acc 0.6571901738890422, auc: 0.7077714173027658, precision: 0.6579256360078278, recall: 0.8204001952171791\n",
      "train: 2019-01-04T16:15:44.004236: step 707, loss 0.09049294143915176, acc 0.7954607601859448, auc: 0.8338216810398065, precision: 0.8040118382111148, recall: 0.941470927993839\n",
      "train: 2019-01-04T16:15:45.786034: step 708, loss 0.07597541064023972, acc 0.7928530479648386, auc: 0.850866802338041, precision: 0.7966265060240963, recall: 0.9323181049069373\n",
      "train: 2019-01-04T16:15:47.097907: step 709, loss 0.07555516064167023, acc 0.7474882786336235, auc: 0.742018970189702, precision: 0.7585794094173982, recall: 0.9273170731707318\n",
      "train: 2019-01-04T16:15:48.118569: step 710, loss 0.096967913210392, acc 0.72724399494311, auc: 0.7089374331959077, precision: 0.7512032580525732, recall: 0.913963963963964\n",
      "train: 2019-01-04T16:15:49.911149: step 711, loss 0.08921618014574051, acc 0.7768045417680454, auc: 0.8276296077286116, precision: 0.7909768211920529, recall: 0.9126074498567335\n",
      "train: 2019-01-04T16:15:51.217860: step 712, loss 0.07302699238061905, acc 0.7311744049941474, auc: 0.7849218168765058, precision: 0.7313883299798792, recall: 0.9036668738346799\n",
      "train: 2019-01-04T16:15:52.139481: step 713, loss 0.08080296963453293, acc 0.7987536656891495, auc: 0.7847411821133412, precision: 0.8183299389002037, recall: 0.9512310606060606\n",
      "train: 2019-01-04T16:15:53.612494: step 714, loss 0.0773380696773529, acc 0.6631363170481722, auc: 0.6984330098826388, precision: 0.6843109203624225, recall: 0.8158044343376919\n",
      "train: 2019-01-04T16:15:54.464743: step 715, loss 0.09905126690864563, acc 0.757504215851602, auc: 0.7155578827725972, precision: 0.7776946107784432, recall: 0.9432591920108943\n",
      "train: 2019-01-04T16:15:56.001434: step 716, loss 0.09137175977230072, acc 0.7047830923248053, auc: 0.7158036108750307, precision: 0.7179689555380163, recall: 0.914544235924933\n",
      "train: 2019-01-04T16:15:57.465673: step 717, loss 0.057573623955249786, acc 0.8483130138928268, auc: 0.9321818756796749, precision: 0.8598277212216131, recall: 0.9254108723135271\n",
      "train: 2019-01-04T16:15:58.606274: step 718, loss 0.09424495697021484, acc 0.7494226327944573, auc: 0.8203471755356743, precision: 0.7608422375864237, recall: 0.8819672131147541\n",
      "train: 2019-01-04T16:15:59.383397: step 719, loss 0.09116481244564056, acc 0.7071722247601645, auc: 0.6892472203312912, precision: 0.7218649517684887, recall: 0.9169503063308373\n",
      "train: 2019-01-04T16:16:00.602535: step 720, loss 0.08292447030544281, acc 0.7712096332785988, auc: 0.8109097780217209, precision: 0.7800322061191627, recall: 0.9405825242718446\n",
      "train: 2019-01-04T16:16:02.268875: step 721, loss 0.05658331885933876, acc 0.8302848575712144, auc: 0.9125227830805424, precision: 0.8434178250204415, recall: 0.9185218165627783\n",
      "train: 2019-01-04T16:16:03.751851: step 722, loss 0.07634218037128448, acc 0.8009817958682757, auc: 0.8721474954661435, precision: 0.81425, recall: 0.9340407226842558\n",
      "train: 2019-01-04T16:16:04.225784: step 723, loss 0.127091184258461, acc 0.7110785749145925, auc: 0.7352248521079688, precision: 0.7268777157045313, recall: 0.8851095993953136\n",
      "train: 2019-01-04T16:16:05.604227: step 724, loss 0.1095389723777771, acc 0.7662315930388219, auc: 0.7791460393239282, precision: 0.779618082618862, recall: 0.9376611202249824\n",
      "train: 2019-01-04T16:16:06.801731: step 725, loss 0.07751196622848511, acc 0.7168968318440292, auc: 0.7026213469495988, precision: 0.7447432762836186, recall: 0.8969375736160189\n",
      "train: 2019-01-04T16:16:07.642537: step 726, loss 0.0792853981256485, acc 0.711088504577823, auc: 0.734280407264722, precision: 0.743020304568528, recall: 0.8778110944527736\n",
      "train: 2019-01-04T16:16:09.266400: step 727, loss 0.10390382260084152, acc 0.6986472054111783, auc: 0.72084385656811, precision: 0.7095343680709535, recall: 0.8931063354730673\n",
      "train: 2019-01-04T16:16:10.633774: step 728, loss 0.09311109036207199, acc 0.6937649880095923, auc: 0.748888408240226, precision: 0.6970853573907009, recall: 0.8325735598839619\n",
      "train: 2019-01-04T16:16:12.262977: step 729, loss 0.07774265110492706, acc 0.8332739579622372, auc: 0.8444259477721034, precision: 0.8359966358284272, recall: 0.9622458857696031\n",
      "train: 2019-01-04T16:16:14.355057: step 730, loss 0.05089470371603966, acc 0.8821048403293834, auc: 0.9531311730729674, precision: 0.8797996661101837, recall: 0.9532710280373832\n",
      "train: 2019-01-04T16:16:15.323833: step 731, loss 0.08924262225627899, acc 0.7823323800444868, auc: 0.7801475016641484, precision: 0.7937984496124031, recall: 0.9304861426624261\n",
      "train: 2019-01-04T16:16:16.536686: step 732, loss 0.07139196246862411, acc 0.8468512619456016, auc: 0.8704979893882241, precision: 0.8542936288088643, recall: 0.9688972667295005\n",
      "train: 2019-01-04T16:16:17.348008: step 733, loss 0.11513528972864151, acc 0.7879642689233662, auc: 0.7943620181251365, precision: 0.7945886250690226, recall: 0.9479578392621871\n",
      "train: 2019-01-04T16:16:18.742622: step 734, loss 0.07848452031612396, acc 0.6888489208633094, auc: 0.7499076859829443, precision: 0.686046511627907, recall: 0.8475952529668956\n",
      "train: 2019-01-04T16:16:19.584941: step 735, loss 0.08895810693502426, acc 0.7482185273159145, auc: 0.8214384508990318, precision: 0.7335640138408305, recall: 0.8796680497925311\n",
      "train: 2019-01-04T16:16:20.635487: step 736, loss 0.07777045667171478, acc 0.7631578947368421, auc: 0.7238958710761791, precision: 0.7754906653901388, recall: 0.9429569266589057\n",
      "train: 2019-01-04T16:16:21.505270: step 737, loss 0.12013846635818481, acc 0.7307479224376732, auc: 0.7851164757448291, precision: 0.7426584234930448, recall: 0.8626570915619389\n",
      "train: 2019-01-04T16:16:23.388419: step 738, loss 0.0602656751871109, acc 0.8296515600091096, auc: 0.8861515935454014, precision: 0.8448375870069605, recall: 0.9318618042226487\n",
      "train: 2019-01-04T16:16:24.644580: step 739, loss 0.09390458464622498, acc 0.7040729136998006, auc: 0.7028484205189718, precision: 0.7258232235701907, recall: 0.8941076003415884\n",
      "train: 2019-01-04T16:16:25.493483: step 740, loss 0.09385835379362106, acc 0.7693089430894309, auc: 0.8286752067087754, precision: 0.7948600275355667, recall: 0.8809766022380467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:16:25.781532: step 741, loss 0.20013819634914398, acc 0.6851654215581644, auc: 0.7353991785281867, precision: 0.6824902723735409, recall: 0.8281397544853636\n",
      "train: 2019-01-04T16:16:27.088875: step 742, loss 0.09527431428432465, acc 0.7101262561195568, auc: 0.7363145047270753, precision: 0.7255520504731862, recall: 0.9001956947162426\n",
      "train: 2019-01-04T16:16:27.461333: step 743, loss 0.1415073573589325, acc 0.730072463768116, auc: 0.8172302744160155, precision: 0.7172464840858623, recall: 0.8191039729501268\n",
      "train: 2019-01-04T16:16:28.687231: step 744, loss 0.05865265056490898, acc 0.8619444444444444, auc: 0.9317916541752711, precision: 0.884828349944629, recall: 0.9283501161890008\n",
      "train: 2019-01-04T16:16:30.206195: step 745, loss 0.0765272006392479, acc 0.7141579731743666, auc: 0.7454808236684601, precision: 0.7416342412451362, recall: 0.8659700136301681\n",
      "train: 2019-01-04T16:16:31.657298: step 746, loss 0.05720669403672218, acc 0.8500823723228995, auc: 0.9320914317526158, precision: 0.865265760197775, recall: 0.9055627425614489\n",
      "train: 2019-01-04T16:16:32.184062: step 747, loss 0.14491812884807587, acc 0.707218761451081, auc: 0.7525662409375178, precision: 0.7219827586206896, recall: 0.8256315465187923\n",
      "train: 2019-01-04T16:16:33.436149: step 748, loss 0.07050611823797226, acc 0.6808149405772496, auc: 0.7373968195092082, precision: 0.690537084398977, recall: 0.8011869436201781\n",
      "train: 2019-01-04T16:16:34.862257: step 749, loss 0.09165720641613007, acc 0.7752327746741154, auc: 0.8412355286853614, precision: 0.8023966740034238, recall: 0.8915760869565217\n",
      "train: 2019-01-04T16:16:35.711332: step 750, loss 0.0836033746600151, acc 0.7033860045146727, auc: 0.7872292608580189, precision: 0.7166793602437167, recall: 0.767536704730832\n",
      "train: 2019-01-04T16:16:36.187142: step 751, loss 0.09455668181180954, acc 0.7403462050599201, auc: 0.7993010559155267, precision: 0.7581888246628131, recall: 0.8498920086393088\n",
      "train: 2019-01-04T16:16:37.214667: step 752, loss 0.11524415761232376, acc 0.7273924495171202, auc: 0.7566117067700648, precision: 0.7463184217838288, recall: 0.8908789386401327\n",
      "train: 2019-01-04T16:16:38.664397: step 753, loss 0.062072742730379105, acc 0.6920605782469023, auc: 0.6933767012895667, precision: 0.71875, recall: 0.8778625954198473\n",
      "train: 2019-01-04T16:16:39.230916: step 754, loss 0.08397132158279419, acc 0.7210365853658537, auc: 0.7705409710341573, precision: 0.7091319052987599, recall: 0.8534599728629579\n",
      "train: 2019-01-04T16:16:40.893274: step 755, loss 0.07086000591516495, acc 0.8433903034530869, auc: 0.8848396420313096, precision: 0.8515804898471844, recall: 0.9556025369978859\n",
      "train: 2019-01-04T16:16:42.048817: step 756, loss 0.08085446804761887, acc 0.7884615384615384, auc: 0.856155732177933, precision: 0.7863247863247863, recall: 0.8975609756097561\n",
      "train: 2019-01-04T16:16:42.800806: step 757, loss 0.09831711649894714, acc 0.7507788161993769, auc: 0.7752776640178901, precision: 0.7738867271941202, recall: 0.9008555611474585\n",
      "train: 2019-01-04T16:16:43.560115: step 758, loss 0.09534093737602234, acc 0.8050927381326627, auc: 0.8356100434215394, precision: 0.8336012861736335, recall: 0.9096491228070176\n",
      "train: 2019-01-04T16:16:45.397573: step 759, loss 0.0836198627948761, acc 0.685352233676976, auc: 0.7148071900025025, precision: 0.7011657662780779, recall: 0.85625\n",
      "train: 2019-01-04T16:16:46.126561: step 760, loss 0.09784699231386185, acc 0.7514644351464436, auc: 0.7561307901907357, precision: 0.7689969604863222, recall: 0.9166666666666666\n",
      "train: 2019-01-04T16:16:47.678534: step 761, loss 0.10005991905927658, acc 0.7522359956064648, auc: 0.8076161341671846, precision: 0.7525302639412582, recall: 0.9194956353055286\n",
      "train: 2019-01-04T16:16:49.296332: step 762, loss 0.05496389418840408, acc 0.845349165959853, auc: 0.9298945075868199, precision: 0.85041430440471, recall: 0.9052924791086351\n",
      "train: 2019-01-04T16:16:50.432461: step 763, loss 0.09265854209661484, acc 0.7398554752640356, auc: 0.753634567712218, precision: 0.7441624365482233, recall: 0.9243379571248423\n",
      "train: 2019-01-04T16:16:51.933736: step 764, loss 0.08033328503370285, acc 0.7154471544715447, auc: 0.7624707682833443, precision: 0.7185672514619883, recall: 0.8619026742656729\n",
      "train: 2019-01-04T16:16:53.553309: step 765, loss 0.07118100672960281, acc 0.8732548590199836, auc: 0.9152419015202943, precision: 0.8816568047337278, recall: 0.9673626952096859\n",
      "train: 2019-01-04T16:16:55.406915: step 766, loss 0.06703739613294601, acc 0.8009727626459144, auc: 0.8636540705542022, precision: 0.8166583291645823, recall: 0.9184247538677919\n",
      "train: 2019-01-04T16:16:56.376644: step 767, loss 0.06514078378677368, acc 0.7948421862971516, auc: 0.8525786185071207, precision: 0.8408759124087591, recall: 0.8934850051706308\n",
      "train: 2019-01-04T16:16:57.316303: step 768, loss 0.09224455058574677, acc 0.7254113345521024, auc: 0.7570614260423011, precision: 0.7404385517593065, recall: 0.8571428571428571\n",
      "train: 2019-01-04T16:16:58.445989: step 769, loss 0.08325423300266266, acc 0.7545857052498419, auc: 0.7687628873334902, precision: 0.7588792423046566, recall: 0.9209770114942529\n",
      "train: 2019-01-04T16:17:00.078575: step 770, loss 0.062093138694763184, acc 0.8178182626083093, auc: 0.9073687008148906, precision: 0.8358028424553976, recall: 0.9089115422558369\n",
      "train: 2019-01-04T16:17:01.472969: step 771, loss 0.06083741784095764, acc 0.8092643051771117, auc: 0.8486898164772562, precision: 0.8265813788201848, recall: 0.9424635332252836\n",
      "train: 2019-01-04T16:17:03.223314: step 772, loss 0.06914601475000381, acc 0.7896259557759868, auc: 0.8783175812173862, precision: 0.8069722401549386, recall: 0.8561643835616438\n",
      "train: 2019-01-04T16:17:04.446636: step 773, loss 0.08075793087482452, acc 0.7375068643602416, auc: 0.819052766878854, precision: 0.7500946611132147, recall: 0.8700043917435222\n",
      "train: 2019-01-04T16:17:05.331222: step 774, loss 0.07057196646928787, acc 0.8252773375594294, auc: 0.8619059204485882, precision: 0.8249619482496194, recall: 0.9442508710801394\n",
      "train: 2019-01-04T16:17:06.282833: step 775, loss 0.08521468937397003, acc 0.8413731209333632, auc: 0.8865883356571798, precision: 0.8528767123287672, recall: 0.9482180932074322\n",
      "train: 2019-01-04T16:17:07.749721: step 776, loss 0.08507158607244492, acc 0.7429569775060056, auc: 0.7621448645218583, precision: 0.7566579634464752, recall: 0.9220489977728286\n",
      "train: 2019-01-04T16:17:08.849902: step 777, loss 0.08309098333120346, acc 0.7695167286245354, auc: 0.8210240036928758, precision: 0.7823508894314615, recall: 0.9249484536082474\n",
      "train: 2019-01-04T16:17:09.913981: step 778, loss 0.06974995136260986, acc 0.8377316114542392, auc: 0.8941161320562918, precision: 0.8572905894519132, recall: 0.938136552244436\n",
      "train: 2019-01-04T16:17:10.214054: step 779, loss 0.16710740327835083, acc 0.6996587030716723, auc: 0.7655365603119366, precision: 0.6898305084745763, recall: 0.8340163934426229\n",
      "train: 2019-01-04T16:17:11.619447: step 780, loss 0.09051594138145447, acc 0.7834496510468594, auc: 0.8476503559651052, precision: 0.7697963800904978, recall: 0.9091516366065464\n",
      "train: 2019-01-04T16:17:12.807117: step 781, loss 0.09146686643362045, acc 0.7697351828499369, auc: 0.7455260248304534, precision: 0.789011611441518, recall: 0.943127962085308\n",
      "train: 2019-01-04T16:17:13.802314: step 782, loss 0.09054124355316162, acc 0.7150313152400835, auc: 0.7409792969150354, precision: 0.7247058823529412, recall: 0.8680947012401353\n",
      "train: 2019-01-04T16:17:14.837834: step 783, loss 0.07335349917411804, acc 0.8307591192901742, auc: 0.8387022128773118, precision: 0.8412267080745341, recall: 0.9533655961284646\n",
      "train: 2019-01-04T16:17:16.398196: step 784, loss 0.07971993088722229, acc 0.7616279069767442, auc: 0.7536665324087112, precision: 0.7786006128702758, recall: 0.9270294922468836\n",
      "train: 2019-01-04T16:17:17.320093: step 785, loss 0.10112952440977097, acc 0.7534602076124568, auc: 0.8043613374079492, precision: 0.772191673212883, recall: 0.8772869254796966\n",
      "train: 2019-01-04T16:17:18.526733: step 786, loss 0.09573324769735336, acc 0.7417852522639069, auc: 0.7155665369721605, precision: 0.7540889526542324, recall: 0.9490790899241603\n",
      "train: 2019-01-04T16:17:19.738819: step 787, loss 0.08324665576219559, acc 0.725705329153605, auc: 0.739008921006379, precision: 0.7355308547336145, recall: 0.9120722433460076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:17:20.316434: step 788, loss 0.11867163330316544, acc 0.6962190352020861, auc: 0.7364435063354388, precision: 0.711864406779661, recall: 0.8553370786516854\n",
      "train: 2019-01-04T16:17:21.459600: step 789, loss 0.05881252512335777, acc 0.8380721220527045, auc: 0.8808150009952619, precision: 0.8532110091743119, recall: 0.9467838963442851\n",
      "train: 2019-01-04T16:17:22.528148: step 790, loss 0.09521865844726562, acc 0.7824245632609846, auc: 0.768760598521072, precision: 0.7952706907280647, recall: 0.9397058823529412\n",
      "train: 2019-01-04T16:17:24.496698: step 791, loss 0.04731446132063866, acc 0.8901408450704226, auc: 0.9491315061571528, precision: 0.8988899613899614, recall: 0.9670301142263759\n",
      "train: 2019-01-04T16:17:25.692740: step 792, loss 0.06791224330663681, acc 0.8488155443172745, auc: 0.8962984530371609, precision: 0.8491228070175438, recall: 0.946051602814699\n",
      "train: 2019-01-04T16:17:27.180491: step 793, loss 0.10867683589458466, acc 0.7215836526181354, auc: 0.7003936700167895, precision: 0.7366874770473743, recall: 0.928273947246645\n",
      "train: 2019-01-04T16:17:27.624706: step 794, loss 0.1300555169582367, acc 0.6907459867799811, auc: 0.723640982505903, precision: 0.6909090909090909, recall: 0.9019908116385911\n",
      "train: 2019-01-04T16:17:28.529571: step 795, loss 0.12216290831565857, acc 0.7349707602339182, auc: 0.7036051570247934, precision: 0.7590361445783133, recall: 0.9163636363636364\n",
      "train: 2019-01-04T16:17:30.219173: step 796, loss 0.08769773691892624, acc 0.7276338188008322, auc: 0.7565628944705989, precision: 0.7409931840311588, recall: 0.8900584795321638\n",
      "train: 2019-01-04T16:17:31.840441: step 797, loss 0.07559666037559509, acc 0.7966616084977238, auc: 0.8327988573102842, precision: 0.8190255220417634, recall: 0.9308526223263991\n",
      "train: 2019-01-04T16:17:33.502493: step 798, loss 0.04793299734592438, acc 0.8919952913478517, auc: 0.9342272296203559, precision: 0.8876404494382022, recall: 0.972507180960197\n",
      "train: 2019-01-04T16:17:35.268362: step 799, loss 0.1028713583946228, acc 0.7604982663413381, auc: 0.8066197290031984, precision: 0.7695574065000816, recall: 0.9121176926054975\n",
      "train: 2019-01-04T16:17:36.378874: step 800, loss 0.071406789124012, acc 0.7694962042788129, auc: 0.835297807433233, precision: 0.7834249084249084, recall: 0.897691500524659\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:17:49.067593, step: 800, loss: 0.08315700593476112, acc: 0.7771828814691657, auc: 0.8138933786029069, precision: 0.7882228372655444, recall: 0.909765027277584\n",
      "Saved model checkpoint to model/my-model-800\n",
      "\n",
      "train: 2019-01-04T16:17:50.213962: step 801, loss 0.09017368406057358, acc 0.7095834547043403, auc: 0.7082290280860205, precision: 0.7320861277797388, recall: 0.8970588235294118\n",
      "train: 2019-01-04T16:17:50.520514: step 802, loss 0.12592104077339172, acc 0.6983199415631848, auc: 0.722204002870736, precision: 0.7264325323475046, recall: 0.8704318936877077\n",
      "train: 2019-01-04T16:17:54.246350: step 803, loss 0.04900672286748886, acc 0.8234092694422623, auc: 0.9002793688214965, precision: 0.8193031811296256, recall: 0.929079754601227\n",
      "train: 2019-01-04T16:17:55.731558: step 804, loss 0.07228431105613708, acc 0.8191725529767911, auc: 0.8690875514762042, precision: 0.8212992030910408, recall: 0.9561428169806017\n",
      "train: 2019-01-04T16:17:56.765716: step 805, loss 0.06716788560152054, acc 0.7269267364414843, auc: 0.7782924197777212, precision: 0.727328431372549, recall: 0.9019756838905775\n",
      "train: 2019-01-04T16:17:58.836788: step 806, loss 0.0731726586818695, acc 0.8053194837897387, auc: 0.8421005763041387, precision: 0.8053030303030303, recall: 0.9531495180452814\n",
      "train: 2019-01-04T16:18:00.626878: step 807, loss 0.07877393066883087, acc 0.7440749235474006, auc: 0.8073004837460404, precision: 0.7586206896551724, recall: 0.9077973819009676\n",
      "train: 2019-01-04T16:18:01.940254: step 808, loss 0.04844270274043083, acc 0.8274250097389949, auc: 0.9069816050488118, precision: 0.8476141611082606, recall: 0.9187986651835373\n",
      "train: 2019-01-04T16:18:03.377471: step 809, loss 0.06006450206041336, acc 0.8208252561617281, auc: 0.8892510973666407, precision: 0.8041237113402062, recall: 0.9298486932599724\n",
      "train: 2019-01-04T16:18:04.333286: step 810, loss 0.05783829092979431, acc 0.7285245901639344, auc: 0.7019171304885591, precision: 0.7557781201848999, recall: 0.9100185528756958\n",
      "train: 2019-01-04T16:18:06.099084: step 811, loss 0.0597861185669899, acc 0.8344827586206897, auc: 0.9188690527778216, precision: 0.862235803412274, recall: 0.9070452718992768\n",
      "train: 2019-01-04T16:18:07.687527: step 812, loss 0.056969527155160904, acc 0.8837812789620019, auc: 0.9312418982933399, precision: 0.8957856161245992, recall: 0.9578741121724222\n",
      "train: 2019-01-04T16:18:08.968924: step 813, loss 0.10026752948760986, acc 0.7618492618492618, auc: 0.7735284714832341, precision: 0.7805847953216374, recall: 0.9205517241379311\n",
      "train: 2019-01-04T16:18:10.506729: step 814, loss 0.06563562899827957, acc 0.7257175993041461, auc: 0.8241000667010797, precision: 0.724977856510186, recall: 0.8343527013251784\n",
      "train: 2019-01-04T16:18:11.969265: step 815, loss 0.10468307882547379, acc 0.7632273079906905, auc: 0.7979276297186123, precision: 0.7822596061938444, recall: 0.9135967849966511\n",
      "train: 2019-01-04T16:18:14.220720: step 816, loss 0.05672542378306389, acc 0.8341163129758626, auc: 0.9192145843190164, precision: 0.8346491228070175, recall: 0.9337585868498528\n",
      "train: 2019-01-04T16:18:14.604902: step 817, loss 0.1329859495162964, acc 0.7015873015873015, auc: 0.7261940870750618, precision: 0.7181500872600349, recall: 0.8484536082474227\n",
      "train: 2019-01-04T16:18:16.060365: step 818, loss 0.0920908972620964, acc 0.7313463514902364, auc: 0.7158978982679213, precision: 0.7421413377452138, recall: 0.9356376638855781\n",
      "train: 2019-01-04T16:18:23.325631: step 819, loss 0.0384950116276741, acc 0.8935093290988487, auc: 0.9403354270656145, precision: 0.9030124040165387, recall: 0.9680851063829787\n",
      "train: 2019-01-04T16:18:24.523367: step 820, loss 0.06698298454284668, acc 0.7524292353189692, auc: 0.7831886828868169, precision: 0.7438340807174888, recall: 0.9114010989010989\n",
      "train: 2019-01-04T16:18:24.882494: step 821, loss 0.1458248347043991, acc 0.8077285579641847, auc: 0.8338454221822287, precision: 0.8056930693069307, recall: 0.9326647564469914\n",
      "train: 2019-01-04T16:18:26.972893: step 822, loss 0.05923445150256157, acc 0.8563813107557228, auc: 0.911067303267054, precision: 0.8532740501212611, recall: 0.956935630099728\n",
      "train: 2019-01-04T16:18:28.000891: step 823, loss 0.07996014505624771, acc 0.72782874617737, auc: 0.7242043130324918, precision: 0.7341361741547012, recall: 0.9199071387115496\n",
      "train: 2019-01-04T16:18:31.209399: step 824, loss 0.041396547108888626, acc 0.8812346688470973, auc: 0.9355782083275654, precision: 0.8725376593279258, recall: 0.9552806850618458\n",
      "train: 2019-01-04T16:18:32.449874: step 825, loss 0.07718484848737717, acc 0.7231638418079096, auc: 0.7583139002986331, precision: 0.7293519695044473, recall: 0.8763358778625954\n",
      "train: 2019-01-04T16:18:33.978684: step 826, loss 0.08961112797260284, acc 0.7494440326167532, auc: 0.7858565522143213, precision: 0.7641466727025803, recall: 0.9158979924036896\n",
      "train: 2019-01-04T16:18:35.478670: step 827, loss 0.07790613919496536, acc 0.7783018867924528, auc: 0.811449147247731, precision: 0.7941176470588235, recall: 0.925893635571055\n",
      "train: 2019-01-04T16:18:36.501829: step 828, loss 0.1044485941529274, acc 0.7582625118035883, auc: 0.8022992272495187, precision: 0.7672413793103449, recall: 0.9028985507246376\n",
      "train: 2019-01-04T16:18:37.294162: step 829, loss 0.12254159897565842, acc 0.7517577068685776, auc: 0.7512279990297461, precision: 0.7710604558969276, recall: 0.9120750293083235\n",
      "train: 2019-01-04T16:18:38.593942: step 830, loss 0.0748399868607521, acc 0.7887063179819486, auc: 0.845437963229839, precision: 0.8055004252906153, recall: 0.9260104302477183\n",
      "train: 2019-01-04T16:18:40.142205: step 831, loss 0.05782143026590347, acc 0.7109190887666929, auc: 0.7798592014419352, precision: 0.6668826960466624, recall: 0.8225419664268585\n",
      "train: 2019-01-04T16:18:40.397847: step 832, loss 0.15235953032970428, acc 0.6935598018400566, auc: 0.7635321894526877, precision: 0.6843177189409368, recall: 0.8452830188679246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:18:41.653609: step 833, loss 0.09390085190534592, acc 0.8214463015058746, auc: 0.8468372487567913, precision: 0.8326279338207002, recall: 0.953934317831166\n",
      "train: 2019-01-04T16:18:44.337540: step 834, loss 0.04628106206655502, acc 0.8767103347889375, auc: 0.9515649300028727, precision: 0.8833138856476079, recall: 0.9484234704531217\n",
      "train: 2019-01-04T16:18:45.953960: step 835, loss 0.05316237360239029, acc 0.868830591373944, auc: 0.9222106996735184, precision: 0.8880275624461671, recall: 0.9392651078044336\n",
      "train: 2019-01-04T16:18:47.400288: step 836, loss 0.05335957929491997, acc 0.8373639661426844, auc: 0.8829681333555071, precision: 0.8525807649461568, recall: 0.9421419778416086\n",
      "train: 2019-01-04T16:18:50.156046: step 837, loss 0.04171999543905258, acc 0.8596715717637022, auc: 0.924725335017378, precision: 0.8577860169491526, recall: 0.9639880952380953\n",
      "train: 2019-01-04T16:18:51.369911: step 838, loss 0.07035387307405472, acc 0.6941870261162595, auc: 0.6998154233658858, precision: 0.7099476439790576, recall: 0.887434554973822\n",
      "train: 2019-01-04T16:19:00.669270: step 839, loss 0.03634391352534294, acc 0.8875611080934275, auc: 0.9473646021276025, precision: 0.898585783401563, recall: 0.9641609264250773\n",
      "train: 2019-01-04T16:19:02.543286: step 840, loss 0.061427392065525055, acc 0.7993171471927162, auc: 0.8865229475956535, precision: 0.8228820066547223, recall: 0.8977939123149958\n",
      "train: 2019-01-04T16:19:03.372174: step 841, loss 0.08861561119556427, acc 0.701511879049676, auc: 0.7443080532387523, precision: 0.7, recall: 0.8601913171449596\n",
      "train: 2019-01-04T16:19:04.959052: step 842, loss 0.06149812042713165, acc 0.8052830188679245, auc: 0.8878785597751812, precision: 0.8052805280528053, recall: 0.9003690036900369\n",
      "train: 2019-01-04T16:19:05.599064: step 843, loss 0.062162503600120544, acc 0.7949843260188088, auc: 0.8603034884549319, precision: 0.8393148450244698, recall: 0.8878343399482312\n",
      "train: 2019-01-04T16:19:06.540310: step 844, loss 0.08163700252771378, acc 0.7487702037947997, auc: 0.7573667993865807, precision: 0.7565957446808511, recall: 0.9255596043727226\n",
      "train: 2019-01-04T16:19:07.677529: step 845, loss 0.08631482720375061, acc 0.8029393500310494, auc: 0.8234180365244848, precision: 0.8070046534410973, recall: 0.9525874530211044\n",
      "train: 2019-01-04T16:19:09.224457: step 846, loss 0.07739167660474777, acc 0.7906976744186046, auc: 0.8313270326747721, precision: 0.7972493345164152, recall: 0.9558510638297872\n",
      "train: 2019-01-04T16:19:10.554915: step 847, loss 0.09719229489564896, acc 0.7126754993078901, auc: 0.777036926730834, precision: 0.6931482575310101, recall: 0.8500543281419776\n",
      "train: 2019-01-04T16:19:11.792115: step 848, loss 0.07789312303066254, acc 0.8029384328358209, auc: 0.805944190507768, precision: 0.8146111547525531, recall: 0.957820197044335\n",
      "train: 2019-01-04T16:19:12.089341: step 849, loss 0.09917929023504257, acc 0.7542147293700089, auc: 0.8225632868552863, precision: 0.7514450867052023, recall: 0.9129213483146067\n",
      "train: 2019-01-04T16:19:13.782971: step 850, loss 0.048654425889253616, acc 0.8966029723991508, auc: 0.9479890393757965, precision: 0.9033984479529034, recall: 0.9640205596801827\n",
      "train: 2019-01-04T16:19:15.253162: step 851, loss 0.04697982221841812, acc 0.8274713069233617, auc: 0.9230451317901032, precision: 0.8141534391534392, recall: 0.8693502824858758\n",
      "train: 2019-01-04T16:19:15.698482: step 852, loss 0.08141852170228958, acc 0.7522255192878339, auc: 0.7978537766920056, precision: 0.7539103232533889, recall: 0.8806333739342266\n",
      "train: 2019-01-04T16:19:17.236931: step 853, loss 0.055344656109809875, acc 0.8406801007556675, auc: 0.9272491456594452, precision: 0.8573130771522193, recall: 0.9113362887903736\n",
      "train: 2019-01-04T16:19:18.238671: step 854, loss 0.0621122345328331, acc 0.8156424581005587, auc: 0.86716173871233, precision: 0.8331002331002331, recall: 0.9287941787941788\n",
      "train: 2019-01-04T16:19:19.101897: step 855, loss 0.076482854783535, acc 0.7697211155378486, auc: 0.8035458200748966, precision: 0.776595744680851, recall: 0.8856960408684547\n",
      "train: 2019-01-04T16:19:20.504491: step 856, loss 0.06857457011938095, acc 0.701153212520593, auc: 0.7516506706312072, precision: 0.7245417515274949, recall: 0.7954164337618781\n",
      "train: 2019-01-04T16:19:22.463621: step 857, loss 0.07266492396593094, acc 0.8504484304932736, auc: 0.9204405574280656, precision: 0.8732054015636105, recall: 0.9328777524677296\n",
      "train: 2019-01-04T16:19:23.797809: step 858, loss 0.07692009955644608, acc 0.7664182580344667, auc: 0.8312430000350232, precision: 0.7944444444444444, recall: 0.8842322226039162\n",
      "train: 2019-01-04T16:19:26.531271: step 859, loss 0.052419569343328476, acc 0.8278175313059034, auc: 0.8899923838188317, precision: 0.8403659447348768, recall: 0.9375130181212248\n",
      "train: 2019-01-04T16:19:27.366884: step 860, loss 0.10245966911315918, acc 0.747853463079565, auc: 0.7499736940827698, precision: 0.7709298183113644, recall: 0.9009159034138218\n",
      "train: 2019-01-04T16:19:28.801316: step 861, loss 0.08122806996107101, acc 0.7207702888583218, auc: 0.7154230057650693, precision: 0.7351493272070889, recall: 0.9150326797385621\n",
      "train: 2019-01-04T16:19:30.296177: step 862, loss 0.07206851243972778, acc 0.7583431952662721, auc: 0.7966963475209857, precision: 0.7801724137931034, recall: 0.9138337260181757\n",
      "train: 2019-01-04T16:19:32.411559: step 863, loss 0.045193348079919815, acc 0.8795529003661592, auc: 0.957481457464278, precision: 0.8910460736018545, recall: 0.9250902527075813\n",
      "train: 2019-01-04T16:19:33.294713: step 864, loss 0.07486210763454437, acc 0.8144262295081968, auc: 0.8917504640422023, precision: 0.7992106561420819, recall: 0.9106239460370995\n",
      "train: 2019-01-04T16:19:34.306050: step 865, loss 0.0653916597366333, acc 0.7151248164464024, auc: 0.7274132239151038, precision: 0.721142162818955, recall: 0.9061068702290076\n",
      "train: 2019-01-04T16:19:36.326225: step 866, loss 0.04640219360589981, acc 0.8268141354194767, auc: 0.915823800452323, precision: 0.8330721003134797, recall: 0.9077711357813835\n",
      "train: 2019-01-04T16:19:37.593254: step 867, loss 0.06625477969646454, acc 0.8271141532347096, auc: 0.8928114006360374, precision: 0.8395573997233748, recall: 0.9373069796170476\n",
      "train: 2019-01-04T16:19:38.472721: step 868, loss 0.12052138894796371, acc 0.7287581699346405, auc: 0.7876053845497379, precision: 0.728319783197832, recall: 0.8565737051792829\n",
      "train: 2019-01-04T16:19:39.397817: step 869, loss 0.09615404903888702, acc 0.7769507186858317, auc: 0.824571968808422, precision: 0.7718348002708192, recall: 0.9212121212121213\n",
      "train: 2019-01-04T16:19:41.570431: step 870, loss 0.04269289970397949, acc 0.8553100498930862, auc: 0.9422698502016068, precision: 0.8519842688594923, recall: 0.9243599689681924\n",
      "train: 2019-01-04T16:19:42.749064: step 871, loss 0.07782960683107376, acc 0.8112566715186803, auc: 0.7915201950196993, precision: 0.8169127516778524, recall: 0.9694170117871934\n",
      "train: 2019-01-04T16:19:43.122378: step 872, loss 0.10844627022743225, acc 0.75, auc: 0.7818011600755167, precision: 0.7719869706840391, recall: 0.8713235294117647\n",
      "train: 2019-01-04T16:19:44.559322: step 873, loss 0.07085113227367401, acc 0.8103114930182599, auc: 0.8608279484432773, precision: 0.8272679324894515, recall: 0.9322436849925706\n",
      "train: 2019-01-04T16:19:44.992301: step 874, loss 0.14748607575893402, acc 0.7162045060658578, auc: 0.7250258437550698, precision: 0.7385026737967915, recall: 0.8926955397543633\n",
      "train: 2019-01-04T16:19:46.330836: step 875, loss 0.08751235902309418, acc 0.7406684027777778, auc: 0.7547903490696176, precision: 0.75, recall: 0.9414147709566844\n",
      "train: 2019-01-04T16:19:48.081121: step 876, loss 0.06660883128643036, acc 0.8012600229095075, auc: 0.8319051961587136, precision: 0.8093342180933422, recall: 0.9533611255862429\n",
      "train: 2019-01-04T16:19:49.816394: step 877, loss 0.05491345375776291, acc 0.7754547922888949, auc: 0.8600448131513828, precision: 0.7860154602951511, recall: 0.9112016293279023\n",
      "train: 2019-01-04T16:19:51.013229: step 878, loss 0.10688045620918274, acc 0.7113821138211383, auc: 0.7370199843260188, precision: 0.716355017118778, recall: 0.9090909090909091\n",
      "train: 2019-01-04T16:19:52.280451: step 879, loss 0.07547464966773987, acc 0.7454927884615384, auc: 0.7500380312930184, precision: 0.75431654676259, recall: 0.9274657231313578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:19:53.468952: step 880, loss 0.09198813140392303, acc 0.7, auc: 0.7059312363528522, precision: 0.7132933375434165, recall: 0.8960729869099564\n",
      "train: 2019-01-04T16:19:54.822977: step 881, loss 0.06897386163473129, acc 0.7840336134453781, auc: 0.7498033593075297, precision: 0.7964015151515151, recall: 0.9524348810872028\n",
      "train: 2019-01-04T16:19:55.365102: step 882, loss 0.12395218014717102, acc 0.7472331310246341, auc: 0.8075641122951063, precision: 0.7478019435446552, recall: 0.9083754918493536\n",
      "train: 2019-01-04T16:19:57.010352: step 883, loss 0.07484752684831619, acc 0.6465807174887892, auc: 0.6923073588834289, precision: 0.6585760517799353, recall: 0.7960880195599022\n",
      "train: 2019-01-04T16:19:58.615125: step 884, loss 0.06897873431444168, acc 0.744410569105691, auc: 0.7534876513322981, precision: 0.7590546347452425, recall: 0.9179658500371195\n",
      "train: 2019-01-04T16:20:00.235091: step 885, loss 0.06943605840206146, acc 0.8156171284634761, auc: 0.8961154352540273, precision: 0.8309668695064233, recall: 0.9137546468401487\n",
      "train: 2019-01-04T16:20:02.157145: step 886, loss 0.060785114765167236, acc 0.8728711277993831, auc: 0.9177520537030762, precision: 0.8784103453713925, recall: 0.9692013224290934\n",
      "train: 2019-01-04T16:20:03.787443: step 887, loss 0.045369431376457214, acc 0.8794941730721547, auc: 0.9412789026517911, precision: 0.9014701282452299, recall: 0.9439895185063871\n",
      "train: 2019-01-04T16:20:04.813683: step 888, loss 0.07157012075185776, acc 0.7627992633517495, auc: 0.7671045542949356, precision: 0.7641299510458389, recall: 0.9377389404696886\n",
      "train: 2019-01-04T16:20:06.234868: step 889, loss 0.09317772090435028, acc 0.7310976879816705, auc: 0.7259029995785369, precision: 0.7437792559743779, recall: 0.9232415902140673\n",
      "train: 2019-01-04T16:20:07.306049: step 890, loss 0.08092096447944641, acc 0.792508757747238, auc: 0.8529106121229826, precision: 0.8135283363802559, recall: 0.8953722334004024\n",
      "train: 2019-01-04T16:20:08.323009: step 891, loss 0.0919346958398819, acc 0.7601769911504425, auc: 0.8080291481938151, precision: 0.7787903893951947, recall: 0.8707735062528948\n",
      "train: 2019-01-04T16:20:09.495539: step 892, loss 0.07669046521186829, acc 0.7742093650404511, auc: 0.8544758395809798, precision: 0.7983789260385005, recall: 0.8794642857142857\n",
      "train: 2019-01-04T16:20:10.478402: step 893, loss 0.07399583607912064, acc 0.8346784691486592, auc: 0.8026153728503246, precision: 0.8547470153496305, recall: 0.9603960396039604\n",
      "train: 2019-01-04T16:20:11.715460: step 894, loss 0.11762262135744095, acc 0.7289815447710185, auc: 0.7044857111025968, precision: 0.7440881763527054, recall: 0.9231725509696669\n",
      "train: 2019-01-04T16:20:12.956471: step 895, loss 0.08523125946521759, acc 0.6808211473565804, auc: 0.7330734316439724, precision: 0.6774475524475524, recall: 0.7960965588084232\n",
      "train: 2019-01-04T16:20:13.687656: step 896, loss 0.06371258944272995, acc 0.7833914053426249, auc: 0.8273500411400514, precision: 0.8115631691648822, recall: 0.9125200642054575\n",
      "train: 2019-01-04T16:20:14.542656: step 897, loss 0.07989522814750671, acc 0.7472924187725631, auc: 0.7346719298031211, precision: 0.7646498332539304, recall: 0.9218839747271683\n",
      "train: 2019-01-04T16:20:16.019044: step 898, loss 0.06857225298881531, acc 0.787975355852985, auc: 0.8562140675994241, precision: 0.7956322458883797, recall: 0.9247884675650266\n",
      "train: 2019-01-04T16:20:17.562597: step 899, loss 0.09029575437307358, acc 0.7036265110462693, auc: 0.7117642817910425, precision: 0.7066963182604399, recall: 0.9240710823909531\n",
      "train: 2019-01-04T16:20:18.369623: step 900, loss 0.07194801419973373, acc 0.8016227180527383, auc: 0.8823363240010089, precision: 0.8072222222222222, recall: 0.9109717868338558\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:20:31.417545, step: 900, loss: 0.07874650952334587, acc: 0.7778278075477018, auc: 0.8156770029295762, precision: 0.7869804627598852, recall: 0.913784933584252\n",
      "Saved model checkpoint to model/my-model-900\n",
      "\n",
      "train: 2019-01-04T16:20:32.713405: step 901, loss 0.0983884334564209, acc 0.7449989008573313, auc: 0.7140912469756242, precision: 0.7681086519114688, recall: 0.9277035236938032\n",
      "train: 2019-01-04T16:20:33.156445: step 902, loss 0.10717363655567169, acc 0.6986924388857305, auc: 0.7404083241582733, precision: 0.7200647249190939, recall: 0.8286778398510242\n",
      "train: 2019-01-04T16:20:34.455258: step 903, loss 0.05968062952160835, acc 0.8734915526950925, auc: 0.9337420471266036, precision: 0.8862183584760931, recall: 0.9495890410958904\n",
      "train: 2019-01-04T16:20:35.021292: step 904, loss 0.1414758414030075, acc 0.6984179301252472, auc: 0.7429118285058058, precision: 0.7085765603951504, recall: 0.8557483731019523\n",
      "train: 2019-01-04T16:20:36.116670: step 905, loss 0.058900635689496994, acc 0.7885017421602788, auc: 0.8445151624116025, precision: 0.8140942498919153, recall: 0.9140776699029126\n",
      "train: 2019-01-04T16:20:37.279452: step 906, loss 0.07380449771881104, acc 0.7402282947077136, auc: 0.7397987217201614, precision: 0.7530500631047539, recall: 0.9160696008188332\n",
      "train: 2019-01-04T16:20:40.159338: step 907, loss 0.03945440426468849, acc 0.8977454287235931, auc: 0.9538302585285439, precision: 0.8980309423347398, recall: 0.9645015105740181\n",
      "train: 2019-01-04T16:20:41.257072: step 908, loss 0.06534846127033234, acc 0.821161587119034, auc: 0.8920878400386221, precision: 0.8160164271047228, recall: 0.9194817214252661\n",
      "train: 2019-01-04T16:20:42.293725: step 909, loss 0.07394757121801376, acc 0.8281008749356665, auc: 0.878743758002561, precision: 0.8285984848484849, recall: 0.9545454545454546\n",
      "train: 2019-01-04T16:20:46.032842: step 910, loss 0.03953922539949417, acc 0.8627182471756247, auc: 0.9291704370962222, precision: 0.8547986076578816, recall: 0.9403719912472648\n",
      "train: 2019-01-04T16:20:47.009239: step 911, loss 0.08008722215890884, acc 0.817549491706795, auc: 0.8580165759183955, precision: 0.8277479892761395, recall: 0.9363153904473086\n",
      "train: 2019-01-04T16:20:48.044145: step 912, loss 0.08045245707035065, acc 0.8020408163265306, auc: 0.7908653989461965, precision: 0.8126255380200861, recall: 0.9583756345177665\n",
      "train: 2019-01-04T16:20:48.617713: step 913, loss 0.08027827739715576, acc 0.7904191616766467, auc: 0.8244688663726094, precision: 0.8226872246696035, recall: 0.9182544560540873\n",
      "train: 2019-01-04T16:20:50.082919: step 914, loss 0.0719829797744751, acc 0.8159203980099502, auc: 0.8820680994442862, precision: 0.8291917973462002, recall: 0.9311839609861826\n",
      "train: 2019-01-04T16:20:51.417156: step 915, loss 0.0748063325881958, acc 0.6989430617115582, auc: 0.709319581139735, precision: 0.715374271627073, recall: 0.8655097613882863\n",
      "train: 2019-01-04T16:20:53.069585: step 916, loss 0.07081963866949081, acc 0.7374358974358974, auc: 0.7331765236108198, precision: 0.7520119225037257, recall: 0.9292817679558011\n",
      "train: 2019-01-04T16:20:55.071581: step 917, loss 0.04885566979646683, acc 0.8380771663504112, auc: 0.9104324408461606, precision: 0.853021978021978, recall: 0.9301977231875375\n",
      "train: 2019-01-04T16:21:02.420037: step 918, loss 0.03356840834021568, acc 0.8860878562694008, auc: 0.9449144730176606, precision: 0.9003409090909091, recall: 0.9595494731742764\n",
      "train: 2019-01-04T16:21:03.179191: step 919, loss 0.0863209068775177, acc 0.6970497578159401, auc: 0.7051730293636143, precision: 0.7206434316353887, recall: 0.8894771674387822\n",
      "train: 2019-01-04T16:21:04.199552: step 920, loss 0.07514123618602753, acc 0.7718727404193781, auc: 0.7735987965123419, precision: 0.7831643895989423, recall: 0.9274530271398748\n",
      "train: 2019-01-04T16:21:05.965241: step 921, loss 0.09781545400619507, acc 0.7766406549827299, auc: 0.8242195129244809, precision: 0.7864286813355901, recall: 0.9345401353081002\n",
      "train: 2019-01-04T16:21:07.719625: step 922, loss 0.06023547425866127, acc 0.8128089404685149, auc: 0.8807944836519807, precision: 0.8270632837561971, recall: 0.9107257546563905\n",
      "train: 2019-01-04T16:21:09.128318: step 923, loss 0.07745613902807236, acc 0.7264292878635907, auc: 0.7355980759659807, precision: 0.7343161856963614, recall: 0.9056092843326886\n",
      "train: 2019-01-04T16:21:10.791347: step 924, loss 0.0648752972483635, acc 0.7773611436606516, auc: 0.8673000651472743, precision: 0.7502675704602212, recall: 0.8937526561835955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:21:12.261440: step 925, loss 0.058000918477773666, acc 0.8321318228630278, auc: 0.8574559699453402, precision: 0.8512648582749162, recall: 0.9445383834967873\n",
      "train: 2019-01-04T16:21:13.391352: step 926, loss 0.08123525232076645, acc 0.7795315682281059, auc: 0.821747579542806, precision: 0.7995018679950187, recall: 0.9204301075268817\n",
      "train: 2019-01-04T16:21:14.485468: step 927, loss 0.06735634803771973, acc 0.8441265060240963, auc: 0.9131643701295655, precision: 0.8506578947368421, recall: 0.9393389030148929\n",
      "train: 2019-01-04T16:21:16.115222: step 928, loss 0.06547972559928894, acc 0.7129461584996976, auc: 0.7357643890649509, precision: 0.7288854608561511, recall: 0.8848314606741573\n",
      "train: 2019-01-04T16:21:16.441881: step 929, loss 0.13795475661754608, acc 0.7408036219581211, auc: 0.7499014876874766, precision: 0.7649074708704592, recall: 0.9065800162469537\n",
      "train: 2019-01-04T16:21:17.007001: step 930, loss 0.055701471865177155, acc 0.8284904323175053, auc: 0.9032128813024934, precision: 0.8535031847133758, recall: 0.8854625550660793\n",
      "train: 2019-01-04T16:21:18.466897: step 931, loss 0.04440145567059517, acc 0.8553351573187414, auc: 0.9032068368552664, precision: 0.8692893401015228, recall: 0.9474412171507607\n",
      "train: 2019-01-04T16:21:27.649666: step 932, loss 0.030712706968188286, acc 0.9066783551163532, auc: 0.9567396198063991, precision: 0.9082845819966031, recall: 0.9797455470737914\n",
      "train: 2019-01-04T16:21:28.651129: step 933, loss 0.06578021496534348, acc 0.7513513513513513, auc: 0.7323161629642719, precision: 0.7563395810363837, recall: 0.9554317548746518\n",
      "train: 2019-01-04T16:21:29.883636: step 934, loss 0.06616602838039398, acc 0.704422032583398, auc: 0.7471085427381428, precision: 0.7051751176163095, recall: 0.8720103425985779\n",
      "train: 2019-01-04T16:21:31.036704: step 935, loss 0.06583036482334137, acc 0.7132149901380671, auc: 0.7672191258052272, precision: 0.714975845410628, recall: 0.8717277486910995\n",
      "train: 2019-01-04T16:21:32.796062: step 936, loss 0.08040639013051987, acc 0.7213208627600687, auc: 0.7605487980122934, precision: 0.7300154718927282, recall: 0.8726880394574599\n",
      "train: 2019-01-04T16:21:33.876947: step 937, loss 0.10973351448774338, acc 0.7753434831043446, auc: 0.7820709406857265, precision: 0.7988013698630136, recall: 0.9325337331334332\n",
      "train: 2019-01-04T16:21:37.149651: step 938, loss 0.037008337676525116, acc 0.8734556345937851, auc: 0.9496062374532818, precision: 0.8741038141669056, recall: 0.9278538812785389\n",
      "train: 2019-01-04T16:21:38.260968: step 939, loss 0.09472915530204773, acc 0.7385409941897999, auc: 0.7891526472121886, precision: 0.7615677735438214, recall: 0.8919349697162895\n",
      "train: 2019-01-04T16:21:39.189987: step 940, loss 0.08388436585664749, acc 0.7451223288943946, auc: 0.7918571274950161, precision: 0.7497962510187449, recall: 0.8979990239141045\n",
      "train: 2019-01-04T16:21:40.909047: step 941, loss 0.07173148542642593, acc 0.7947560340336864, auc: 0.833607176684851, precision: 0.8012611437268972, recall: 0.9322033898305084\n",
      "train: 2019-01-04T16:21:41.979337: step 942, loss 0.09277231991291046, acc 0.7120500782472613, auc: 0.73975752539624, precision: 0.728830313014827, recall: 0.887284396309667\n",
      "train: 2019-01-04T16:21:43.416762: step 943, loss 0.07779056578874588, acc 0.6757950530035336, auc: 0.7331256040020465, precision: 0.6831556503198294, recall: 0.8173469387755102\n",
      "train: 2019-01-04T16:21:44.617999: step 944, loss 0.06630013138055801, acc 0.7126351099515468, auc: 0.7553883392333062, precision: 0.729106628242075, recall: 0.88\n",
      "train: 2019-01-04T16:21:46.482879: step 945, loss 0.0554242767393589, acc 0.784115523465704, auc: 0.841254259766918, precision: 0.7975910693301997, recall: 0.928840232637701\n",
      "train: 2019-01-04T16:21:53.754065: step 946, loss 0.02636030502617359, acc 0.946078431372549, auc: 0.9844043319732481, precision: 0.9491291818067912, recall: 0.9841496687020918\n",
      "train: 2019-01-04T16:21:55.355128: step 947, loss 0.05347412824630737, acc 0.8549091745228788, auc: 0.9193011506045342, precision: 0.8485225505443235, recall: 0.9498607242339833\n",
      "train: 2019-01-04T16:21:56.362899: step 948, loss 0.08824143558740616, acc 0.7665567875337129, auc: 0.7453321479049834, precision: 0.7825494205862304, recall: 0.9421419778416086\n",
      "train: 2019-01-04T16:21:57.862647: step 949, loss 0.06918855011463165, acc 0.7443365695792881, auc: 0.826475250298778, precision: 0.7439508847959552, recall: 0.8662741799831791\n",
      "train: 2019-01-04T16:22:01.646035: step 950, loss 0.032750070095062256, acc 0.9049858889934148, auc: 0.9590007952346034, precision: 0.9053902302077484, recall: 0.9504862953138815\n",
      "train: 2019-01-04T16:22:03.248683: step 951, loss 0.06899864226579666, acc 0.7150147730325007, auc: 0.7716940947544524, precision: 0.7058386017106731, recall: 0.8754612546125461\n",
      "train: 2019-01-04T16:22:03.661189: step 952, loss 0.10810736566781998, acc 0.691696750902527, auc: 0.7459759376854225, precision: 0.6747624076029567, recall: 0.8430079155672823\n",
      "train: 2019-01-04T16:22:05.041985: step 953, loss 0.07656504213809967, acc 0.7745740498034076, auc: 0.7906283511146248, precision: 0.7861001964636543, recall: 0.9521118381915527\n",
      "train: 2019-01-04T16:22:06.290186: step 954, loss 0.08226852118968964, acc 0.7433606953162724, auc: 0.7542936223063497, precision: 0.7610062893081762, recall: 0.905559515324305\n",
      "train: 2019-01-04T16:22:07.538180: step 955, loss 0.08509371429681778, acc 0.6988384955752213, auc: 0.7374686164269149, precision: 0.7012171181782489, recall: 0.8448438978240302\n",
      "train: 2019-01-04T16:22:08.014672: step 956, loss 0.11151859164237976, acc 0.7496706192358367, auc: 0.7459020740371338, precision: 0.7791509940891993, recall: 0.9011808576755749\n",
      "train: 2019-01-04T16:22:09.736418: step 957, loss 0.05400823429226875, acc 0.8434281005356407, auc: 0.8817104737490429, precision: 0.8542982030111704, recall: 0.9564980967917346\n",
      "train: 2019-01-04T16:22:10.884473: step 958, loss 0.0815807357430458, acc 0.7670394036208733, auc: 0.7300455027294427, precision: 0.7863483318028772, recall: 0.9355426074289876\n",
      "train: 2019-01-04T16:22:13.835177: step 959, loss 0.05529431626200676, acc 0.8247586459733594, auc: 0.8897172197643833, precision: 0.8363636363636363, recall: 0.9378105191022785\n",
      "train: 2019-01-04T16:22:14.789354: step 960, loss 0.09765371680259705, acc 0.7390465693028383, auc: 0.8154702443885427, precision: 0.7096171802054155, recall: 0.8238482384823849\n",
      "train: 2019-01-04T16:22:15.050304: step 961, loss 0.10851984471082687, acc 0.7429864253393665, auc: 0.7905838122194584, precision: 0.7708860759493671, recall: 0.8553370786516854\n",
      "train: 2019-01-04T16:22:16.191575: step 962, loss 0.06823254376649857, acc 0.8047901107391192, auc: 0.8866131366432264, precision: 0.8083511777301927, recall: 0.911102172164119\n",
      "train: 2019-01-04T16:22:16.862978: step 963, loss 0.06734549254179001, acc 0.7804123711340206, auc: 0.8714124689578366, precision: 0.776657060518732, recall: 0.9028475711892797\n",
      "train: 2019-01-04T16:22:18.568157: step 964, loss 0.07633298635482788, acc 0.7246570121951219, auc: 0.783094727185085, precision: 0.7339055793991416, recall: 0.8814432989690721\n",
      "train: 2019-01-04T16:22:19.644664: step 965, loss 0.0832979828119278, acc 0.767519804996953, auc: 0.7182466289926637, precision: 0.78060522696011, recall: 0.9478079331941545\n",
      "train: 2019-01-04T16:22:19.847652: step 966, loss 0.1746470183134079, acc 0.722940226171244, auc: 0.7239614280055455, precision: 0.7436856875584659, recall: 0.9201388888888888\n",
      "train: 2019-01-04T16:22:20.851050: step 967, loss 0.06101422384381294, acc 0.8041847611527833, auc: 0.8851925075922793, precision: 0.8228929384965832, recall: 0.8865030674846626\n",
      "train: 2019-01-04T16:22:21.569304: step 968, loss 0.0878247618675232, acc 0.6921944035346097, auc: 0.7579432938951187, precision: 0.6638396299151889, recall: 0.8184410646387833\n",
      "train: 2019-01-04T16:22:30.868020: step 969, loss 0.02777561917901039, acc 0.9148721399730821, auc: 0.972666619674533, precision: 0.9274594938049349, recall: 0.9640066042927903\n",
      "train: 2019-01-04T16:22:32.224836: step 970, loss 0.05696752667427063, acc 0.851356993736952, auc: 0.9112595230185165, precision: 0.8660122048288671, recall: 0.9403630077787382\n",
      "train: 2019-01-04T16:22:33.524919: step 971, loss 0.10198527574539185, acc 0.7271786379631546, auc: 0.7605407400791205, precision: 0.7417815482502651, recall: 0.890515595162317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:22:34.467450: step 972, loss 0.06248552352190018, acc 0.7466460268317854, auc: 0.7832390482417488, precision: 0.747599451303155, recall: 0.8985985160758451\n",
      "train: 2019-01-04T16:22:35.945790: step 973, loss 0.045455701649188995, acc 0.8411949685534591, auc: 0.9018335769980506, precision: 0.854728370221328, recall: 0.9365079365079365\n",
      "train: 2019-01-04T16:22:38.017132: step 974, loss 0.039307475090026855, acc 0.8848341232227488, auc: 0.9425289856678517, precision: 0.8875885432707115, recall: 0.9597069597069597\n",
      "train: 2019-01-04T16:22:41.367998: step 975, loss 0.03757224604487419, acc 0.9028593811202507, auc: 0.9629865108962832, precision: 0.9117382707172389, recall: 0.9524882629107981\n",
      "train: 2019-01-04T16:22:42.852377: step 976, loss 0.06415774673223495, acc 0.7780952380952381, auc: 0.8278897300407619, precision: 0.8070913461538461, recall: 0.9025537634408602\n",
      "train: 2019-01-04T16:22:44.566829: step 977, loss 0.06382247805595398, acc 0.8219627873039037, auc: 0.848764714889171, precision: 0.8289855072463768, recall: 0.9467787114845938\n",
      "train: 2019-01-04T16:22:45.760730: step 978, loss 0.10158903151750565, acc 0.7126183659986787, auc: 0.7438085826401655, precision: 0.7268102932375823, recall: 0.8610421836228288\n",
      "train: 2019-01-04T16:22:46.993026: step 979, loss 0.11300427466630936, acc 0.7052058111380145, auc: 0.6977330473519221, precision: 0.7285112707456032, recall: 0.8895946763460375\n",
      "train: 2019-01-04T16:22:48.438674: step 980, loss 0.036848608404397964, acc 0.8500435919790759, auc: 0.9196236535259287, precision: 0.867862969004894, recall: 0.9404832056570418\n",
      "train: 2019-01-04T16:22:49.490425: step 981, loss 0.06466510146856308, acc 0.8129474011826953, auc: 0.8603789811768943, precision: 0.8402182385035074, recall: 0.9186195142735407\n",
      "train: 2019-01-04T16:22:51.227832: step 982, loss 0.06040389835834503, acc 0.8471784086926043, auc: 0.9022715196876321, precision: 0.8728644331040604, recall: 0.9293645168910938\n",
      "train: 2019-01-04T16:22:51.659262: step 983, loss 0.11060776561498642, acc 0.7368421052631579, auc: 0.7264433330943132, precision: 0.7810276679841898, recall: 0.8583840139009556\n",
      "train: 2019-01-04T16:22:52.414796: step 984, loss 0.08241993188858032, acc 0.7285776424532405, auc: 0.7758685958773914, precision: 0.734338747099768, recall: 0.8840782122905028\n",
      "train: 2019-01-04T16:22:52.790925: step 985, loss 0.12814350426197052, acc 0.7216435185185185, auc: 0.758578633546228, precision: 0.7379718726868986, recall: 0.8870106761565836\n",
      "train: 2019-01-04T16:22:53.916070: step 986, loss 0.0859457477927208, acc 0.7731516527882917, auc: 0.7740505734512724, precision: 0.7731510254816656, recall: 0.9363944298080542\n",
      "train: 2019-01-04T16:22:55.149129: step 987, loss 0.090826116502285, acc 0.7577197149643705, auc: 0.7531867056639965, precision: 0.7626313156578289, recall: 0.9463066418373681\n",
      "train: 2019-01-04T16:22:55.642231: step 988, loss 0.12057553231716156, acc 0.6818609022556391, auc: 0.7253340144572248, precision: 0.6825208085612366, recall: 0.8892331525948877\n",
      "train: 2019-01-04T16:22:57.931510: step 989, loss 0.0360107459127903, acc 0.8629690048939641, auc: 0.9381593057804205, precision: 0.8553893364019947, recall: 0.9461179465422147\n",
      "train: 2019-01-04T16:22:59.237091: step 990, loss 0.04059039056301117, acc 0.9157119476268413, auc: 0.9520499452574929, precision: 0.9322621298046628, recall: 0.969210612512283\n",
      "train: 2019-01-04T16:23:00.817182: step 991, loss 0.059106022119522095, acc 0.7550901687027342, auc: 0.7348676954149171, precision: 0.7803689523146536, recall: 0.9139828781084387\n",
      "train: 2019-01-04T16:23:02.147555: step 992, loss 0.04648543894290924, acc 0.8310055865921788, auc: 0.9080849342770475, precision: 0.8361527307878202, recall: 0.9226666666666666\n",
      "train: 2019-01-04T16:23:03.757335: step 993, loss 0.05389099195599556, acc 0.666391070690368, auc: 0.7067175853758267, precision: 0.6625211984171848, recall: 0.8480463096960926\n",
      "train: 2019-01-04T16:23:04.326822: step 994, loss 0.1206238716840744, acc 0.7008688456764585, auc: 0.7411210101362145, precision: 0.7166853303471444, recall: 0.8550434201736807\n",
      "train: 2019-01-04T16:23:05.184704: step 995, loss 0.09489768743515015, acc 0.6922576447625244, auc: 0.7369748441817846, precision: 0.702914310569813, recall: 0.8600319318786589\n",
      "train: 2019-01-04T16:23:06.317054: step 996, loss 0.09503588825464249, acc 0.7455447778028423, auc: 0.7778890107561436, precision: 0.7688557499283052, recall: 0.8927738927738927\n",
      "train: 2019-01-04T16:23:07.706823: step 997, loss 0.06002902239561081, acc 0.8459183673469388, auc: 0.8826795400797469, precision: 0.8586387434554974, recall: 0.9572830989652428\n",
      "train: 2019-01-04T16:23:09.032592: step 998, loss 0.061627984046936035, acc 0.7317939609236235, auc: 0.7471157511536451, precision: 0.7351054078826764, recall: 0.9006176305446378\n",
      "train: 2019-01-04T16:23:10.042263: step 999, loss 0.05879644677042961, acc 0.8393151135989463, auc: 0.8579251525146531, precision: 0.8611755607115236, recall: 0.9452461799660441\n",
      "train: 2019-01-04T16:23:11.478064: step 1000, loss 0.06553733348846436, acc 0.7696307852314093, auc: 0.8524840283037391, precision: 0.7807332854061826, recall: 0.8872549019607843\n",
      "\n",
      "Evaluation:\n",
      "dev: 2019-01-04T16:23:24.176871, step: 1000, loss: 0.07629480886344726, acc: 0.775564594045446, auc: 0.8100859555498202, precision: 0.7852836349719523, recall: 0.9126725375470686\n",
      "Saved model checkpoint to model/my-model-1000\n",
      "\n",
      "train: 2019-01-04T16:23:25.628092: step 1001, loss 0.05802546814084053, acc 0.8503030303030303, auc: 0.895778356520354, precision: 0.8654070445533006, recall: 0.9576664945792462\n",
      "train: 2019-01-04T16:23:27.118507: step 1002, loss 0.06356611102819443, acc 0.6736415794802565, auc: 0.7134328158637196, precision: 0.6865470852017937, recall: 0.8510283490828238\n",
      "train: 2019-01-04T16:23:28.334961: step 1003, loss 0.09689003974199295, acc 0.7521821631878558, auc: 0.7800604864216681, precision: 0.7735805860805861, recall: 0.9142316017316018\n",
      "train: 2019-01-04T16:23:29.557608: step 1004, loss 0.07778634876012802, acc 0.7978165938864629, auc: 0.7766632760773537, precision: 0.8144152978671243, recall: 0.9515898023488971\n",
      "train: 2019-01-04T16:23:31.226982: step 1005, loss 0.056484319269657135, acc 0.7083469721767595, auc: 0.7643018867924528, precision: 0.7086026852312283, recall: 0.8236994219653179\n",
      "train: 2019-01-04T16:23:32.557567: step 1006, loss 0.08775771409273148, acc 0.7391003460207612, auc: 0.7221289348592503, precision: 0.7584463953179037, recall: 0.9274560832791151\n",
      "train: 2019-01-04T16:23:33.411973: step 1007, loss 0.06517920643091202, acc 0.7315550510783201, auc: 0.7001150528504805, precision: 0.7518151815181519, recall: 0.9215210355987055\n",
      "train: 2019-01-04T16:23:35.097931: step 1008, loss 0.06849285215139389, acc 0.683679192372406, auc: 0.7269810503572717, precision: 0.6898199191473723, recall: 0.8685793614067562\n",
      "train: 2019-01-04T16:23:35.977645: step 1009, loss 0.06970487534999847, acc 0.8095580309019044, auc: 0.8273758688197191, precision: 0.8173766343315056, recall: 0.9523341523341523\n",
      "train: 2019-01-04T16:23:36.710699: step 1010, loss 0.12574359774589539, acc 0.6597769028871391, auc: 0.7109539040124723, precision: 0.6534701857282502, recall: 0.803003003003003\n",
      "train: 2019-01-04T16:23:38.636769: step 1011, loss 0.057174354791641235, acc 0.8033826638477801, auc: 0.873928979333767, precision: 0.8068982880161127, recall: 0.9260329384570933\n",
      "train: 2019-01-04T16:23:39.463364: step 1012, loss 0.08545032888650894, acc 0.7364620938628159, auc: 0.7453131723643318, precision: 0.757847533632287, recall: 0.898936170212766\n",
      "train: 2019-01-04T16:23:41.093312: step 1013, loss 0.0427701361477375, acc 0.8593177332957429, auc: 0.9131502756976861, precision: 0.8743994509265615, recall: 0.9503916449086162\n",
      "train: 2019-01-04T16:23:41.517681: step 1014, loss 0.09784384816884995, acc 0.7926401869158879, auc: 0.863300395256917, precision: 0.7260812581913499, recall: 0.7914285714285715\n",
      "train: 2019-01-04T16:23:43.015083: step 1015, loss 0.07893400639295578, acc 0.6903131115459883, auc: 0.6978394880054102, precision: 0.7062250843299601, recall: 0.8820375335120644\n",
      "train: 2019-01-04T16:23:44.096547: step 1016, loss 0.07329852133989334, acc 0.7951598962834918, auc: 0.8495040064048504, precision: 0.8082663605051664, recall: 0.9095607235142119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 2019-01-04T16:23:45.731052: step 1017, loss 0.07470189034938812, acc 0.7759320782576596, auc: 0.7969807817544442, precision: 0.7960327727468737, recall: 0.9323232323232323\n",
      "train: 2019-01-04T16:23:46.221030: step 1018, loss 0.09599720686674118, acc 0.7406593406593407, auc: 0.7998089589511471, precision: 0.7383399209486166, recall: 0.8688372093023256\n",
      "train: 2019-01-04T16:23:46.901344: step 1019, loss 0.07998091727495193, acc 0.7350383631713555, auc: 0.766487689345848, precision: 0.7540214477211796, recall: 0.8816614420062696\n",
      "train: 2019-01-04T16:23:47.583115: step 1020, loss 0.11057641357183456, acc 0.7272410184862226, auc: 0.7754951916550938, precision: 0.7384830153559795, recall: 0.87825124515772\n",
      "train: 2019-01-04T16:23:48.742633: step 1021, loss 0.05726763978600502, acc 0.7428326914848096, auc: 0.7631263646475606, precision: 0.7526132404181185, recall: 0.9356435643564357\n",
      "train: 2019-01-04T16:23:50.199906: step 1022, loss 0.06466130167245865, acc 0.7991404659579281, auc: 0.8249306771096813, precision: 0.8065749235474006, recall: 0.9608378870673953\n",
      "train: 2019-01-04T16:23:51.721980: step 1023, loss 0.06219477578997612, acc 0.7106938255887969, auc: 0.72962675495601, precision: 0.7204800619434766, recall: 0.90869140625\n",
      "train: 2019-01-04T16:23:52.807016: step 1024, loss 0.09613273292779922, acc 0.6984949420182581, auc: 0.7135378367110908, precision: 0.7106194690265487, recall: 0.9090566037735849\n",
      "train: 2019-01-04T16:23:54.534194: step 1025, loss 0.04935767501592636, acc 0.8925081433224755, auc: 0.9511045815840153, precision: 0.9073861461921163, recall: 0.9577863057968087\n",
      "train: 2019-01-04T16:23:55.896490: step 1026, loss 0.06761656701564789, acc 0.7211155378486056, auc: 0.7792015321201176, precision: 0.7279411764705882, recall: 0.8569246435845214\n",
      "train: 2019-01-04T16:23:57.065385: step 1027, loss 0.06001148745417595, acc 0.7665838949982263, auc: 0.821216144040115, precision: 0.7779362815026153, recall: 0.8954570333880679\n",
      "train: 2019-01-04T16:23:59.073708: step 1028, loss 0.061922959983348846, acc 0.8024367674275139, auc: 0.878742095916157, precision: 0.7914002205071665, recall: 0.9146279306829765\n",
      "train: 2019-01-04T16:24:00.982806: step 1029, loss 0.07701878249645233, acc 0.7023765317489788, auc: 0.7125388506697682, precision: 0.7342184952247846, recall: 0.872163807415606\n",
      "train: 2019-01-04T16:24:02.582784: step 1030, loss 0.040879253298044205, acc 0.8759923350670682, auc: 0.9489110236028012, precision: 0.8884414015304067, recall: 0.926112510495382\n",
      "train: 2019-01-04T16:24:03.765231: step 1031, loss 0.10028281807899475, acc 0.7223367697594502, auc: 0.7424929090499748, precision: 0.7459777227722773, recall: 0.8604568165596003\n",
      "train: 2019-01-04T16:24:05.123065: step 1032, loss 0.0721893459558487, acc 0.8091631258082395, auc: 0.8346743081182719, precision: 0.8315089074908779, recall: 0.9398350315380883\n",
      "train: 2019-01-04T16:24:06.329962: step 1033, loss 0.10141228139400482, acc 0.8207784519757289, auc: 0.7846318068629712, precision: 0.8392571245597182, recall: 0.9620113782345384\n",
      "train: 2019-01-04T16:24:07.597775: step 1034, loss 0.10455505549907684, acc 0.7310985607578794, auc: 0.7428083838776938, precision: 0.7422785845776232, recall: 0.9438231469440832\n",
      "train: 2019-01-04T16:24:08.597968: step 1035, loss 0.08224675059318542, acc 0.8333333333333334, auc: 0.8350557367534379, precision: 0.8392419175027871, recall: 0.9708537529017282\n",
      "train: 2019-01-04T16:24:09.905220: step 1036, loss 0.07362320274114609, acc 0.7650569723277265, auc: 0.8074574209657599, precision: 0.7413976587442356, recall: 0.9384822631342613\n",
      "train: 2019-01-04T16:24:11.742968: step 1037, loss 0.045453380793333054, acc 0.8556851311953353, auc: 0.912824963125388, precision: 0.8527592180153427, recall: 0.9723476297968398\n",
      "train: 2019-01-04T16:24:12.826110: step 1038, loss 0.08936601877212524, acc 0.7597292724196277, auc: 0.8096057260904396, precision: 0.7557736720554272, recall: 0.9464931308749096\n",
      "train: 2019-01-04T16:24:14.154896: step 1039, loss 0.0905110314488411, acc 0.729055638456619, auc: 0.7412939197290729, precision: 0.7314070351758793, recall: 0.9351108255701895\n",
      "train: 2019-01-04T16:24:15.634305: step 1040, loss 0.0719815343618393, acc 0.7156158552806597, auc: 0.7785776120667347, precision: 0.7089806744979159, recall: 0.8614180478821363\n",
      "train: 2019-01-04T16:24:16.145344: step 1041, loss 0.12213676422834396, acc 0.7369716864997948, auc: 0.8001549895069215, precision: 0.7287104622871047, recall: 0.8600143575017947\n",
      "train: 2019-01-04T16:24:17.443863: step 1042, loss 0.059317439794540405, acc 0.7790476190476191, auc: 0.846633218761159, precision: 0.791921664626683, recall: 0.9125528913963329\n",
      "train: 2019-01-04T16:24:18.805285: step 1043, loss 0.10181032866239548, acc 0.7475042236215635, auc: 0.7925680528091767, precision: 0.7656797818465134, recall: 0.8991308325709058\n",
      "train: 2019-01-04T16:24:20.675986: step 1044, loss 0.04748651757836342, acc 0.8559611580650962, auc: 0.9064679516706231, precision: 0.8707232568211347, recall: 0.951715976331361\n",
      "train: 2019-01-04T16:24:22.557482: step 1045, loss 0.07610155642032623, acc 0.703382166757099, auc: 0.7695370324966901, precision: 0.7307015338194619, recall: 0.8362589928057554\n",
      "train: 2019-01-04T16:24:23.340202: step 1046, loss 0.1237119510769844, acc 0.7350796651363759, auc: 0.7982740712703636, precision: 0.735617039964866, recall: 0.8154819863680624\n",
      "train: 2019-01-04T16:24:26.048060: step 1047, loss 0.0533466674387455, acc 0.8070717934928744, auc: 0.9001016134372413, precision: 0.829088612923595, recall: 0.8634370889960543\n",
      "train: 2019-01-04T16:24:27.625420: step 1048, loss 0.05243657901883125, acc 0.8373900593168337, auc: 0.9073552356560395, precision: 0.8637083993660856, recall: 0.9213863060016906\n",
      "train: 2019-01-04T16:24:28.744741: step 1049, loss 0.062156423926353455, acc 0.8204702627939142, auc: 0.887724118644364, precision: 0.8456505003849115, recall: 0.8985685071574642\n",
      "train: 2019-01-04T16:24:29.637742: step 1050, loss 0.06316298991441727, acc 0.8139365195184239, auc: 0.8502734168520303, precision: 0.8308790718429273, recall: 0.9342699448068239\n"
     ]
    }
   ],
   "source": [
    "class DKTEngine(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.train_dkt = None\n",
    "        self.test_dkt = None\n",
    "        self.sess = None\n",
    "        self.global_step = 0\n",
    "\n",
    "    def add_gradient_noise(self, grad, stddev=1e-3, name=None):\n",
    "        \"\"\"\n",
    "        Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n",
    "        \"\"\"\n",
    "        with tf.op_scope([grad, stddev], name, \"add_gradient_noise\") as name:\n",
    "            grad = tf.convert_to_tensor(grad, name=\"grad\")\n",
    "            gn = tf.random_normal(tf.shape(grad), stddev=stddev)\n",
    "            return tf.add(grad, gn, name=name)\n",
    "\n",
    "    def train_step(self, params, train_op, train_summary_op, train_summary_writer):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        dkt = self.train_dkt\n",
    "        sess = self.sess\n",
    "        global_step = self.global_step\n",
    "\n",
    "        feed_dict = {dkt.input_data: params['input_x'],\n",
    "                     dkt.target_id: params['target_id'],\n",
    "                     dkt.target_correctness: params['target_correctness'],\n",
    "                     dkt.max_steps: params['max_len'],\n",
    "                     dkt.sequence_len: params['seq_len'],\n",
    "                     dkt.keep_prob: self.config.modelConfig.dropout_keep_prob}\n",
    "\n",
    "        _, step, summaries, loss, binary_pred, pred, target_correctness = sess.run(\n",
    "            [train_op, global_step, train_summary_op, dkt.loss, dkt.binary_pred, dkt.pred, dkt.target_correctness],\n",
    "            feed_dict)\n",
    "\n",
    "        auc, accuracy, precision, recall = gen_metrics(params['seq_len'], binary_pred, pred, target_correctness)\n",
    "\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"train: {}: step {}, loss {}, acc {}, auc: {}, precision: {}, recall: {}\".format(time_str, step, loss, accuracy, \n",
    "                                                                                               auc, precision, recall))\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(self, params, dev_summary_op, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        dkt = self.test_dkt\n",
    "        sess = self.sess\n",
    "        global_step = self.global_step\n",
    "\n",
    "        feed_dict = {dkt.input_data: params['input_x'],\n",
    "                     dkt.target_id: params['target_id'],\n",
    "                     dkt.target_correctness: params['target_correctness'],\n",
    "                     dkt.max_steps: params['max_len'],\n",
    "                     dkt.sequence_len: params['seq_len'],\n",
    "                     dkt.keep_prob: 1.0}\n",
    "        step, summaries, loss, pred, binary_pred, target_correctness = sess.run(\n",
    "            [global_step, dev_summary_op, dkt.loss, dkt.pred, dkt.binary_pred, dkt.target_correctness],\n",
    "            feed_dict)\n",
    "\n",
    "        auc, accuracy, precision, recall = gen_metrics(params['seq_len'], binary_pred, pred, target_correctness)\n",
    "\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "        return loss, accuracy, auc, precision, recall\n",
    "\n",
    "    def run_epoch(self, fileName):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param filePath:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 实例化配置参数对象\n",
    "        config = Config()\n",
    "\n",
    "        # 实例化数据生成对象\n",
    "        dataGen = DataGenerator(fileName, config)\n",
    "        dataGen.gen_attr()  # 生成训练集和测试集\n",
    "\n",
    "        train_seqs = dataGen.train_seqs\n",
    "        test_seqs = dataGen.test_seqs\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        self.sess = sess\n",
    "\n",
    "        with sess.as_default():\n",
    "            # 实例化dkt模型对象\n",
    "            with tf.name_scope(\"train\"):\n",
    "                with tf.variable_scope(\"dkt\", reuse=None):\n",
    "                    train_dkt = TensorFlowDKT(config)\n",
    "\n",
    "            with tf.name_scope(\"test\"):\n",
    "                with tf.variable_scope(\"dkt\", reuse=True):\n",
    "                    test_dkt = TensorFlowDKT(config)\n",
    "\n",
    "            self.train_dkt = train_dkt\n",
    "            self.test_dkt = test_dkt\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            self.global_step = global_step\n",
    "\n",
    "            # 定义一个优化器\n",
    "            optimizer = tf.train.AdamOptimizer(config.trainConfig.learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(train_dkt.loss)\n",
    "\n",
    "            # 对梯度进行截断，并且加上梯度噪音\n",
    "            grads_and_vars = [(tf.clip_by_norm(g, config.trainConfig.max_grad_norm), v)\n",
    "                              for g, v in grads_and_vars if g is not None]\n",
    "            # grads_and_vars = [(self.add_gradient_noise(g), v) for g, v in grads_and_vars]\n",
    "\n",
    "            # 定义图中最后的节点\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name=\"train_op\")\n",
    "\n",
    "            # 保存各种变量或结果的值\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"writing to {}\".format(out_dir))\n",
    "\n",
    "            # 训练时的 Summaries\n",
    "            train_loss_summary = tf.summary.scalar(\"loss\", train_dkt.loss)\n",
    "            train_summary_op = tf.summary.merge([train_loss_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # 测试时的 summaries\n",
    "            test_loss_summary = tf.summary.scalar(\"loss\", test_dkt.loss)\n",
    "            dev_summary_op = tf.summary.merge([test_loss_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            print(\"初始化完毕，开始训练\")\n",
    "            for i in range(config.trainConfig.epochs):\n",
    "                np.random.shuffle(train_seqs)\n",
    "                for params in dataGen.next_batch(train_seqs):\n",
    "                    # 批次获得训练集，训练模型\n",
    "                    self.train_step(params, train_op, train_summary_op, train_summary_writer)\n",
    "\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    # train_step.run(feed_dict={x: batch_train[0], y_actual: batch_train[1], keep_prob: 0.5})\n",
    "                    # 对结果进行记录\n",
    "                    if current_step % config.trainConfig.evaluate_every == 0:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        # 获得测试数据\n",
    "\n",
    "                        losses = []\n",
    "                        accuracys = []\n",
    "                        aucs = []\n",
    "                        precisions = []\n",
    "                        recalls = []\n",
    "                        for params in dataGen.next_batch(test_seqs):\n",
    "                            loss, accuracy, auc, precision, recall = self.dev_step(params, dev_summary_op, writer=None)\n",
    "                            losses.append(loss)\n",
    "                            accuracys.append(accuracy)\n",
    "                            aucs.append(auc)\n",
    "                            precisions.append(precision)\n",
    "                            recalls.append(recall)\n",
    "\n",
    "                        time_str = datetime.datetime.now().isoformat()\n",
    "                        print(\"dev: {}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".\n",
    "                              format(time_str, current_step, mean(losses), mean(accuracys), mean(aucs), mean(precisions), mean(recalls)))\n",
    "\n",
    "                    if current_step % config.trainConfig.checkpoint_every == 0:\n",
    "                        path = saver.save(sess, \"model/my-model\", global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fileName = \"./data/assistments.txt\"\n",
    "    dktEngine = DKTEngine()\n",
    "    dktEngine.run_epoch(fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-80244db9fbdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mfileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/assistments.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-80244db9fbdd>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                               sequence_len: params[\"seq_len\"]})\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seq_len\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_correctness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0maccuracys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "def load_model(fileName):\n",
    "    # 实例化配置参数对象\n",
    "    config = Config()\n",
    "\n",
    "    # 实例化数据生成对象\n",
    "    dataGen = DataGenerator(fileName, config)\n",
    "    dataGen.gen_attr()  # 生成训练集和测试集\n",
    "\n",
    "    test_seqs = dataGen.test_seqs\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        accuracys = []\n",
    "        aucs = []\n",
    "        step = 1\n",
    "\n",
    "        for params in dataGen.next_batch(test_seqs):\n",
    "            print(\"step: {}\".format(step))\n",
    "\n",
    "            saver = tf.train.import_meta_graph(\"model/my-model-800.meta\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(\"model/\"))\n",
    "\n",
    "            # 获得默认的计算图结构\n",
    "            graph = tf.get_default_graph()\n",
    "\n",
    "            # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "            input_x = graph.get_operation_by_name(\"test/dkt/input_x\").outputs[0]\n",
    "            target_id = graph.get_operation_by_name(\"test/dkt/target_id\").outputs[0]\n",
    "            keep_prob = graph.get_operation_by_name(\"test/dkt/keep_prob\").outputs[0]\n",
    "            max_steps = graph.get_operation_by_name(\"test/dkt/max_steps\").outputs[0]\n",
    "            sequence_len = graph.get_operation_by_name(\"test/dkt/sequence_len\").outputs[0]\n",
    "\n",
    "            # 获得输出的结果\n",
    "            pred_all = graph.get_tensor_by_name(\"test/dkt/pred_all:0\")\n",
    "            pred = graph.get_tensor_by_name(\"test/dkt/pred:0\")\n",
    "            binary_pred = graph.get_tensor_by_name(\"test/dkt/binary_pred:0\")\n",
    "\n",
    "            target_correctness = params['target_correctness']\n",
    "            pred_all, pred, binary_pred = sess.run([pred_all, pred, binary_pred],\n",
    "                                                   feed_dict={input_x: params[\"input_x\"],\n",
    "                                                              target_id: params[\"target_id\"],\n",
    "                                                              keep_prob: 1.0,\n",
    "                                                              max_steps: params[\"max_len\"],\n",
    "                                                              sequence_len: params[\"seq_len\"]})\n",
    "\n",
    "            auc, acc, precision, recall = gen_metrics(params[\"seq_len\"], binary_pred, pred, target_correctness)\n",
    "            print(auc, acc)\n",
    "            accuracys.append(acc)\n",
    "            aucs.append(auc)\n",
    "            step += 1\n",
    "\n",
    "        aucMean = mean(aucs)\n",
    "        accMean = mean(accuracys)\n",
    "\n",
    "        print(\"inference  auc: {}  acc: {}\".format(aucMean, accMean))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fileName = \"./data/assistments.txt\"\n",
    "    load_model(fileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
