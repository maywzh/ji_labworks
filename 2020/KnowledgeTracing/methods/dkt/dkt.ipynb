{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置config\n",
    "class TrainConfig(object):\n",
    "    epochs = 10\n",
    "    decay_rate = 0.92\n",
    "    learning_rate = 0.01\n",
    "    evaluate_every = 100\n",
    "    checkpoint_every = 100\n",
    "    max_grad_norm = 3.0\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    hidden_layers = [200]\n",
    "    dropout_keep_prob = 0.6\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    batch_size = 32\n",
    "    num_skills = 124\n",
    "    input_size = num_skills * 2\n",
    "\n",
    "    trainConfig = TrainConfig()\n",
    "    modelConfig = ModelConfig()\n",
    "    \n",
    "\n",
    "# 实例化config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "class DataGenerator(object):\n",
    "    # 导入的seqs是train_seqs，或者是test_seqs\n",
    "    def __init__(self, fileName, config):\n",
    "        self.fileName = fileName\n",
    "        self.train_seqs = []\n",
    "        self.test_seqs = []\n",
    "        self.infer_seqs = []\n",
    "        self.batch_size = config.batch_size\n",
    "        self.pos = 0\n",
    "        self.end = False\n",
    "        self.num_skills = config.num_skills\n",
    "        self.skills_to_int = {}  # 知识点到索引的映射\n",
    "        self.int_to_skills = {}  # 索引到知识点的映射\n",
    "\n",
    "    def read_file(self):\n",
    "        # 从文件中读取数据，返回读取出来的数据和知识点个数\n",
    "        # 保存每个学生的做题信息 {学生id: [[知识点id，答题结果], [知识点id，答题结果], ...]}，用一个二元列表来表示一个学生的答题信息\n",
    "        seqs_by_student = {}\n",
    "        skills = []  # 统计知识点的数量，之后输入的向量长度就是两倍的知识点数量\n",
    "        count = 0\n",
    "        with open(self.fileName, 'r') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split(\" \")  # 一个列表，[学生id，知识点id，答题结果]\n",
    "                student, skill, is_correct = int(fields[0]), int(fields[1]), int(fields[2])\n",
    "                skills.append(skill)  # skill实际上是用该题所属知识点来表示的\n",
    "                seqs_by_student[student] = seqs_by_student.get(student, []) + [[skill, is_correct]]  # 保存每个学生的做题信息\n",
    "        return seqs_by_student, list(set(skills))\n",
    "\n",
    "    def gen_dict(self, unique_skills):\n",
    "        \"\"\"\n",
    "        构建知识点映射表，将知识点id映射到[0, 1, 2...]表示\n",
    "        :param unique_skills: 无重复的知识点列表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sorted_skills = sorted(unique_skills)\n",
    "        skills_to_int = {}\n",
    "        int_to_skills = {}\n",
    "        for i in range(len(sorted_skills)):\n",
    "            skills_to_int[sorted_skills[i]] = i\n",
    "            int_to_skills[i] = sorted_skills[i]\n",
    "\n",
    "        self.skills_to_int = skills_to_int\n",
    "        self.int_to_skills = int_to_skills\n",
    "\n",
    "    def split_dataset(self, seqs_by_student, sample_rate=0.2, random_seed=1):\n",
    "        # 将数据分割成测试集和训练集\n",
    "        sorted_keys = sorted(seqs_by_student.keys())  # 得到排好序的学生id的列表\n",
    "\n",
    "        random.seed(random_seed)\n",
    "        # 随机抽取学生id，将这部分学生作为测试集\n",
    "        test_keys = set(random.sample(sorted_keys, int(len(sorted_keys) * sample_rate)))\n",
    "\n",
    "        # 此时是一个三层的列表来表示的，最外层的列表中的每一个列表表示一个学生的做题信息\n",
    "        test_seqs = [seqs_by_student[k] for k in seqs_by_student if k in test_keys]\n",
    "        train_seqs = [seqs_by_student[k] for k in seqs_by_student if k not in test_keys]\n",
    "        return train_seqs, test_seqs\n",
    "\n",
    "    def gen_attr(self, is_infer=False):\n",
    "        \"\"\"\n",
    "        生成待处理的数据集\n",
    "        :param is_infer: 判断当前是训练模型还是利用模型进行预测\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if is_infer:\n",
    "            seqs_by_students, skills = self.read_file()\n",
    "            self.infer_seqs = seqs_by_students\n",
    "        else:\n",
    "            seqs_by_students, skills = self.read_file()\n",
    "            train_seqs, test_seqs = self.split_dataset(seqs_by_students)\n",
    "            self.train_seqs = train_seqs\n",
    "            self.test_seqs = test_seqs\n",
    "\n",
    "        self.gen_dict(skills)  # 生成知识点到索引的映射字典\n",
    "\n",
    "    def pad_sequences(self, sequences, maxlen=None, value=0.):\n",
    "        # 按每个batch中最长的序列进行补全, 传入的sequences是二层列表\n",
    "        # 统计一个batch中每个序列的长度，其实等于seqs_len\n",
    "        lengths = [len(s) for s in sequences]\n",
    "        # 统计下该batch中序列的数量\n",
    "        nb_samples = len(sequences)\n",
    "        # 如果没有传入maxlen参数就自动获取最大的序列长度\n",
    "        if maxlen is None:\n",
    "            maxlen = np.max(lengths)\n",
    "        # 构建x矩阵\n",
    "        x = (np.ones((nb_samples, maxlen)) * value).astype(np.int32)\n",
    "\n",
    "        # 遍历batch，去除每一个序列\n",
    "        for idx, s in enumerate(sequences):\n",
    "            trunc = np.asarray(s, dtype=np.int32)\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_to_one_hot(self, num, dim):\n",
    "        # 将题目转换成one-hot的形式， 其中dim=num_skills * 2，前半段表示错误，后半段表示正确\n",
    "        base = np.zeros(dim)\n",
    "        if num >= 0:\n",
    "            base[num] += 1\n",
    "        return base\n",
    "\n",
    "    def format_data(self, seqs):\n",
    "        # 生成输入数据和输出数据，输入数据是每条序列的前n-1个元素，输出数据是每条序列的后n-1个元素\n",
    "\n",
    "        # 统计一个batch_size中每条序列的长度，在这里不对序列固定长度，通过条用tf.nn.dynamic_rnn让序列长度可以不固定\n",
    "        seq_len = np.array(list(map(lambda seq: len(seq) - 1, seqs)))\n",
    "        max_len = max(seq_len)  # 获得一个batch_size中最大的长度\n",
    "        # i表示第i条数据，j只从0到len(i)-1，x作为输入只取前len(i)-1个，sequences=[j[0] + num_skills * j[1], ....]\n",
    "        # 此时要将知识点id j[0] 转换成index表示\n",
    "        x_sequences = np.array([[(self.skills_to_int[j[0]] + self.num_skills * j[1]) for j in i[:-1]] for i in seqs])\n",
    "        # 将输入的序列用-1进行补全，补全后的长度为当前batch的最大序列长度\n",
    "        x = self.pad_sequences(x_sequences, maxlen=max_len, value=-1)\n",
    "\n",
    "        # 构建输入值input_x，x为一个二层列表，i表示一个学生的做题信息，也就是一个序列，j就是一道题的信息\n",
    "        input_x = np.array([[self.num_to_one_hot(j, self.num_skills * 2) for j in i] for i in x])\n",
    "\n",
    "        # 遍历batch_size，然后取每条序列的后len(i)-1 个元素中的知识点id为target_id\n",
    "        target_id_seqs = np.array([[self.skills_to_int[j[0]] for j in i[1:]] for i in seqs])\n",
    "        target_id = self.pad_sequences(target_id_seqs, maxlen=max_len, value=0)\n",
    "\n",
    "        # 同target_id\n",
    "        target_correctness_seqs = np.array([[j[1] for j in i[1:]] for i in seqs])\n",
    "        target_correctness = self.pad_sequences(target_correctness_seqs, maxlen=max_len, value=0)\n",
    "\n",
    "        return dict(input_x=input_x, target_id=target_id, target_correctness=target_correctness,\n",
    "                    seq_len=seq_len, max_len=max_len)\n",
    "\n",
    "    def next_batch(self, seqs):\n",
    "        # 接收一个序列，生成batch\n",
    "\n",
    "        length = len(seqs)\n",
    "        num_batchs = length // self.batch_size\n",
    "        start = 0\n",
    "        for i in range(num_batchs):\n",
    "            batch_seqs = seqs[start: start + self.batch_size]\n",
    "            start += self.batch_size\n",
    "            params = self.format_data(batch_seqs)\n",
    "\n",
    "            yield params\n",
    "\n",
    "            \n",
    "fileName = \"./data/assistments.txt\"\n",
    "dataGen = DataGenerator(fileName, config)\n",
    "dataGen.gen_attr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dataGen' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a4ca4b24e720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_seqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skill num: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskills_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_seqs length: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_seqs length: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataGen' is not defined"
     ]
    }
   ],
   "source": [
    "train_seqs = dataGen.train_seqs\n",
    "params = next(dataGen.next_batch(train_seqs))\n",
    "print(\"skill num: {}\".format(len(dataGen.skills_to_int)))\n",
    "print(\"train_seqs length: {}\".format(len(dataGen.train_seqs)))\n",
    "print(\"test_seqs length: {}\".format(len(dataGen.test_seqs)))\n",
    "print(\"input_x shape: {}\".format(params['input_x'].shape))\n",
    "print(params[\"input_x\"][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class TensorFlowDKT(object):\n",
    "    def __init__(self, config):\n",
    "        # 导入配置好的参数\n",
    "        self.hiddens = hiddens = config.modelConfig.hidden_layers\n",
    "        self.num_skills = num_skills = config.num_skills\n",
    "        self.input_size = input_size = config.input_size\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.keep_prob_value = config.modelConfig.dropout_keep_prob\n",
    "\n",
    "        # 定义需要喂给模型的参数\n",
    "        self.max_steps = tf.placeholder(tf.int32, name=\"max_steps\")  # 当前batch中最大序列长度\n",
    "        self.input_data = tf.placeholder(tf.float32, [batch_size, None, input_size], name=\"input_x\")\n",
    "\n",
    "        self.sequence_len = tf.placeholder(tf.int32, [batch_size], name=\"sequence_len\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")  # dropout keep prob\n",
    "\n",
    "        self.target_id = tf.placeholder(tf.int32, [batch_size, None], name=\"target_id\")\n",
    "        self.target_correctness = tf.placeholder(tf.float32, [batch_size, None], name=\"target_correctness\")\n",
    "        self.flat_target_correctness = None\n",
    "\n",
    "        # 构建lstm模型结构\n",
    "        hidden_layers = []\n",
    "        for idx, hidden_size in enumerate(hiddens):\n",
    "            lstm_layer = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            hidden_layer = tf.nn.rnn_cell.DropoutWrapper(cell=lstm_layer,\n",
    "                                                         output_keep_prob=self.keep_prob)\n",
    "            hidden_layers.append(hidden_layer)\n",
    "        self.hidden_cell = tf.nn.rnn_cell.MultiRNNCell(cells=hidden_layers, state_is_tuple=True)\n",
    "\n",
    "        # 采用动态rnn，动态输入序列的长度\n",
    "        outputs, self.current_state = tf.nn.dynamic_rnn(cell=self.hidden_cell,\n",
    "                                                        inputs=self.input_data,\n",
    "                                                        sequence_length=self.sequence_len,\n",
    "                                                        dtype=tf.float32)\n",
    "\n",
    "        # 隐层到输出层的权重系数[最后隐层的神经元数量，知识点数]\n",
    "        output_w = tf.get_variable(\"W\", [hiddens[-1], num_skills])\n",
    "        output_b = tf.get_variable(\"b\", [num_skills])\n",
    "\n",
    "        self.output = tf.reshape(outputs, [batch_size * self.max_steps, hiddens[-1]])\n",
    "        # 因为权值共享的原因，对生成的矩阵[batch_size * self.max_steps, num_skills]中的每一行都加上b\n",
    "        self.logits = tf.matmul(self.output, output_w) + output_b\n",
    "\n",
    "        self.mat_logits = tf.reshape(self.logits, [batch_size, self.max_steps, num_skills])\n",
    "\n",
    "        # 对每个batch中每个序列中的每个时间点的输出中的每个值进行sigmoid计算，这里的值表示对某个知识点的掌握情况，\n",
    "        # 每个时间点都会输出对所有知识点的掌握情况\n",
    "        self.pred_all = tf.sigmoid(self.mat_logits, name=\"pred_all\")\n",
    "\n",
    "        # 计算损失loss\n",
    "        flat_logits = tf.reshape(self.logits, [-1])\n",
    "\n",
    "        flat_target_correctness = tf.reshape(self.target_correctness, [-1])\n",
    "        self.flat_target_correctness = flat_target_correctness\n",
    "\n",
    "        flat_base_target_index = tf.range(batch_size * self.max_steps) * num_skills\n",
    "\n",
    "        # 因为flat_logits的长度为batch_size * num_steps * num_skills，我们要根据每一步的target_id将其长度变成batch_size * num_steps\n",
    "        flat_base_target_id = tf.reshape(self.target_id, [-1])\n",
    "\n",
    "        flat_target_id = flat_base_target_id + flat_base_target_index\n",
    "        # gather是从一个tensor中切片一个子集\n",
    "        flat_target_logits = tf.gather(flat_logits, flat_target_id)\n",
    "\n",
    "        # 对切片后的数据进行sigmoid转换\n",
    "        self.pred = tf.sigmoid(tf.reshape(flat_target_logits, [batch_size, self.max_steps]), name=\"pred\")\n",
    "        # 将sigmoid后的值表示为0或1\n",
    "        self.binary_pred = tf.cast(tf.greater_equal(self.pred, 0.5), tf.float32, name=\"binary_pred\")\n",
    "\n",
    "        # 定义损失函数\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # flat_target_logits_sigmoid = tf.nn.log_softmax(flat_target_logits)\n",
    "            # self.loss = -tf.reduce_mean(flat_target_correctness * flat_target_logits_sigmoid)\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=flat_target_correctness,\n",
    "                                                                               logits=flat_target_logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def gen_metrics(sequence_len, binary_pred, pred, target_correctness):\n",
    "    \"\"\"\n",
    "    生成auc和accuracy的指标值\n",
    "    :param sequence_len: 每一个batch中各序列的长度组成的列表\n",
    "    :param binary_pred:\n",
    "    :param pred:\n",
    "    :param target_correctness:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    binary_preds = []\n",
    "    preds = []\n",
    "    target_correctnesses = []\n",
    "    for seq_idx, seq_len in enumerate(sequence_len):\n",
    "        binary_preds.append(binary_pred[seq_idx, :seq_len])\n",
    "        preds.append(pred[seq_idx, :seq_len])\n",
    "        target_correctnesses.append(target_correctness[seq_idx, :seq_len])\n",
    "\n",
    "    new_binary_pred = np.concatenate(binary_preds)\n",
    "    new_pred = np.concatenate(preds)\n",
    "    new_target_correctness = np.concatenate(target_correctnesses)\n",
    "\n",
    "    auc = roc_auc_score(new_target_correctness, new_pred)\n",
    "    accuracy = accuracy_score(new_target_correctness, new_binary_pred)\n",
    "    precision = precision_score(new_target_correctness, new_binary_pred)\n",
    "    recall = recall_score(new_target_correctness, new_binary_pred)\n",
    "\n",
    "    return auc, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "all: 0.9547038327526133\n",
      "train: 2020-11-07T15:40:05.805521: step 940, loss 0.09880921989679337, acc 0.7395054629097182, auc: 0.7128551923237681, precision: 0.7561385805583586, recall: 0.9254837381638534\n",
      "train: 2020-11-07T15:40:07.171199: step 941, loss 0.09308870136737823, acc 0.7241877256317689, auc: 0.697976385541382, precision: 0.7366120218579235, recall: 0.936761640027797\n",
      "train: 2020-11-07T15:40:08.531774: step 942, loss 0.06172078475356102, acc 0.819023569023569, auc: 0.8548883015303117, precision: 0.8440428380187416, recall: 0.933728248796742\n",
      "train: 2020-11-07T15:40:10.565070: step 943, loss 0.035755448043346405, acc 0.8751334044823906, auc: 0.9516302952503208, precision: 0.8897338403041825, recall: 0.9171332586786114\n",
      "train: 2020-11-07T15:40:20.666727: step 944, loss 0.03103458136320114, acc 0.9003960396039604, auc: 0.9616044395593617, precision: 0.918724279835391, recall: 0.9502527267890396\n",
      "train: 2020-11-07T15:40:21.609071: step 945, loss 0.09118214249610901, acc 0.7403846153846154, auc: 0.7903811958032861, precision: 0.7603271983640082, recall: 0.8924627940470475\n",
      "train: 2020-11-07T15:40:22.312266: step 946, loss 0.08765742927789688, acc 0.71826171875, auc: 0.7535875124036333, precision: 0.7258172114743162, recall: 0.8676236044657097\n",
      "train: 2020-11-07T15:40:24.020353: step 947, loss 0.06692900508642197, acc 0.8247026532479415, auc: 0.8983013789718665, precision: 0.8345475910693302, recall: 0.933245729303548\n",
      "train: 2020-11-07T15:40:24.490128: step 948, loss 0.08035135269165039, acc 0.7082273112807463, auc: 0.7857587064676618, precision: 0.6926910299003323, recall: 0.7239583333333334\n",
      "train: 2020-11-07T15:40:26.233461: step 949, loss 0.055106133222579956, acc 0.8627450980392157, auc: 0.9380413359535527, precision: 0.8543329532497149, recall: 0.938615721891638\n",
      "train: 2020-11-07T15:40:28.113800: step 950, loss 0.10152770578861237, acc 0.7009718018161543, auc: 0.7671032711535277, precision: 0.6974395113929998, recall: 0.8344575604272063\n",
      "train: 2020-11-07T15:40:29.868500: step 951, loss 0.08951835334300995, acc 0.7229811169144235, auc: 0.750029577768653, precision: 0.7357540418764908, recall: 0.8790373654211526\n",
      "train: 2020-11-07T15:40:31.031529: step 952, loss 0.08863729983568192, acc 0.7311596431867118, auc: 0.7250417461832062, precision: 0.7513935340022296, recall: 0.9079479119892232\n",
      "train: 2020-11-07T15:40:33.704816: step 953, loss 0.03748447448015213, acc 0.924668750930475, auc: 0.9770562397153414, precision: 0.9236747708250299, recall: 0.9741488020176545\n",
      "train: 2020-11-07T15:40:35.276843: step 954, loss 0.05417044833302498, acc 0.8353310502283106, auc: 0.8746746561678735, precision: 0.8527796842827728, recall: 0.943790353209267\n",
      "train: 2020-11-07T15:40:37.384683: step 955, loss 0.05006280541419983, acc 0.8850487210718636, auc: 0.9457985332003657, precision: 0.9032561641257293, recall: 0.9521825396825396\n",
      "train: 2020-11-07T15:40:38.255023: step 956, loss 0.08398902416229248, acc 0.7153344208809136, auc: 0.7547742019019119, precision: 0.7416107382550335, recall: 0.8489116517285531\n",
      "train: 2020-11-07T15:40:40.317362: step 957, loss 0.07155988365411758, acc 0.7282062486344767, auc: 0.7612644332233678, precision: 0.7446684195150453, recall: 0.8732442617334704\n",
      "train: 2020-11-07T15:40:41.688419: step 958, loss 0.0893702283501625, acc 0.7245450073782588, auc: 0.7584987975858573, precision: 0.7360676268002505, recall: 0.8945966514459666\n",
      "train: 2020-11-07T15:40:43.462510: step 959, loss 0.03897028788924217, acc 0.8845302119582411, auc: 0.9287123385939744, precision: 0.8946972546069951, recall: 0.9655032467532467\n",
      "train: 2020-11-07T15:40:44.776553: step 960, loss 0.06200416758656502, acc 0.7146596858638743, auc: 0.7175888758474064, precision: 0.7267283614589004, recall: 0.8977807666442502\n",
      "train: 2020-11-07T15:40:46.258653: step 961, loss 0.11024066060781479, acc 0.7750865051903114, auc: 0.7166200283944775, precision: 0.7894642215030565, recall: 0.9576881134133043\n",
      "train: 2020-11-07T15:40:46.568186: step 962, loss 0.14331740140914917, acc 0.6949152542372882, auc: 0.7576180181277469, precision: 0.6986006458557589, recall: 0.8102372034956304\n",
      "train: 2020-11-07T15:40:46.947564: step 963, loss 0.13318637013435364, acc 0.7408993576017131, auc: 0.8044102737411288, precision: 0.7252663622526636, recall: 0.8856877323420075\n",
      "train: 2020-11-07T15:40:48.040497: step 964, loss 0.06845641136169434, acc 0.7356701030927835, auc: 0.7836958514766321, precision: 0.7615298087739033, recall: 0.8618714194780395\n",
      "train: 2020-11-07T15:40:49.849326: step 965, loss 0.08489704132080078, acc 0.7624932831810854, auc: 0.7937319980729609, precision: 0.775129300652125, recall: 0.9135966074741585\n",
      "train: 2020-11-07T15:40:51.205837: step 966, loss 0.07588331401348114, acc 0.7215549936788875, auc: 0.7460078586511989, precision: 0.7484586929716399, recall: 0.8712918660287081\n",
      "train: 2020-11-07T15:41:04.134892: step 967, loss 0.025850269943475723, acc 0.9258541740049313, auc: 0.9708002013683634, precision: 0.931173253534207, recall: 0.9809761930644635\n",
      "train: 2020-11-07T15:41:04.675843: step 968, loss 0.09567117691040039, acc 0.7045951859956237, auc: 0.7168119783311246, precision: 0.7290076335877863, recall: 0.8632768361581921\n",
      "train: 2020-11-07T15:41:05.694664: step 969, loss 0.09051329642534256, acc 0.7619331742243437, auc: 0.7412497424818927, precision: 0.7815687653886739, recall: 0.9262192580241767\n",
      "train: 2020-11-07T15:41:06.969248: step 970, loss 0.10879779607057571, acc 0.7375049980007997, auc: 0.7602515343489346, precision: 0.7471583733265976, recall: 0.9045871559633027\n",
      "train: 2020-11-07T15:41:08.418918: step 971, loss 0.0704617127776146, acc 0.750363689263893, auc: 0.7741760480152953, precision: 0.7626314004747372, recall: 0.9343581221437474\n",
      "train: 2020-11-07T15:41:10.104803: step 972, loss 0.04772648960351944, acc 0.8398902104300091, auc: 0.8820159338442921, precision: 0.8506819813352476, recall: 0.9560306575231948\n",
      "train: 2020-11-07T15:41:12.061158: step 973, loss 0.06651997566223145, acc 0.8372352285395763, auc: 0.8973485606499688, precision: 0.8373023814288573, recall: 0.9524243114045071\n",
      "train: 2020-11-07T15:41:13.494038: step 974, loss 0.06030074134469032, acc 0.8540540540540541, auc: 0.9058644134187628, precision: 0.8686679174484052, recall: 0.9535157399235069\n",
      "train: 2020-11-07T15:41:15.312356: step 975, loss 0.046824220567941666, acc 0.8557572743098731, auc: 0.9239419361646378, precision: 0.8556962025316456, recall: 0.9561527581329562\n",
      "train: 2020-11-07T15:41:17.129972: step 976, loss 0.06353624910116196, acc 0.8181624840493407, auc: 0.882074331109345, precision: 0.8078408078408078, recall: 0.9289617486338798\n",
      "train: 2020-11-07T15:41:18.853675: step 977, loss 0.038031790405511856, acc 0.891777705557361, auc: 0.9530112466736957, precision: 0.9020530367835757, recall: 0.9530049706281066\n",
      "train: 2020-11-07T15:41:20.940707: step 978, loss 0.06420683115720749, acc 0.7436701229804679, auc: 0.8254160423021978, precision: 0.7137315748642359, recall: 0.8498845265588915\n",
      "train: 2020-11-07T15:41:22.106466: step 979, loss 0.0676896721124649, acc 0.7763408139977178, auc: 0.7884990578251785, precision: 0.7730769230769231, recall: 0.9327146171693735\n",
      "train: 2020-11-07T15:41:23.319504: step 980, loss 0.0852830708026886, acc 0.7037392138063279, auc: 0.7054476299313867, precision: 0.7223950233281493, recall: 0.8971511347175277\n",
      "train: 2020-11-07T15:41:24.144501: step 981, loss 0.07773224264383316, acc 0.7633451957295374, auc: 0.8122633483735597, precision: 0.7532070861331704, recall: 0.9059515062454078\n",
      "train: 2020-11-07T15:41:25.489774: step 982, loss 0.07582495361566544, acc 0.7086690299709958, auc: 0.696670273176559, precision: 0.7225130890052356, recall: 0.9226361031518625\n",
      "train: 2020-11-07T15:41:26.720535: step 983, loss 0.0989001914858818, acc 0.7282813975448537, auc: 0.7422625793470403, precision: 0.742579908675799, recall: 0.9126622237811294\n",
      "train: 2020-11-07T15:41:27.806298: step 984, loss 0.06635423749685287, acc 0.79419795221843, auc: 0.8963825241150276, precision: 0.7660919540229885, recall: 0.8718116415958143\n",
      "train: 2020-11-07T15:41:29.138004: step 985, loss 0.06339552253484726, acc 0.7900324388086111, auc: 0.8231645882048482, precision: 0.8096532970768185, recall: 0.9400157853196527\n",
      "train: 2020-11-07T15:41:32.635974: step 986, loss 0.03572731465101242, acc 0.8889294182017143, auc: 0.952627816013932, precision: 0.8938826466916354, recall: 0.9511158342189161\n",
      "train: 2020-11-07T15:41:34.244777: step 987, loss 0.05490615963935852, acc 0.8228252194732641, auc: 0.868920249305553, precision: 0.8369565217391305, recall: 0.9437635183850036\n",
      "train: 2020-11-07T15:41:35.591900: step 988, loss 0.0820017158985138, acc 0.7392241379310345, auc: 0.7403685092127303, precision: 0.7545992115637319, recall: 0.9122319301032565\n",
      "train: 2020-11-07T15:41:36.549099: step 989, loss 0.06145600229501724, acc 0.7664041994750657, auc: 0.8035582532854028, precision: 0.7745241581259151, recall: 0.8853556485355648\n",
      "train: 2020-11-07T15:41:38.763466: step 990, loss 0.0687759593129158, acc 0.8288172595351226, auc: 0.8989107646017533, precision: 0.8387096774193549, recall: 0.9328648048825596\n",
      "train: 2020-11-07T15:41:41.343049: step 991, loss 0.049543410539627075, acc 0.8651510872634849, auc: 0.9275528578136998, precision: 0.8678015211879754, recall: 0.9551524815626868\n",
      "train: 2020-11-07T15:41:42.272498: step 992, loss 0.0807926133275032, acc 0.8016378525932666, auc: 0.8781331985078964, precision: 0.8181019332161688, recall: 0.8858230256898192\n",
      "train: 2020-11-07T15:41:44.128695: step 993, loss 0.08625084161758423, acc 0.7192812982998454, auc: 0.7216481507500033, precision: 0.7352313167259786, recall: 0.9019208381839348\n",
      "train: 2020-11-07T15:41:45.254823: step 994, loss 0.09604397416114807, acc 0.7425997425997426, auc: 0.7308100045298239, precision: 0.7662337662337663, recall: 0.9103600293901543\n",
      "train: 2020-11-07T15:41:46.693285: step 995, loss 0.07435604929924011, acc 0.7111924367185117, auc: 0.745879761098218, precision: 0.7301263758662861, recall: 0.8627167630057804\n",
      "train: 2020-11-07T15:41:47.363591: step 996, loss 0.10359995067119598, acc 0.7532091097308489, auc: 0.8289280197713933, precision: 0.717056856187291, recall: 0.8610441767068273\n",
      "train: 2020-11-07T15:41:48.861537: step 997, loss 0.09482664614915848, acc 0.7398868161810941, auc: 0.7148380918250967, precision: 0.7504337050805452, recall: 0.9282648681790313\n",
      "train: 2020-11-07T15:41:52.530945: step 998, loss 0.04155028238892555, acc 0.8528248140528565, auc: 0.9281286060697824, precision: 0.8615782664941786, recall: 0.9327731092436975\n",
      "train: 2020-11-07T15:41:53.994315: step 999, loss 0.05789259821176529, acc 0.8127572016460906, auc: 0.8237504017277507, precision: 0.8371939113170086, recall: 0.9457943925233645\n",
      "train: 2020-11-07T15:42:03.944302: step 1000, loss 0.02673240564763546, acc 0.9175578859931471, auc: 0.972096574653986, precision: 0.9329301075268818, recall: 0.9592316196793809\n",
      "\n",
      "Evaluation:\n",
      "dev: 2020-11-07T15:42:23.497567, step: 1000, loss: 0.07488964526699139, acc: 0.7773070235825288, auc: 0.8149410779099553, precision: 0.793519726307673, recall: 0.8988541925470629\n",
      "Saved model checkpoint to model/my-model-1000\n",
      "\n",
      "train: 2020-11-07T15:42:25.001114: step 1001, loss 0.08559494465589523, acc 0.7547390592089867, auc: 0.7769993458205113, precision: 0.7792507204610951, recall: 0.9055592766242465\n",
      "train: 2020-11-07T15:42:27.289792: step 1002, loss 0.07279868423938751, acc 0.7040607054963085, auc: 0.7455431307050607, precision: 0.7172659895690365, recall: 0.8635161929940516\n",
      "train: 2020-11-07T15:42:28.884046: step 1003, loss 0.05435830354690552, acc 0.7263374485596708, auc: 0.7609971440000471, precision: 0.7383627608346709, recall: 0.8868894601542416\n",
      "train: 2020-11-07T15:42:29.204466: step 1004, loss 0.13521625101566315, acc 0.7029034436191762, auc: 0.7566850968543428, precision: 0.7222222222222222, recall: 0.834070796460177\n",
      "train: 2020-11-07T15:42:30.436176: step 1005, loss 0.055102232843637466, acc 0.8030842230130486, auc: 0.8639365681926195, precision: 0.8242485990830362, recall: 0.9136081309994354\n",
      "train: 2020-11-07T15:42:32.889315: step 1006, loss 0.04439445585012436, acc 0.8684503901895206, auc: 0.936516867173236, precision: 0.8785662033650329, recall: 0.94492525570417\n",
      "train: 2020-11-07T15:42:34.189970: step 1007, loss 0.03861113637685776, acc 0.8451059535822402, auc: 0.9253745997220713, precision: 0.8401184307920059, recall: 0.9257748776508973\n",
      "train: 2020-11-07T15:42:35.451681: step 1008, loss 0.0663510113954544, acc 0.7086680761099365, auc: 0.7588468492916327, precision: 0.7161410018552876, recall: 0.8342939481268011\n",
      "train: 2020-11-07T15:42:40.264264: step 1009, loss 0.032836563885211945, acc 0.9034362074451161, auc: 0.9531305999203163, precision: 0.9071249715456408, recall: 0.9524378585086042\n",
      "train: 2020-11-07T15:42:42.183445: step 1010, loss 0.07585727423429489, acc 0.7945250659630607, auc: 0.8273597382912854, precision: 0.8083700440528634, recall: 0.9331946370781322\n",
      "train: 2020-11-07T15:42:43.645360: step 1011, loss 0.09608608484268188, acc 0.7256267409470752, auc: 0.7896654544230057, precision: 0.7391185327128388, recall: 0.8637236084452975\n",
      "train: 2020-11-07T15:42:45.525030: step 1012, loss 0.0770621970295906, acc 0.7213333333333334, auc: 0.7090175801873868, precision: 0.745503355704698, recall: 0.90074602659747\n",
      "train: 2020-11-07T15:42:46.486362: step 1013, loss 0.06579001247882843, acc 0.8173849525200877, auc: 0.8750628061801281, precision: 0.8333333333333334, recall: 0.9210526315789473\n",
      "train: 2020-11-07T15:42:47.725467: step 1014, loss 0.09784049540758133, acc 0.7412818096135722, auc: 0.6911997115977899, precision: 0.7641383007236666, recall: 0.9289670902574129\n",
      "train: 2020-11-07T15:42:49.441011: step 1015, loss 0.06538256257772446, acc 0.7811149032992036, auc: 0.8255013479644647, precision: 0.7982456140350878, recall: 0.9279796048438496\n",
      "train: 2020-11-07T15:42:51.333235: step 1016, loss 0.035849589854478836, acc 0.899601593625498, auc: 0.9452218426827802, precision: 0.9142066420664207, recall: 0.9677734375\n",
      "train: 2020-11-07T15:42:52.122893: step 1017, loss 0.050478316843509674, acc 0.8248587570621468, auc: 0.8911045700414764, precision: 0.8484848484848485, recall: 0.9168141592920354\n",
      "train: 2020-11-07T15:42:53.953963: step 1018, loss 0.07289356738328934, acc 0.7443509082853346, auc: 0.7954665500089114, precision: 0.7474355420016634, recall: 0.9173188159237836\n",
      "train: 2020-11-07T15:42:55.941818: step 1019, loss 0.10771248489618301, acc 0.7187706981057094, auc: 0.7713224176441378, precision: 0.7333887043189369, recall: 0.8949929049260085\n",
      "train: 2020-11-07T15:42:58.502452: step 1020, loss 0.05136510357260704, acc 0.8372323552735924, auc: 0.8567367046995351, precision: 0.8451155342026996, recall: 0.962480458572173\n",
      "train: 2020-11-07T15:43:00.112549: step 1021, loss 0.07003607600927353, acc 0.6692262256349675, auc: 0.7448219339556685, precision: 0.6467391304347826, recall: 0.8076923076923077\n",
      "train: 2020-11-07T15:43:02.167250: step 1022, loss 0.043883852660655975, acc 0.8373015873015873, auc: 0.9158166766463066, precision: 0.8541139240506329, recall: 0.9326192121630961\n",
      "train: 2020-11-07T15:43:03.266557: step 1023, loss 0.0784219279885292, acc 0.7659699735060347, auc: 0.7604024914710206, precision: 0.788596198732911, recall: 0.9362628661916073\n",
      "train: 2020-11-07T15:43:07.161680: step 1024, loss 0.040668241679668427, acc 0.8617126680820948, auc: 0.9283959438784708, precision: 0.8617932609937179, recall: 0.9474675596483885\n",
      "train: 2020-11-07T15:43:09.005249: step 1025, loss 0.06339054554700851, acc 0.8139857712996703, auc: 0.8827187236235706, precision: 0.8315381301163292, recall: 0.9301204819277108\n",
      "train: 2020-11-07T15:43:10.978723: step 1026, loss 0.06477930396795273, acc 0.7818963751791931, auc: 0.8143202912842933, precision: 0.800976800976801, recall: 0.9291784702549575\n",
      "train: 2020-11-07T15:43:12.266982: step 1027, loss 0.09641341865062714, acc 0.7400598546387345, auc: 0.7520610483997212, precision: 0.7664107010156056, recall: 0.918918918918919\n",
      "train: 2020-11-07T15:43:13.407845: step 1028, loss 0.08794082701206207, acc 0.7669348559563977, auc: 0.7956627675112082, precision: 0.783381829938571, recall: 0.9139947189739721\n",
      "train: 2020-11-07T15:43:14.431114: step 1029, loss 0.08242757618427277, acc 0.7810304449648712, auc: 0.8355789089060464, precision: 0.7961309523809523, recall: 0.9145299145299145\n",
      "train: 2020-11-07T15:43:14.911379: step 1030, loss 0.11424825340509415, acc 0.7045690550363447, auc: 0.7422373641538726, precision: 0.7127882599580713, recall: 0.865874363327674\n",
      "train: 2020-11-07T15:43:15.175481: step 1031, loss 0.10860620439052582, acc 0.7314814814814815, auc: 0.7949961985875446, precision: 0.750788643533123, recall: 0.8221070811744386\n",
      "train: 2020-11-07T15:43:16.348474: step 1032, loss 0.0843936875462532, acc 0.7498512790005949, auc: 0.7553561883957413, precision: 0.7632045373980858, recall: 0.9256233877901978\n",
      "train: 2020-11-07T15:43:17.643268: step 1033, loss 0.06647005677223206, acc 0.7350677903994137, auc: 0.7572298138139582, precision: 0.7555242125058769, recall: 0.8878453038674033\n",
      "train: 2020-11-07T15:43:18.720750: step 1034, loss 0.06259872764348984, acc 0.845703125, auc: 0.9262262485337533, precision: 0.8665581773799838, recall: 0.9044585987261147\n",
      "train: 2020-11-07T15:43:20.392197: step 1035, loss 0.045864515006542206, acc 0.8272351256761057, auc: 0.9004165029469549, precision: 0.840948275862069, recall: 0.9181176470588235\n",
      "train: 2020-11-07T15:43:20.627349: step 1036, loss 0.16411232948303223, acc 0.7315340909090909, auc: 0.7198647832229643, precision: 0.7549611734253667, recall: 0.9029927760577915\n",
      "train: 2020-11-07T15:43:22.027917: step 1037, loss 0.07906404882669449, acc 0.7683076169483223, auc: 0.7443948392406741, precision: 0.7857142857142857, recall: 0.9393837910247823\n",
      "train: 2020-11-07T15:43:22.486694: step 1038, loss 0.11557968705892563, acc 0.7180762852404643, auc: 0.7601028289754557, precision: 0.7353823088455772, recall: 0.8620386643233744\n",
      "train: 2020-11-07T15:43:24.356101: step 1039, loss 0.07997316867113113, acc 0.6960535425801985, auc: 0.7132681066689166, precision: 0.7123481209381182, recall: 0.8939716312056738\n",
      "train: 2020-11-07T15:43:25.879400: step 1040, loss 0.04393525421619415, acc 0.8663711209626346, auc: 0.9119615969500312, precision: 0.8846761453396524, recall: 0.9451476793248945\n",
      "train: 2020-11-07T15:43:27.484481: step 1041, loss 0.06061837449669838, acc 0.7586663138396401, auc: 0.8536959642087153, precision: 0.7777777777777778, recall: 0.8429319371727748\n",
      "train: 2020-11-07T15:43:29.186752: step 1042, loss 0.06799610704183578, acc 0.713953488372093, auc: 0.7570636601975633, precision: 0.716376306620209, recall: 0.8752660706683695\n",
      "train: 2020-11-07T15:43:30.389359: step 1043, loss 0.06807605922222137, acc 0.7463054187192119, auc: 0.7584944241950881, precision: 0.7548974943052392, recall: 0.9005434782608696\n",
      "train: 2020-11-07T15:43:30.715907: step 1044, loss 0.1301286369562149, acc 0.69140625, auc: 0.726366169770425, precision: 0.694229112833764, recall: 0.8713513513513513\n",
      "train: 2020-11-07T15:43:32.181648: step 1045, loss 0.09122787415981293, acc 0.6896858988638895, auc: 0.7114104557124167, precision: 0.6998279816513762, recall: 0.8758521707929674\n",
      "train: 2020-11-07T15:43:34.294237: step 1046, loss 0.0638379454612732, acc 0.7691338015487124, auc: 0.8347341611044353, precision: 0.7854424357754519, recall: 0.8967952199891364\n",
      "train: 2020-11-07T15:43:35.560406: step 1047, loss 0.10689554363489151, acc 0.7231615400285191, auc: 0.706372047081213, precision: 0.7528985507246376, recall: 0.9026933101650738\n",
      "train: 2020-11-07T15:43:37.619452: step 1048, loss 0.04652697965502739, acc 0.7818181818181819, auc: 0.8713998028645945, precision: 0.7944129201222174, recall: 0.8695652173913043\n",
      "train: 2020-11-07T15:43:38.381544: step 1049, loss 0.06644351780414581, acc 0.7579963789981895, auc: 0.7601114730823433, precision: 0.7793904208998549, recall: 0.9171648163962425\n",
      "train: 2020-11-07T15:43:39.665347: step 1050, loss 0.03972627595067024, acc 0.8928571428571429, auc: 0.9384450213623564, precision: 0.9009700889248181, recall: 0.9670281995661605\n"
     ]
    }
   ],
   "source": [
    "class DKTEngine(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.train_dkt = None\n",
    "        self.test_dkt = None\n",
    "        self.sess = None\n",
    "        self.global_step = 0\n",
    "\n",
    "    def add_gradient_noise(self, grad, stddev=1e-3, name=None):\n",
    "        \"\"\"\n",
    "        Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n",
    "        \"\"\"\n",
    "        with tf.op_scope([grad, stddev], name, \"add_gradient_noise\") as name:\n",
    "            grad = tf.convert_to_tensor(grad, name=\"grad\")\n",
    "            gn = tf.random_normal(tf.shape(grad), stddev=stddev)\n",
    "            return tf.add(grad, gn, name=name)\n",
    "\n",
    "    def train_step(self, params, train_op, train_summary_op, train_summary_writer):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        dkt = self.train_dkt\n",
    "        sess = self.sess\n",
    "        global_step = self.global_step\n",
    "\n",
    "        feed_dict = {dkt.input_data: params['input_x'],\n",
    "                     dkt.target_id: params['target_id'],\n",
    "                     dkt.target_correctness: params['target_correctness'],\n",
    "                     dkt.max_steps: params['max_len'],\n",
    "                     dkt.sequence_len: params['seq_len'],\n",
    "                     dkt.keep_prob: self.config.modelConfig.dropout_keep_prob}\n",
    "\n",
    "        _, step, summaries, loss, binary_pred, pred, target_correctness = sess.run(\n",
    "            [train_op, global_step, train_summary_op, dkt.loss, dkt.binary_pred, dkt.pred, dkt.target_correctness],\n",
    "            feed_dict)\n",
    "\n",
    "        auc, accuracy, precision, recall = gen_metrics(params['seq_len'], binary_pred, pred, target_correctness)\n",
    "\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"train: {}: step {}, loss {}, acc {}, auc: {}, precision: {}, recall: {}\".format(time_str, step, loss, accuracy, \n",
    "                                                                                               auc, precision, recall))\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "    def dev_step(self, params, dev_summary_op, writer=None):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        dkt = self.test_dkt\n",
    "        sess = self.sess\n",
    "        global_step = self.global_step\n",
    "\n",
    "        feed_dict = {dkt.input_data: params['input_x'],\n",
    "                     dkt.target_id: params['target_id'],\n",
    "                     dkt.target_correctness: params['target_correctness'],\n",
    "                     dkt.max_steps: params['max_len'],\n",
    "                     dkt.sequence_len: params['seq_len'],\n",
    "                     dkt.keep_prob: 1.0}\n",
    "        step, summaries, loss, pred, binary_pred, target_correctness = sess.run(\n",
    "            [global_step, dev_summary_op, dkt.loss, dkt.pred, dkt.binary_pred, dkt.target_correctness],\n",
    "            feed_dict)\n",
    "\n",
    "        auc, accuracy, precision, recall = gen_metrics(params['seq_len'], binary_pred, pred, target_correctness)\n",
    "\n",
    "        if writer:\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "        return loss, accuracy, auc, precision, recall\n",
    "\n",
    "    def run_epoch(self, fileName):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param filePath:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 实例化配置参数对象\n",
    "        config = Config()\n",
    "\n",
    "        # 实例化数据生成对象\n",
    "        dataGen = DataGenerator(fileName, config)\n",
    "        dataGen.gen_attr()  # 生成训练集和测试集\n",
    "\n",
    "        train_seqs = dataGen.train_seqs\n",
    "        test_seqs = dataGen.test_seqs\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False\n",
    "        )\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        self.sess = sess\n",
    "\n",
    "        with sess.as_default():\n",
    "            # 实例化dkt模型对象\n",
    "            with tf.name_scope(\"train\"):\n",
    "                with tf.variable_scope(\"dkt\", reuse=None):\n",
    "                    train_dkt = TensorFlowDKT(config)\n",
    "\n",
    "            with tf.name_scope(\"test\"):\n",
    "                with tf.variable_scope(\"dkt\", reuse=True):\n",
    "                    test_dkt = TensorFlowDKT(config)\n",
    "\n",
    "            self.train_dkt = train_dkt\n",
    "            self.test_dkt = test_dkt\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            self.global_step = global_step\n",
    "\n",
    "            # 定义一个优化器\n",
    "            optimizer = tf.train.AdamOptimizer(config.trainConfig.learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(train_dkt.loss)\n",
    "\n",
    "            # 对梯度进行截断，并且加上梯度噪音\n",
    "            grads_and_vars = [(tf.clip_by_norm(g, config.trainConfig.max_grad_norm), v)\n",
    "                              for g, v in grads_and_vars if g is not None]\n",
    "            # grads_and_vars = [(self.add_gradient_noise(g), v) for g, v in grads_and_vars]\n",
    "\n",
    "            # 定义图中最后的节点\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step, name=\"train_op\")\n",
    "\n",
    "            # 保存各种变量或结果的值\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"writing to {}\".format(out_dir))\n",
    "\n",
    "            # 训练时的 Summaries\n",
    "            train_loss_summary = tf.summary.scalar(\"loss\", train_dkt.loss)\n",
    "            train_summary_op = tf.summary.merge([train_loss_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # 测试时的 summaries\n",
    "            test_loss_summary = tf.summary.scalar(\"loss\", test_dkt.loss)\n",
    "            dev_summary_op = tf.summary.merge([test_loss_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            print(\"初始化完毕，开始训练\")\n",
    "            for i in range(config.trainConfig.epochs):\n",
    "                np.random.shuffle(train_seqs)\n",
    "                for params in dataGen.next_batch(train_seqs):\n",
    "                    # 批次获得训练集，训练模型\n",
    "                    self.train_step(params, train_op, train_summary_op, train_summary_writer)\n",
    "\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    # train_step.run(feed_dict={x: batch_train[0], y_actual: batch_train[1], keep_prob: 0.5})\n",
    "                    # 对结果进行记录\n",
    "                    if current_step % config.trainConfig.evaluate_every == 0:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        # 获得测试数据\n",
    "\n",
    "                        losses = []\n",
    "                        accuracys = []\n",
    "                        aucs = []\n",
    "                        precisions = []\n",
    "                        recalls = []\n",
    "                        for params in dataGen.next_batch(test_seqs):\n",
    "                            loss, accuracy, auc, precision, recall = self.dev_step(params, dev_summary_op, writer=None)\n",
    "                            losses.append(loss)\n",
    "                            accuracys.append(accuracy)\n",
    "                            aucs.append(auc)\n",
    "                            precisions.append(precision)\n",
    "                            recalls.append(recall)\n",
    "\n",
    "                        time_str = datetime.datetime.now().isoformat()\n",
    "                        print(\"dev: {}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".\n",
    "                              format(time_str, current_step, mean(losses), mean(accuracys), mean(aucs), mean(precisions), mean(recalls)))\n",
    "\n",
    "                    if current_step % config.trainConfig.checkpoint_every == 0:\n",
    "                        path = saver.save(sess, \"model/my-model\", global_step=current_step)\n",
    "                        print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fileName = \"./data/assistments.txt\"\n",
    "    dktEngine = DKTEngine()\n",
    "    dktEngine.run_epoch(fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step: 1\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7574747830872554 0.719632768361582\n",
      "step: 2\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.8833076573005431 0.7919024814976056\n",
      "step: 3\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.8400032290804212 0.7782783969646668\n",
      "step: 4\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7319648502178364 0.7001841620626151\n",
      "step: 5\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9173542099865968 0.8443186413263243\n",
      "step: 6\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.8993574930614142 0.8245547555892383\n",
      "step: 7\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7461597651889139 0.7120954003407155\n",
      "step: 8\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7359475085314461 0.7674609695973705\n",
      "step: 9\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9097889745972318 0.8608511922728644\n",
      "step: 10\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7229079994509462 0.7225\n",
      "step: 11\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7102629296625503 0.7281043791241751\n",
      "step: 12\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9461758573589638 0.8855919241358213\n",
      "step: 13\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9736094501058902 0.921553576911268\n",
      "step: 14\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7274201183431953 0.7268939393939394\n",
      "step: 15\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9102294525601298 0.8273846153846154\n",
      "step: 16\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9015870024914683 0.8338670936749399\n",
      "step: 17\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.8152988683362347 0.7827121332275971\n",
      "step: 18\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7756712513299688 0.7756696428571429\n",
      "step: 19\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7566396225629062 0.71684350132626\n",
      "step: 20\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.8369570317339401 0.7851239669421488\n",
      "step: 21\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7290165837059713 0.7449486584961907\n",
      "step: 22\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.9462303489034871 0.8747116237799467\n",
      "step: 23\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7034485985663209 0.7047451669595782\n",
      "step: 24\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.7312509179926561 0.6884033613445378\n",
      "step: 25\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.710674664344694 0.6780797740910696\n",
      "step: 26\n",
      "INFO:tensorflow:Restoring parameters from model/my-model-1000\n",
      "0.8697288571578559 0.813570487483531\n",
      "inference  auc: 0.8149410779099553  acc: 0.7773070235825288\n"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "def load_model(fileName):\n",
    "    # 实例化配置参数对象\n",
    "    config = Config()\n",
    "\n",
    "    # 实例化数据生成对象\n",
    "    dataGen = DataGenerator(fileName, config)\n",
    "    dataGen.gen_attr()  # 生成训练集和测试集\n",
    "\n",
    "    test_seqs = dataGen.test_seqs\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        accuracys = []\n",
    "        aucs = []\n",
    "        step = 1\n",
    "\n",
    "        for params in dataGen.next_batch(test_seqs):\n",
    "            print(\"step: {}\".format(step))\n",
    "\n",
    "            saver = tf.train.import_meta_graph(\"model/my-model-800.meta\")\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(\"model/\"))\n",
    "\n",
    "            # 获得默认的计算图结构\n",
    "            graph = tf.get_default_graph()\n",
    "\n",
    "            # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "            input_x = graph.get_operation_by_name(\"test/dkt/input_x\").outputs[0]\n",
    "            target_id = graph.get_operation_by_name(\"test/dkt/target_id\").outputs[0]\n",
    "            keep_prob = graph.get_operation_by_name(\"test/dkt/keep_prob\").outputs[0]\n",
    "            max_steps = graph.get_operation_by_name(\"test/dkt/max_steps\").outputs[0]\n",
    "            sequence_len = graph.get_operation_by_name(\"test/dkt/sequence_len\").outputs[0]\n",
    "\n",
    "            # 获得输出的结果\n",
    "            pred_all = graph.get_tensor_by_name(\"test/dkt/pred_all:0\")\n",
    "            pred = graph.get_tensor_by_name(\"test/dkt/pred:0\")\n",
    "            binary_pred = graph.get_tensor_by_name(\"test/dkt/binary_pred:0\")\n",
    "\n",
    "            target_correctness = params['target_correctness']\n",
    "            pred_all, pred, binary_pred = sess.run([pred_all, pred, binary_pred],\n",
    "                                                   feed_dict={input_x: params[\"input_x\"],\n",
    "                                                              target_id: params[\"target_id\"],\n",
    "                                                              keep_prob: 1.0,\n",
    "                                                              max_steps: params[\"max_len\"],\n",
    "                                                              sequence_len: params[\"seq_len\"]})\n",
    "\n",
    "            auc, acc, precision, recall = gen_metrics(params[\"seq_len\"], binary_pred, pred, target_correctness)\n",
    "            print(auc, acc)\n",
    "            accuracys.append(acc)\n",
    "            aucs.append(auc)\n",
    "            step += 1\n",
    "\n",
    "        aucMean = mean(aucs)\n",
    "        accMean = mean(accuracys)\n",
    "\n",
    "        print(\"inference  auc: {}  acc: {}\".format(aucMean, accMean))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fileName = \"./data/assistments.txt\"\n",
    "    load_model(fileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}